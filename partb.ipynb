{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "def leaky_relu_grad(x):\n",
    "    return np.where(x > 0, 1, 0.01)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_grad(x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - x.max(axis=0, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=0, keepdims=True)\n",
    "\n",
    "def weight_zero(number, prev):\n",
    "    return np.zeros((number, prev))\n",
    "\n",
    "def weight_random(number, prev):\n",
    "    return np.random.uniform(-0.5, 0.5, (number, prev))\n",
    "\n",
    "def weight_normal(number, prev):\n",
    "    return np.random.normal(0, 1, (number, prev))\n",
    "\n",
    "class neuralnetwork:\n",
    "\n",
    "    def __init__(self, layers, Layer_sizes, lr, activation, initialisation, epochs, batch_size):\n",
    "        self.layers = layers\n",
    "        self.Layer_sizes = Layer_sizes\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.initialisation = initialisation\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "    def weight_initialisation(self, number, prev):\n",
    "        if self.initialisation == 'zero':\n",
    "            xyy=weight_zero(number, prev)\n",
    "            return xyy\n",
    "        elif self.initialisation == 'random':\n",
    "            xyy=weight_random(number, prev)\n",
    "            return xyy\n",
    "        elif self.initialisation == 'normal':\n",
    "            xyy=weight_normal(number, prev)\n",
    "            return xyy\n",
    "        else:\n",
    "            print(\"Invalid initialisation\")\n",
    "            br\n",
    "            return None\n",
    "\n",
    "    def activation_gradient(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            if self.activation == 'sigmoid':\n",
    "                return sigmoid_grad(x)\n",
    "            elif self.activation == 'tanh':\n",
    "                return tanh_grad(x)\n",
    "            elif self.activation == 'relu':\n",
    "                return relu_grad(x)\n",
    "            elif self.activation == 'leaky_relu':\n",
    "                return leaky_relu_grad(x)\n",
    "            elif self.activation == 'linear':\n",
    "                return linear_grad(x)\n",
    "            else:\n",
    "                print(\"Invalid activation\")\n",
    "                return None\n",
    "        else:\n",
    "            if self.activation == 'sigmoid':\n",
    "                return sigmoid(x)\n",
    "            elif self.activation == 'tanh':\n",
    "                return tanh(x)\n",
    "            elif self.activation == 'relu':\n",
    "                return relu(x)\n",
    "            elif self.activation == 'leaky_relu':\n",
    "                return leaky_relu(x)\n",
    "            elif self.activation == 'linear':\n",
    "                return linear(x)\n",
    "            elif self.activation == 'softmax':\n",
    "                return softmax(x)\n",
    "            else:\n",
    "                print(\"Invalid activation\")\n",
    "                return None\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        input = [X]\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            yy=np.dot(self.weights[i], input[-1])\n",
    "            input_eqn = self.biases[i] + yy\n",
    "            gradient_descent = self.activation_gradient(input_eqn, derivative=False)\n",
    "            input.append(gradient_descent)\n",
    "        f=np.dot(self.weights[-1], input[-1])\n",
    "        input_eqn = self.biases[-1] + f\n",
    "        input.append(softmax(input_eqn))\n",
    "        return input\n",
    "\n",
    "    def backward_propagation(self, X_batch, y_batch):\n",
    "        derivative_1 = self.forward_propagation(X_batch)[-1] - y_batch\n",
    "        derivatives = []\n",
    "        derivatives.append(derivative_1 / self.batch_size)\n",
    "\n",
    "        derivative_1 = derivatives[0]\n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            gg=self.weights[i + 1].T.dot(derivative_1)\n",
    "            hh=self.activation_gradient(self.forward_propagation(X_batch)[i + 1], derivative=True)\n",
    "            derivative_2 = gg * hh\n",
    "            derivatives.append(derivative_2 / self.batch_size)\n",
    "            derivative_1 = derivative_2\n",
    "\n",
    "        derivative_1 = derivatives[0]\n",
    "        a=np.dot(derivative_1, self.forward_propagation(X_batch)[-2].T)\n",
    "        self.weights[-1] += -self.lr * a\n",
    "        b=np.sum(derivative_1)\n",
    "        self.biases[-1] += -self.lr * b\n",
    "\n",
    "        k = 1\n",
    "        i = len(self.weights) - 2\n",
    "        while i >= 0:\n",
    "            derivative_2 = derivatives[k]\n",
    "            c=np.dot(derivative_2, self.forward_propagation(X_batch)[i].T)\n",
    "            self.weights[i] += -self.lr * c\n",
    "            d=np.sum(derivative_2)\n",
    "            self.biases[i] += -self.lr * d\n",
    "            k += 1\n",
    "            i -= 1\n",
    "\n",
    "    def calculate_losses(self, X, y):\n",
    "        input = self.forward_propagation(X)\n",
    "        loss = np.sum(-y * np.log(input[-1])) / X.shape[1]\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def plot_losses(self, loss_train, loss_val, save_path=None):\n",
    "        plt.title('Loss for activation function ' + self.activation + ' and initialization ' + self.initialisation)\n",
    "        plt.plot(loss_train, label='train')\n",
    "        plt.plot(loss_val, label='val')\n",
    "        plt.legend()\n",
    "\n",
    "        if save_path is None:\n",
    "            # Automatically generate file name based on activation and initialization\n",
    "            file_name = f'loss_plot_{self.activation}_{self.initialisation}.png'\n",
    "            save_path = os.path.join(os.getcwd(), file_name)\n",
    "\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    def fit(self, X_train, y_train, X_test, y_test):\n",
    "        print(\"Started Training\")\n",
    "        loss_train = []\n",
    "        loss_val = []\n",
    "        prev = X_train.shape[0]\n",
    "        for j in self.Layer_sizes:\n",
    "            self.weights.append(self.weight_initialisation(j, prev))\n",
    "            self.biases.append(np.zeros((j, 1)))\n",
    "            prev = j\n",
    "\n",
    "        self.weights.append(self.weight_initialisation(y_train.shape[0], prev))\n",
    "        self.biases.append(np.zeros((y_train.shape[0], 1)))\n",
    "\n",
    "        j = 0\n",
    "        while j < self.epochs:\n",
    "            start = 0\n",
    "            while start < X_train.shape[1]:\n",
    "                end = start + self.batch_size\n",
    "                part_x = X_train[:, start:end]\n",
    "                part_y = y_train[:, start:end]\n",
    "\n",
    "                input = self.forward_propagation(part_x)\n",
    "                self.backward_propagation(part_x, part_y)\n",
    "\n",
    "                start = end  \n",
    "\n",
    "            print(\"Epoch: \", j, \"Train Loss: \", self.calculate_losses(X_train, y_train))\n",
    "            print(\"Epoch: \", j, \"Val Loss: \", self.calculate_losses(X_test, y_test))\n",
    "            loss_train.append(self.calculate_losses(X_train, y_train))\n",
    "            loss_val.append(self.calculate_losses(X_test, y_test))\n",
    "\n",
    "            j += 1\n",
    "\n",
    "\n",
    "        self.plot_losses(loss_train, loss_val)\n",
    "\n",
    "        print(\"Training completed\")\n",
    "\n",
    "    def predict_forward_pass(self, input_data):\n",
    "        input = [input_data]\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            yy=np.dot(self.weights[i], input[-1])\n",
    "            input_eqn = self.biases[i] + yy\n",
    "            input.append(self.activation_gradient(input_eqn, derivative=False))\n",
    "\n",
    "        input_eqn = self.biases[-1] + np.dot(self.weights[-1], input[-1])\n",
    "        input.append(softmax(input_eqn))\n",
    "        return input\n",
    "\n",
    "    def _make_predictions(self, X_test):\n",
    "        predictions = []\n",
    "        for i in X_test:\n",
    "            i.shape += (1,)\n",
    "            input = self.predict_forward_pass(i)\n",
    "            predictions.append(input[-1].argmax())\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self._make_predictions(X_test)\n",
    "    \n",
    "    def score(self, Y_PRED, y_test):\n",
    " \n",
    "        accuracy = 0\n",
    "        i = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        while i < len(Y_PRED):\n",
    "            if Y_PRED[i] == y_test[i].argmax():\n",
    "                accuracy += 1\n",
    "            i += 1\n",
    "\n",
    "        return accuracy / len(Y_PRED)\n",
    "\n",
    "    def forward_pass_predict_prob(self, input_data):\n",
    "        input = [input_data]\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            input_eqn = self.biases[i] + np.dot(self.weights[i], input[-1])\n",
    "            input.append(self.activation_gradient(input_eqn, derivative=False))\n",
    "        input_eqn = self.biases[-1] + np.dot(self.weights[-1], input[-1])\n",
    "        input.append(softmax(input_eqn))\n",
    "        return input\n",
    "\n",
    "    def predict_proba_single(self, input_data):\n",
    "        input_data.shape += (1,)\n",
    "        input = self.forward_pass_predict_prob(input_data)\n",
    "        return input[-1]\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        predictions = [self.predict_proba_single(i) for i in X_test]\n",
    "        return predictions\n",
    "    def save_model(self, filename):\n",
    "        model_info = {\n",
    "            'layers': self.layers,\n",
    "            'Layer_sizes': self.Layer_sizes,\n",
    "            'lr': self.lr,\n",
    "            'activation': self.activation,\n",
    "            'initialisation': self.initialisation,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'weights': self.weights,\n",
    "            'biases': self.biases\n",
    "        }\n",
    "\n",
    "        joblib.dump(model_info, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nalin21478/miniconda3/lib/python3.11/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "x_data, y_data = mnist.data, mnist.target.astype(int)\n",
    "x_data = x_data / 255.0\n",
    "x_data = x_data.values.reshape((-1, 28 * 28))\n",
    "y_data = np.eye(10)[y_data]\n",
    "x_train, x_test = x_data[:60000], x_data[60000:]\n",
    "y_train, y_test = y_data[:60000], y_data[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid zero\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  2.3015161070048085\n",
      "Epoch:  0 Val Loss:  2.3014862072172457\n",
      "Epoch:  1 Train Loss:  2.3015160744464884\n",
      "Epoch:  1 Val Loss:  2.3014861583254462\n",
      "Epoch:  2 Train Loss:  2.3015160420438496\n",
      "Epoch:  2 Val Loss:  2.301486109615177\n",
      "Epoch:  3 Train Loss:  2.301516009796685\n",
      "Epoch:  3 Val Loss:  2.3014860610862287\n",
      "Epoch:  4 Train Loss:  2.301515977704788\n",
      "Epoch:  4 Val Loss:  2.301486012738391\n",
      "Epoch:  5 Train Loss:  2.3015159457679504\n",
      "Epoch:  5 Val Loss:  2.3014859645714525\n",
      "Epoch:  6 Train Loss:  2.3015159139859653\n",
      "Epoch:  6 Val Loss:  2.301485916585203\n",
      "Epoch:  7 Train Loss:  2.3015158823586246\n",
      "Epoch:  7 Val Loss:  2.3014858687794293\n",
      "Epoch:  8 Train Loss:  2.3015158508857176\n",
      "Epoch:  8 Val Loss:  2.3014858211539195\n",
      "Epoch:  9 Train Loss:  2.301515819567036\n",
      "Epoch:  9 Val Loss:  2.3014857737084617\n",
      "Epoch:  10 Train Loss:  2.3015157884023716\n",
      "Epoch:  10 Val Loss:  2.3014857264428423\n",
      "Epoch:  11 Train Loss:  2.3015157573915146\n",
      "Epoch:  11 Val Loss:  2.3014856793568486\n",
      "Epoch:  12 Train Loss:  2.3015157265342525\n",
      "Epoch:  12 Val Loss:  2.301485632450265\n",
      "Epoch:  13 Train Loss:  2.3015156958303757\n",
      "Epoch:  13 Val Loss:  2.3014855857228778\n",
      "Epoch:  14 Train Loss:  2.3015156652796747\n",
      "Epoch:  14 Val Loss:  2.3014855391744735\n",
      "Epoch:  15 Train Loss:  2.301515634881937\n",
      "Epoch:  15 Val Loss:  2.301485492804834\n",
      "Epoch:  16 Train Loss:  2.301515604636951\n",
      "Epoch:  16 Val Loss:  2.3014854466137447\n",
      "Epoch:  17 Train Loss:  2.3015155745445024\n",
      "Epoch:  17 Val Loss:  2.301485400600989\n",
      "Epoch:  18 Train Loss:  2.3015155446043836\n",
      "Epoch:  18 Val Loss:  2.3014853547663514\n",
      "Epoch:  19 Train Loss:  2.301515514816379\n",
      "Epoch:  19 Val Loss:  2.3014853091096135\n",
      "Epoch:  20 Train Loss:  2.301515485180273\n",
      "Epoch:  20 Val Loss:  2.3014852636305583\n",
      "Epoch:  21 Train Loss:  2.3015154556958564\n",
      "Epoch:  21 Val Loss:  2.301485218328966\n",
      "Epoch:  22 Train Loss:  2.3015154263629127\n",
      "Epoch:  22 Val Loss:  2.3014851732046204\n",
      "Epoch:  23 Train Loss:  2.301515397181229\n",
      "Epoch:  23 Val Loss:  2.3014851282573012\n",
      "Epoch:  24 Train Loss:  2.3015153681505898\n",
      "Epoch:  24 Val Loss:  2.30148508348679\n",
      "Epoch:  25 Train Loss:  2.301515339270779\n",
      "Epoch:  25 Val Loss:  2.301485038892865\n",
      "Epoch:  26 Train Loss:  2.301515310541585\n",
      "Epoch:  26 Val Loss:  2.301484994475308\n",
      "Epoch:  27 Train Loss:  2.3015152819627867\n",
      "Epoch:  27 Val Loss:  2.301484950233897\n",
      "Epoch:  28 Train Loss:  2.3015152535341734\n",
      "Epoch:  28 Val Loss:  2.301484906168412\n",
      "Epoch:  29 Train Loss:  2.3015152252555264\n",
      "Epoch:  29 Val Loss:  2.30148486227863\n",
      "Epoch:  30 Train Loss:  2.301515197126628\n",
      "Epoch:  30 Val Loss:  2.30148481856433\n",
      "Epoch:  31 Train Loss:  2.3015151691472617\n",
      "Epoch:  31 Val Loss:  2.3014847750252896\n",
      "Epoch:  32 Train Loss:  2.301515141317211\n",
      "Epoch:  32 Val Loss:  2.3014847316612856\n",
      "Epoch:  33 Train Loss:  2.3015151136362575\n",
      "Epoch:  33 Val Loss:  2.3014846884720943\n",
      "Epoch:  34 Train Loss:  2.301515086104184\n",
      "Epoch:  34 Val Loss:  2.3014846454574927\n",
      "Epoch:  35 Train Loss:  2.3015150587207684\n",
      "Epoch:  35 Val Loss:  2.301484602617257\n",
      "Epoch:  36 Train Loss:  2.3015150314857986\n",
      "Epoch:  36 Val Loss:  2.3014845599511617\n",
      "Epoch:  37 Train Loss:  2.3015150043990507\n",
      "Epoch:  37 Val Loss:  2.301484517458982\n",
      "Epoch:  38 Train Loss:  2.3015149774603048\n",
      "Epoch:  38 Val Loss:  2.301484475140494\n",
      "Epoch:  39 Train Loss:  2.301514950669343\n",
      "Epoch:  39 Val Loss:  2.3014844329954705\n",
      "Epoch:  40 Train Loss:  2.3015149240259456\n",
      "Epoch:  40 Val Loss:  2.301484391023685\n",
      "Epoch:  41 Train Loss:  2.3015148975298905\n",
      "Epoch:  41 Val Loss:  2.301484349224912\n",
      "Epoch:  42 Train Loss:  2.301514871180957\n",
      "Epoch:  42 Val Loss:  2.301484307598924\n",
      "Epoch:  43 Train Loss:  2.301514844978927\n",
      "Epoch:  43 Val Loss:  2.301484266145494\n",
      "Epoch:  44 Train Loss:  2.3015148189235752\n",
      "Epoch:  44 Val Loss:  2.301484224864393\n",
      "Epoch:  45 Train Loss:  2.301514793014683\n",
      "Epoch:  45 Val Loss:  2.301484183755395\n",
      "Epoch:  46 Train Loss:  2.301514767252026\n",
      "Epoch:  46 Val Loss:  2.30148414281827\n",
      "Epoch:  47 Train Loss:  2.3015147416353816\n",
      "Epoch:  47 Val Loss:  2.3014841020527883\n",
      "Epoch:  48 Train Loss:  2.3015147161645304\n",
      "Epoch:  48 Val Loss:  2.3014840614587224\n",
      "Epoch:  49 Train Loss:  2.3015146908392454\n",
      "Epoch:  49 Val Loss:  2.3014840210358414\n",
      "Epoch:  50 Train Loss:  2.301514665659305\n",
      "Epoch:  50 Val Loss:  2.3014839807839147\n",
      "Epoch:  51 Train Loss:  2.3015146406244886\n",
      "Epoch:  51 Val Loss:  2.3014839407027137\n",
      "Epoch:  52 Train Loss:  2.301514615734566\n",
      "Epoch:  52 Val Loss:  2.3014839007920043\n",
      "Epoch:  53 Train Loss:  2.3015145909893184\n",
      "Epoch:  53 Val Loss:  2.301483861051558\n",
      "Epoch:  54 Train Loss:  2.301514566388518\n",
      "Epoch:  54 Val Loss:  2.301483821481142\n",
      "Epoch:  55 Train Loss:  2.3015145419319425\n",
      "Epoch:  55 Val Loss:  2.3014837820805236\n",
      "Epoch:  56 Train Loss:  2.3015145176193634\n",
      "Epoch:  56 Val Loss:  2.3014837428494723\n",
      "Epoch:  57 Train Loss:  2.3015144934505596\n",
      "Epoch:  57 Val Loss:  2.3014837037877527\n",
      "Epoch:  58 Train Loss:  2.301514469425302\n",
      "Epoch:  58 Val Loss:  2.3014836648951347\n",
      "Epoch:  59 Train Loss:  2.301514445543365\n",
      "Epoch:  59 Val Loss:  2.3014836261713816\n",
      "Epoch:  60 Train Loss:  2.301514421804523\n",
      "Epoch:  60 Val Loss:  2.3014835876162607\n",
      "Epoch:  61 Train Loss:  2.301514398208548\n",
      "Epoch:  61 Val Loss:  2.3014835492295376\n",
      "Epoch:  62 Train Loss:  2.301514374755217\n",
      "Epoch:  62 Val Loss:  2.301483511010978\n",
      "Epoch:  63 Train Loss:  2.3015143514442973\n",
      "Epoch:  63 Val Loss:  2.301483472960347\n",
      "Epoch:  64 Train Loss:  2.301514328275565\n",
      "Epoch:  64 Val Loss:  2.3014834350774085\n",
      "Epoch:  65 Train Loss:  2.3015143052487903\n",
      "Epoch:  65 Val Loss:  2.301483397361926\n",
      "Epoch:  66 Train Loss:  2.301514282363749\n",
      "Epoch:  66 Val Loss:  2.3014833598136653\n",
      "Epoch:  67 Train Loss:  2.301514259620206\n",
      "Epoch:  67 Val Loss:  2.301483322432387\n",
      "Epoch:  68 Train Loss:  2.3015142370179387\n",
      "Epoch:  68 Val Loss:  2.3014832852178575\n",
      "Epoch:  69 Train Loss:  2.301514214556715\n",
      "Epoch:  69 Val Loss:  2.301483248169838\n",
      "Epoch:  70 Train Loss:  2.3015141922363074\n",
      "Epoch:  70 Val Loss:  2.3014832112880907\n",
      "Epoch:  71 Train Loss:  2.301514170056485\n",
      "Epoch:  71 Val Loss:  2.3014831745723776\n",
      "Epoch:  72 Train Loss:  2.3015141480170174\n",
      "Epoch:  72 Val Loss:  2.3014831380224607\n",
      "Epoch:  73 Train Loss:  2.301514126117677\n",
      "Epoch:  73 Val Loss:  2.3014831016381025\n",
      "Epoch:  74 Train Loss:  2.3015141043582337\n",
      "Epoch:  74 Val Loss:  2.301483065419062\n",
      "Epoch:  75 Train Loss:  2.301514082738453\n",
      "Epoch:  75 Val Loss:  2.3014830293651003\n",
      "Epoch:  76 Train Loss:  2.301514061258107\n",
      "Epoch:  76 Val Loss:  2.3014829934759784\n",
      "Epoch:  77 Train Loss:  2.3015140399169645\n",
      "Epoch:  77 Val Loss:  2.301482957751456\n",
      "Epoch:  78 Train Loss:  2.3015140187147933\n",
      "Epoch:  78 Val Loss:  2.3014829221912927\n",
      "Epoch:  79 Train Loss:  2.301513997651362\n",
      "Epoch:  79 Val Loss:  2.301482886795248\n",
      "Epoch:  80 Train Loss:  2.3015139767264383\n",
      "Epoch:  80 Val Loss:  2.3014828515630814\n",
      "Epoch:  81 Train Loss:  2.3015139559397912\n",
      "Epoch:  81 Val Loss:  2.3014828164945493\n",
      "Epoch:  82 Train Loss:  2.3015139352911866\n",
      "Epoch:  82 Val Loss:  2.3014827815894123\n",
      "Epoch:  83 Train Loss:  2.301513914780392\n",
      "Epoch:  83 Val Loss:  2.301482746847428\n",
      "Epoch:  84 Train Loss:  2.301513894407176\n",
      "Epoch:  84 Val Loss:  2.3014827122683523\n",
      "Epoch:  85 Train Loss:  2.3015138741713037\n",
      "Epoch:  85 Val Loss:  2.301482677851945\n",
      "Epoch:  86 Train Loss:  2.3015138540725424\n",
      "Epoch:  86 Val Loss:  2.301482643597961\n",
      "Epoch:  87 Train Loss:  2.301513834110659\n",
      "Epoch:  87 Val Loss:  2.3014826095061585\n",
      "Epoch:  88 Train Loss:  2.301513814285417\n",
      "Epoch:  88 Val Loss:  2.3014825755762924\n",
      "Epoch:  89 Train Loss:  2.301513794596585\n",
      "Epoch:  89 Val Loss:  2.301482541808119\n",
      "Epoch:  90 Train Loss:  2.301513775043927\n",
      "Epoch:  90 Val Loss:  2.3014825082013957\n",
      "Epoch:  91 Train Loss:  2.301513755627209\n",
      "Epoch:  91 Val Loss:  2.301482474755876\n",
      "Epoch:  92 Train Loss:  2.3015137363461964\n",
      "Epoch:  92 Val Loss:  2.301482441471315\n",
      "Epoch:  93 Train Loss:  2.3015137172006526\n",
      "Epoch:  93 Val Loss:  2.3014824083474683\n",
      "Epoch:  94 Train Loss:  2.3015136981903423\n",
      "Epoch:  94 Val Loss:  2.3014823753840896\n",
      "Epoch:  95 Train Loss:  2.3015136793150313\n",
      "Epoch:  95 Val Loss:  2.3014823425809348\n",
      "Epoch:  96 Train Loss:  2.3015136605744817\n",
      "Epoch:  96 Val Loss:  2.3014823099377555\n",
      "Epoch:  97 Train Loss:  2.301513641968459\n",
      "Epoch:  97 Val Loss:  2.3014822774543053\n",
      "Epoch:  98 Train Loss:  2.3015136234967266\n",
      "Epoch:  98 Val Loss:  2.3014822451303383\n",
      "Epoch:  99 Train Loss:  2.301513605159047\n",
      "Epoch:  99 Val Loss:  2.301482212965608\n",
      "Training completed\n",
      "sigmoid random\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  2.2083798420429592\n",
      "Epoch:  0 Val Loss:  2.204518211457096\n",
      "Epoch:  1 Train Loss:  2.13438732909969\n",
      "Epoch:  1 Val Loss:  2.127926332876038\n",
      "Epoch:  2 Train Loss:  2.055923792337936\n",
      "Epoch:  2 Val Loss:  2.046978842545268\n",
      "Epoch:  3 Train Loss:  1.9711118745371583\n",
      "Epoch:  3 Val Loss:  1.959816713729828\n",
      "Epoch:  4 Train Loss:  1.8805147492627667\n",
      "Epoch:  4 Val Loss:  1.8671049481277995\n",
      "Epoch:  5 Train Loss:  1.7865891648766856\n",
      "Epoch:  5 Val Loss:  1.7714317292361843\n",
      "Epoch:  6 Train Loss:  1.6926097057289713\n",
      "Epoch:  6 Val Loss:  1.6761661991307673\n",
      "Epoch:  7 Train Loss:  1.6015027676698015\n",
      "Epoch:  7 Val Loss:  1.5842478836232228\n",
      "Epoch:  8 Train Loss:  1.5151826880555925\n",
      "Epoch:  8 Val Loss:  1.4975282804094043\n",
      "Epoch:  9 Train Loss:  1.4345364598183257\n",
      "Epoch:  9 Val Loss:  1.4167939308076085\n",
      "Epoch:  10 Train Loss:  1.359763558528278\n",
      "Epoch:  10 Val Loss:  1.342142122385121\n",
      "Epoch:  11 Train Loss:  1.2907315170861926\n",
      "Epoch:  11 Val Loss:  1.2733547127767875\n",
      "Epoch:  12 Train Loss:  1.227196052575668\n",
      "Epoch:  12 Val Loss:  1.210123203681086\n",
      "Epoch:  13 Train Loss:  1.1688915183455741\n",
      "Epoch:  13 Val Loss:  1.1521382861069507\n",
      "Epoch:  14 Train Loss:  1.1155481977120119\n",
      "Epoch:  14 Val Loss:  1.0991040743716463\n",
      "Epoch:  15 Train Loss:  1.0668834010395836\n",
      "Epoch:  15 Val Loss:  1.0507251656738223\n",
      "Epoch:  16 Train Loss:  1.0225913476197117\n",
      "Epoch:  16 Val Loss:  1.0066921886737819\n",
      "Epoch:  17 Train Loss:  0.9823409334394682\n",
      "Epoch:  17 Val Loss:  0.9666756619729662\n",
      "Epoch:  18 Train Loss:  0.94578204395956\n",
      "Epoch:  18 Val Loss:  0.9303293454217799\n",
      "Epoch:  19 Train Loss:  0.9125573654816742\n",
      "Epoch:  19 Val Loss:  0.8973001870740618\n",
      "Epoch:  20 Train Loss:  0.882315752970707\n",
      "Epoch:  20 Val Loss:  0.8672407591380377\n",
      "Epoch:  21 Train Loss:  0.8547239785944792\n",
      "Epoch:  21 Val Loss:  0.8398207032993148\n",
      "Epoch:  22 Train Loss:  0.8294751342472074\n",
      "Epoch:  22 Val Loss:  0.8147351669947439\n",
      "Epoch:  23 Train Loss:  0.806293313777772\n",
      "Epoch:  23 Val Loss:  0.7917096502555432\n",
      "Epoch:  24 Train Loss:  0.7849350712952776\n",
      "Epoch:  24 Val Loss:  0.7705016453776254\n",
      "Epoch:  25 Train Loss:  0.7651885171963934\n",
      "Epoch:  25 Val Loss:  0.7508998835181929\n",
      "Epoch:  26 Train Loss:  0.746870929390368\n",
      "Epoch:  26 Val Loss:  0.7327220553751284\n",
      "Epoch:  27 Train Loss:  0.729825600375917\n",
      "Epoch:  27 Val Loss:  0.7158117323241099\n",
      "Epoch:  28 Train Loss:  0.7139184367595591\n",
      "Epoch:  28 Val Loss:  0.7000350145721506\n",
      "Epoch:  29 Train Loss:  0.699034642332042\n",
      "Epoch:  29 Val Loss:  0.6852772464701942\n",
      "Epoch:  30 Train Loss:  0.6850756726618359\n",
      "Epoch:  30 Val Loss:  0.6714399934539956\n",
      "Epoch:  31 Train Loss:  0.6719565495057617\n",
      "Epoch:  31 Val Loss:  0.6584383729550899\n",
      "Epoch:  32 Train Loss:  0.6596035593443504\n",
      "Epoch:  32 Val Loss:  0.646198765758428\n",
      "Epoch:  33 Train Loss:  0.6479523224079861\n",
      "Epoch:  33 Val Loss:  0.6346568951301598\n",
      "Epoch:  34 Train Loss:  0.6369461983001109\n",
      "Epoch:  34 Val Loss:  0.623756240115734\n",
      "Epoch:  35 Train Loss:  0.6265349854279259\n",
      "Epoch:  35 Val Loss:  0.6134467402415325\n",
      "Epoch:  36 Train Loss:  0.6166738695067999\n",
      "Epoch:  36 Val Loss:  0.603683746884791\n",
      "Epoch:  37 Train Loss:  0.6073225784726969\n",
      "Epoch:  37 Val Loss:  0.5944271787489632\n",
      "Epoch:  38 Train Loss:  0.5984447053184283\n",
      "Epoch:  38 Val Loss:  0.5856408432086035\n",
      "Epoch:  39 Train Loss:  0.5900071654656889\n",
      "Epoch:  39 Val Loss:  0.5772918905131637\n",
      "Epoch:  40 Train Loss:  0.5819797605576273\n",
      "Epoch:  40 Val Loss:  0.5693503731937625\n",
      "Epoch:  41 Train Loss:  0.5743348255693416\n",
      "Epoch:  41 Val Loss:  0.5617888880545534\n",
      "Epoch:  42 Train Loss:  0.5670469406473148\n",
      "Epoch:  42 Val Loss:  0.5545822826168806\n",
      "Epoch:  43 Train Loss:  0.5600926929945258\n",
      "Epoch:  43 Val Loss:  0.5477074117247175\n",
      "Epoch:  44 Train Loss:  0.5534504773917462\n",
      "Epoch:  44 Val Loss:  0.5411429332061831\n",
      "Epoch:  45 Train Loss:  0.5471003266164737\n",
      "Epoch:  45 Val Loss:  0.5348691340625893\n",
      "Epoch:  46 Train Loss:  0.5410237651488798\n",
      "Epoch:  46 Val Loss:  0.5288677806951889\n",
      "Epoch:  47 Train Loss:  0.5352036812136978\n",
      "Epoch:  47 Val Loss:  0.5231219882628392\n",
      "Epoch:  48 Train Loss:  0.5296242134763587\n",
      "Epoch:  48 Val Loss:  0.5176161054731387\n",
      "Epoch:  49 Train Loss:  0.5242706496656289\n",
      "Epoch:  49 Val Loss:  0.5123356120206717\n",
      "Epoch:  50 Train Loss:  0.5191293351004431\n",
      "Epoch:  50 Val Loss:  0.507267026564298\n",
      "Epoch:  51 Train Loss:  0.5141875896127988\n",
      "Epoch:  51 Val Loss:  0.5023978236357218\n",
      "Epoch:  52 Train Loss:  0.5094336317286675\n",
      "Epoch:  52 Val Loss:  0.4977163582380199\n",
      "Epoch:  53 Train Loss:  0.5048565092325118\n",
      "Epoch:  53 Val Loss:  0.4932117971601745\n",
      "Epoch:  54 Train Loss:  0.5004460354274274\n",
      "Epoch:  54 Val Loss:  0.48887405622862934\n",
      "Epoch:  55 Train Loss:  0.49619273053436125\n",
      "Epoch:  55 Val Loss:  0.48469374285959105\n",
      "Epoch:  56 Train Loss:  0.4920877677667867\n",
      "Epoch:  56 Val Loss:  0.4806621033811404\n",
      "Epoch:  57 Train Loss:  0.4881229236835927\n",
      "Epoch:  57 Val Loss:  0.4767709746730242\n",
      "Epoch:  58 Train Loss:  0.48429053247134374\n",
      "Epoch:  58 Val Loss:  0.47301273973209745\n",
      "Epoch:  59 Train Loss:  0.4805834438434603\n",
      "Epoch:  59 Val Loss:  0.4693802868182859\n",
      "Epoch:  60 Train Loss:  0.47699498427232084\n",
      "Epoch:  60 Val Loss:  0.465866971873524\n",
      "Epoch:  61 Train Loss:  0.4735189212935777\n",
      "Epoch:  61 Val Loss:  0.46246658393705675\n",
      "Epoch:  62 Train Loss:  0.47014943064185866\n",
      "Epoch:  62 Val Loss:  0.45917331330665545\n",
      "Epoch:  63 Train Loss:  0.46688106599465956\n",
      "Epoch:  63 Val Loss:  0.4559817222178859\n",
      "Epoch:  64 Train Loss:  0.4637087311173344\n",
      "Epoch:  64 Val Loss:  0.45288671783348017\n",
      "Epoch:  65 Train Loss:  0.4606276542170055\n",
      "Epoch:  65 Val Loss:  0.4498835273526541\n",
      "Epoch:  66 Train Loss:  0.45763336432728363\n",
      "Epoch:  66 Val Loss:  0.446967675066257\n",
      "Epoch:  67 Train Loss:  0.4547216695589133\n",
      "Epoch:  67 Val Loss:  0.4441349611982571\n",
      "Epoch:  68 Train Loss:  0.45188863706399174\n",
      "Epoch:  68 Val Loss:  0.44138144238738314\n",
      "Epoch:  69 Train Loss:  0.4491305745732105\n",
      "Epoch:  69 Val Loss:  0.43870341367496296\n",
      "Epoch:  70 Train Loss:  0.4464440133766828\n",
      "Epoch:  70 Val Loss:  0.43609739187619034\n",
      "Epoch:  71 Train Loss:  0.44382569262932847\n",
      "Epoch:  71 Val Loss:  0.43356010022231783\n",
      "Epoch:  72 Train Loss:  0.44127254487152023\n",
      "Epoch:  72 Val Loss:  0.43108845417069724\n",
      "Epoch:  73 Train Loss:  0.4387816826647336\n",
      "Epoch:  73 Val Loss:  0.4286795482882209\n",
      "Epoch:  74 Train Loss:  0.4363503862503356\n",
      "Epoch:  74 Val Loss:  0.42633064412163324\n",
      "Epoch:  75 Train Loss:  0.4339760921474049\n",
      "Epoch:  75 Val Loss:  0.42403915897543276\n",
      "Epoch:  76 Train Loss:  0.43165638261260875\n",
      "Epoch:  76 Val Loss:  0.42180265552471885\n",
      "Epoch:  77 Train Loss:  0.42938897589174396\n",
      "Epoch:  77 Val Loss:  0.4196188321964162\n",
      "Epoch:  78 Train Loss:  0.4271717171985592\n",
      "Epoch:  78 Val Loss:  0.41748551425786007\n",
      "Epoch:  79 Train Loss:  0.42500257036201117\n",
      "Epoch:  79 Val Loss:  0.4154006455568137\n",
      "Epoch:  80 Train Loss:  0.42287961008812713\n",
      "Epoch:  80 Val Loss:  0.41336228086163446\n",
      "Epoch:  81 Train Loss:  0.42080101478727944\n",
      "Epoch:  81 Val Loss:  0.4113685787545536\n",
      "Epoch:  82 Train Loss:  0.41876505992185875\n",
      "Epoch:  82 Val Loss:  0.4094177950349235\n",
      "Epoch:  83 Train Loss:  0.4167701118331765\n",
      "Epoch:  83 Val Loss:  0.40750827659283745\n",
      "Epoch:  84 Train Loss:  0.41481462200991986\n",
      "Epoch:  84 Val Loss:  0.40563845571678186\n",
      "Epoch:  85 Train Loss:  0.41289712176365584\n",
      "Epoch:  85 Val Loss:  0.4038068448019467\n",
      "Epoch:  86 Train Loss:  0.41101621727979154\n",
      "Epoch:  86 Val Loss:  0.4020120314285473\n",
      "Epoch:  87 Train Loss:  0.4091705850150297\n",
      "Epoch:  87 Val Loss:  0.4002526737819968\n",
      "Epoch:  88 Train Loss:  0.40735896741477096\n",
      "Epoch:  88 Val Loss:  0.3985274963890445\n",
      "Epoch:  89 Train Loss:  0.4055801689261051\n",
      "Epoch:  89 Val Loss:  0.39683528614608665\n",
      "Epoch:  90 Train Loss:  0.4038330522840337\n",
      "Epoch:  90 Val Loss:  0.39517488861776134\n",
      "Epoch:  91 Train Loss:  0.40211653505039396\n",
      "Epoch:  91 Val Loss:  0.3935452045856917\n",
      "Epoch:  92 Train Loss:  0.40042958638661336\n",
      "Epoch:  92 Val Loss:  0.39194518682884216\n",
      "Epoch:  93 Train Loss:  0.398771224042949\n",
      "Epoch:  93 Val Loss:  0.39037383711842544\n",
      "Epoch:  94 Train Loss:  0.3971405115482578\n",
      "Epoch:  94 Val Loss:  0.38883020341163993\n",
      "Epoch:  95 Train Loss:  0.39553655558560163\n",
      "Epoch:  95 Val Loss:  0.38731337722975634\n",
      "Epoch:  96 Train Loss:  0.39395850354016726\n",
      "Epoch:  96 Val Loss:  0.3858224912072022\n",
      "Epoch:  97 Train Loss:  0.39240554120703186\n",
      "Epoch:  97 Val Loss:  0.38435671679933436\n",
      "Epoch:  98 Train Loss:  0.3908768906472911\n",
      "Epoch:  98 Val Loss:  0.3829152621375421\n",
      "Epoch:  99 Train Loss:  0.38937180818194334\n",
      "Epoch:  99 Val Loss:  0.38149737002119993\n",
      "Training completed\n",
      "sigmoid normal\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  2.3051107300125757\n",
      "Epoch:  0 Val Loss:  2.267339539632213\n",
      "Epoch:  1 Train Loss:  2.0553624618843442\n",
      "Epoch:  1 Val Loss:  2.0198315788718944\n",
      "Epoch:  2 Train Loss:  1.9279339361120091\n",
      "Epoch:  2 Val Loss:  1.8939262318166799\n",
      "Epoch:  3 Train Loss:  1.8490885153274346\n",
      "Epoch:  3 Val Loss:  1.8157833475895209\n",
      "Epoch:  4 Train Loss:  1.7934288785042385\n",
      "Epoch:  4 Val Loss:  1.7602521887001021\n",
      "Epoch:  5 Train Loss:  1.75040030806513\n",
      "Epoch:  5 Val Loss:  1.716991001908516\n",
      "Epoch:  6 Train Loss:  1.7150256988806256\n",
      "Epoch:  6 Val Loss:  1.6811694244857833\n",
      "Epoch:  7 Train Loss:  1.6847056143257773\n",
      "Epoch:  7 Val Loss:  1.6502888648975509\n",
      "Epoch:  8 Train Loss:  1.6579590528888817\n",
      "Epoch:  8 Val Loss:  1.6229399481846292\n",
      "Epoch:  9 Train Loss:  1.6338768321754562\n",
      "Epoch:  9 Val Loss:  1.5982651799157832\n",
      "Epoch:  10 Train Loss:  1.6118640215424924\n",
      "Epoch:  10 Val Loss:  1.5757060961793197\n",
      "Epoch:  11 Train Loss:  1.5915097631646167\n",
      "Epoch:  11 Val Loss:  1.554875785509204\n",
      "Epoch:  12 Train Loss:  1.5725174618611626\n",
      "Epoch:  12 Val Loss:  1.5354912558341134\n",
      "Epoch:  13 Train Loss:  1.5546654834894034\n",
      "Epoch:  13 Val Loss:  1.5173363265399742\n",
      "Epoch:  14 Train Loss:  1.5377839948501404\n",
      "Epoch:  14 Val Loss:  1.5002406609494163\n",
      "Epoch:  15 Train Loss:  1.5217405334177327\n",
      "Epoch:  15 Val Loss:  1.4840673243986353\n",
      "Epoch:  16 Train Loss:  1.506430392819339\n",
      "Epoch:  16 Val Loss:  1.4687048053494693\n",
      "Epoch:  17 Train Loss:  1.4917698077435844\n",
      "Epoch:  17 Val Loss:  1.45406145162022\n",
      "Epoch:  18 Train Loss:  1.4776909231323119\n",
      "Epoch:  18 Val Loss:  1.4400613359508736\n",
      "Epoch:  19 Train Loss:  1.4641380151716783\n",
      "Epoch:  19 Val Loss:  1.4266410570788057\n",
      "Epoch:  20 Train Loss:  1.4510646502057287\n",
      "Epoch:  20 Val Loss:  1.4137472059516503\n",
      "Epoch:  21 Train Loss:  1.4384315704283974\n",
      "Epoch:  21 Val Loss:  1.401334343412409\n",
      "Epoch:  22 Train Loss:  1.4262051472527022\n",
      "Epoch:  22 Val Loss:  1.3893634003173478\n",
      "Epoch:  23 Train Loss:  1.4143562680146606\n",
      "Epoch:  23 Val Loss:  1.3778004398546568\n",
      "Epoch:  24 Train Loss:  1.4028595347185764\n",
      "Epoch:  24 Val Loss:  1.3666157272979635\n",
      "Epoch:  25 Train Loss:  1.391692670565396\n",
      "Epoch:  25 Val Loss:  1.355783047015938\n",
      "Epoch:  26 Train Loss:  1.3808360566436677\n",
      "Epoch:  26 Val Loss:  1.3452791998054539\n",
      "Epoch:  27 Train Loss:  1.3702723506456607\n",
      "Epoch:  27 Val Loss:  1.3350836134987185\n",
      "Epoch:  28 Train Loss:  1.359986162946965\n",
      "Epoch:  28 Val Loss:  1.3251780122372114\n",
      "Epoch:  29 Train Loss:  1.3499637791356227\n",
      "Epoch:  29 Val Loss:  1.3155461135742552\n",
      "Epoch:  30 Train Loss:  1.3401929226296434\n",
      "Epoch:  30 Val Loss:  1.306173347816065\n",
      "Epoch:  31 Train Loss:  1.330662549452935\n",
      "Epoch:  31 Val Loss:  1.2970466088106885\n",
      "Epoch:  32 Train Loss:  1.3213626648278116\n",
      "Epoch:  32 Val Loss:  1.2881540450616462\n",
      "Epoch:  33 Train Loss:  1.3122841529258682\n",
      "Epoch:  33 Val Loss:  1.2794848906485914\n",
      "Epoch:  34 Train Loss:  1.3034186175837499\n",
      "Epoch:  34 Val Loss:  1.2710293274039723\n",
      "Epoch:  35 Train Loss:  1.2947582389519494\n",
      "Epoch:  35 Val Loss:  1.2627783683352\n",
      "Epoch:  36 Train Loss:  1.28629565394488\n",
      "Epoch:  36 Val Loss:  1.2547237552078703\n",
      "Epoch:  37 Train Loss:  1.2780238656187002\n",
      "Epoch:  37 Val Loss:  1.2468578663667573\n",
      "Epoch:  38 Train Loss:  1.269936181176711\n",
      "Epoch:  38 Val Loss:  1.239173632763902\n",
      "Epoch:  39 Train Loss:  1.2620261741488814\n",
      "Epoch:  39 Val Loss:  1.2316644614430174\n",
      "Epoch:  40 Train Loss:  1.2542876649202932\n",
      "Epoch:  40 Val Loss:  1.224324166867055\n",
      "Epoch:  41 Train Loss:  1.2467147141367436\n",
      "Epoch:  41 Val Loss:  1.2171469111298905\n",
      "Epoch:  42 Train Loss:  1.239301624122971\n",
      "Epoch:  42 Val Loss:  1.2101271539173664\n",
      "Epoch:  43 Train Loss:  1.2320429438437508\n",
      "Epoch:  43 Val Loss:  1.20325961226961\n",
      "Epoch:  44 Train Loss:  1.2249334734363664\n",
      "Epoch:  44 Val Loss:  1.1965392292766859\n",
      "Epoch:  45 Train Loss:  1.2179682652407937\n",
      "Epoch:  45 Val Loss:  1.1899611502436112\n",
      "Epoch:  46 Train Loss:  1.2111426194636805\n",
      "Epoch:  46 Val Loss:  1.1835207046999796\n",
      "Epoch:  47 Train Loss:  1.2044520738362279\n",
      "Epoch:  47 Val Loss:  1.1772133927729473\n",
      "Epoch:  48 Train Loss:  1.19789238765444\n",
      "Epoch:  48 Val Loss:  1.1710348747104158\n",
      "Epoch:  49 Train Loss:  1.191459521393248\n",
      "Epoch:  49 Val Loss:  1.1649809626277476\n",
      "Epoch:  50 Train Loss:  1.1851496136964101\n",
      "Epoch:  50 Val Loss:  1.1590476138229644\n",
      "Epoch:  51 Train Loss:  1.1789589579220388\n",
      "Epoch:  51 Val Loss:  1.1532309252460424\n",
      "Epoch:  52 Train Loss:  1.172883980455681\n",
      "Epoch:  52 Val Loss:  1.147527128888488\n",
      "Epoch:  53 Train Loss:  1.1669212226177985\n",
      "Epoch:  53 Val Loss:  1.141932587952367\n",
      "Epoch:  54 Train Loss:  1.1610673272641452\n",
      "Epoch:  54 Val Loss:  1.136443793665505\n",
      "Epoch:  55 Train Loss:  1.1553190303200849\n",
      "Epoch:  55 Val Loss:  1.1310573625695437\n",
      "Epoch:  56 Train Loss:  1.1496731567530394\n",
      "Epoch:  56 Val Loss:  1.1257700340717967\n",
      "Epoch:  57 Train Loss:  1.1441266200360982\n",
      "Epoch:  57 Val Loss:  1.1205786680562904\n",
      "Epoch:  58 Train Loss:  1.1386764240152794\n",
      "Epoch:  58 Val Loss:  1.1154802423970882\n",
      "Epoch:  59 Train Loss:  1.1333196661770055\n",
      "Epoch:  59 Val Loss:  1.1104718502847073\n",
      "Epoch:  60 Train Loss:  1.1280535415000874\n",
      "Epoch:  60 Val Loss:  1.105550697335285\n",
      "Epoch:  61 Train Loss:  1.1228753462756955\n",
      "Epoch:  61 Val Loss:  1.1007140984866328\n",
      "Epoch:  62 Train Loss:  1.1177824814476305\n",
      "Epoch:  62 Val Loss:  1.095959474698775\n",
      "Epoch:  63 Train Loss:  1.112772455158814\n",
      "Epoch:  63 Val Loss:  1.0912843494841082\n",
      "Epoch:  64 Train Loss:  1.1078428842973629\n",
      "Epoch:  64 Val Loss:  1.0866863453076674\n",
      "Epoch:  65 Train Loss:  1.1029914949245574\n",
      "Epoch:  65 Val Loss:  1.0821631799247866\n",
      "Epoch:  66 Train Loss:  1.098216121540456\n",
      "Epoch:  66 Val Loss:  1.0777126627531908\n",
      "Epoch:  67 Train Loss:  1.0935147052012386\n",
      "Epoch:  67 Val Loss:  1.0733326913941983\n",
      "Epoch:  68 Train Loss:  1.0888852905469928\n",
      "Epoch:  68 Val Loss:  1.069021248410088\n",
      "Epoch:  69 Train Loss:  1.084326021832214\n",
      "Epoch:  69 Val Loss:  1.0647763984280947\n",
      "Epoch:  70 Train Loss:  1.079835138077322\n",
      "Epoch:  70 Val Loss:  1.060596285583716\n",
      "Epoch:  71 Train Loss:  1.0754109674808459\n",
      "Epoch:  71 Val Loss:  1.0564791312537412\n",
      "Epoch:  72 Train Loss:  1.071051921250376\n",
      "Epoch:  72 Val Loss:  1.0524232319807498\n",
      "Epoch:  73 Train Loss:  1.0667564870263453\n",
      "Epoch:  73 Val Loss:  1.0484269574687013\n",
      "Epoch:  74 Train Loss:  1.0625232220854048\n",
      "Epoch:  74 Val Loss:  1.0444887485366723\n",
      "Epoch:  75 Train Loss:  1.05835074651729\n",
      "Epoch:  75 Val Loss:  1.0406071149482596\n",
      "Epoch:  76 Train Loss:  1.0542377365669482\n",
      "Epoch:  76 Val Loss:  1.0367806330749108\n",
      "Epoch:  77 Train Loss:  1.050182918318074\n",
      "Epoch:  77 Val Loss:  1.0330079433887527\n",
      "Epoch:  78 Train Loss:  1.0461850618622894\n",
      "Epoch:  78 Val Loss:  1.0292877478045968\n",
      "Epoch:  79 Train Loss:  1.042242976051179\n",
      "Epoch:  79 Val Loss:  1.0256188068981542\n",
      "Epoch:  80 Train Loss:  1.0383555038720857\n",
      "Epoch:  80 Val Loss:  1.0219999370211537\n",
      "Epoch:  81 Train Loss:  1.034521518433087\n",
      "Epoch:  81 Val Loss:  1.0184300073210126\n",
      "Epoch:  82 Train Loss:  1.0307399194984075\n",
      "Epoch:  82 Val Loss:  1.0149079366609832\n",
      "Epoch:  83 Train Loss:  1.0270096304906966\n",
      "Epoch:  83 Val Loss:  1.0114326904322062\n",
      "Epoch:  84 Train Loss:  1.0233295958735324\n",
      "Epoch:  84 Val Loss:  1.0080032772542855\n",
      "Epoch:  85 Train Loss:  1.0196987788431993\n",
      "Epoch:  85 Val Loss:  1.0046187455748283\n",
      "Epoch:  86 Train Loss:  1.016116159286187\n",
      "Epoch:  86 Val Loss:  1.001278180197423\n",
      "Epoch:  87 Train Loss:  1.0125807319894589\n",
      "Epoch:  87 Val Loss:  0.9979806987873925\n",
      "Epoch:  88 Train Loss:  1.009091505116812\n",
      "Epoch:  88 Val Loss:  0.9947254484212721\n",
      "Epoch:  89 Train Loss:  1.0056474989813327\n",
      "Epoch:  89 Val Loss:  0.9915116022562617\n",
      "Epoch:  90 Train Loss:  1.0022477451486558\n",
      "Epoch:  90 Val Loss:  0.9883383563982381\n",
      "Epoch:  91 Train Loss:  0.9988912858988209\n",
      "Epoch:  91 Val Loss:  0.9852049270410689\n",
      "Epoch:  92 Train Loss:  0.9955771740583733\n",
      "Epoch:  92 Val Loss:  0.9821105479369063\n",
      "Epoch:  93 Train Loss:  0.992304473192787\n",
      "Epoch:  93 Val Loss:  0.979054468238878\n",
      "Epoch:  94 Train Loss:  0.9890722581265056\n",
      "Epoch:  94 Val Loss:  0.9760359507365763\n",
      "Epoch:  95 Train Loss:  0.985879615737863\n",
      "Epoch:  95 Val Loss:  0.9730542704836062\n",
      "Epoch:  96 Train Loss:  0.9827256459619502\n",
      "Epoch:  96 Val Loss:  0.9701087137974664\n",
      "Epoch:  97 Train Loss:  0.9796094629275738\n",
      "Epoch:  97 Val Loss:  0.9671985775968145\n",
      "Epoch:  98 Train Loss:  0.9765301961551551\n",
      "Epoch:  98 Val Loss:  0.96432316903053\n",
      "Epoch:  99 Train Loss:  0.9734869917493818\n",
      "Epoch:  99 Val Loss:  0.9614818053469723\n",
      "Training completed\n",
      "tanh zero\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  2.3025850929940437\n",
      "Epoch:  0 Val Loss:  2.302585092994046\n",
      "Epoch:  1 Train Loss:  2.3025850929940437\n",
      "Epoch:  1 Val Loss:  2.302585092994046\n",
      "Epoch:  2 Train Loss:  2.3025850929940437\n",
      "Epoch:  2 Val Loss:  2.302585092994046\n",
      "Epoch:  3 Train Loss:  2.3025850929940437\n",
      "Epoch:  3 Val Loss:  2.302585092994046\n",
      "Epoch:  4 Train Loss:  2.3025850929940437\n",
      "Epoch:  4 Val Loss:  2.302585092994046\n",
      "Epoch:  5 Train Loss:  2.3025850929940437\n",
      "Epoch:  5 Val Loss:  2.302585092994046\n",
      "Epoch:  6 Train Loss:  2.3025850929940437\n",
      "Epoch:  6 Val Loss:  2.302585092994046\n",
      "Epoch:  7 Train Loss:  2.3025850929940437\n",
      "Epoch:  7 Val Loss:  2.302585092994046\n",
      "Epoch:  8 Train Loss:  2.3025850929940437\n",
      "Epoch:  8 Val Loss:  2.302585092994046\n",
      "Epoch:  9 Train Loss:  2.3025850929940437\n",
      "Epoch:  9 Val Loss:  2.302585092994046\n",
      "Epoch:  10 Train Loss:  2.3025850929940437\n",
      "Epoch:  10 Val Loss:  2.302585092994046\n",
      "Epoch:  11 Train Loss:  2.3025850929940437\n",
      "Epoch:  11 Val Loss:  2.302585092994046\n",
      "Epoch:  12 Train Loss:  2.3025850929940437\n",
      "Epoch:  12 Val Loss:  2.302585092994046\n",
      "Epoch:  13 Train Loss:  2.3025850929940437\n",
      "Epoch:  13 Val Loss:  2.302585092994046\n",
      "Epoch:  14 Train Loss:  2.3025850929940437\n",
      "Epoch:  14 Val Loss:  2.302585092994046\n",
      "Epoch:  15 Train Loss:  2.3025850929940437\n",
      "Epoch:  15 Val Loss:  2.302585092994046\n",
      "Epoch:  16 Train Loss:  2.3025850929940437\n",
      "Epoch:  16 Val Loss:  2.302585092994046\n",
      "Epoch:  17 Train Loss:  2.3025850929940437\n",
      "Epoch:  17 Val Loss:  2.302585092994046\n",
      "Epoch:  18 Train Loss:  2.3025850929940437\n",
      "Epoch:  18 Val Loss:  2.302585092994046\n",
      "Epoch:  19 Train Loss:  2.3025850929940437\n",
      "Epoch:  19 Val Loss:  2.302585092994046\n",
      "Epoch:  20 Train Loss:  2.3025850929940437\n",
      "Epoch:  20 Val Loss:  2.302585092994046\n",
      "Epoch:  21 Train Loss:  2.3025850929940437\n",
      "Epoch:  21 Val Loss:  2.302585092994046\n",
      "Epoch:  22 Train Loss:  2.3025850929940437\n",
      "Epoch:  22 Val Loss:  2.302585092994046\n",
      "Epoch:  23 Train Loss:  2.3025850929940437\n",
      "Epoch:  23 Val Loss:  2.302585092994046\n",
      "Epoch:  24 Train Loss:  2.3025850929940437\n",
      "Epoch:  24 Val Loss:  2.302585092994046\n",
      "Epoch:  25 Train Loss:  2.3025850929940437\n",
      "Epoch:  25 Val Loss:  2.302585092994046\n",
      "Epoch:  26 Train Loss:  2.3025850929940437\n",
      "Epoch:  26 Val Loss:  2.302585092994046\n",
      "Epoch:  27 Train Loss:  2.3025850929940437\n",
      "Epoch:  27 Val Loss:  2.302585092994046\n",
      "Epoch:  28 Train Loss:  2.3025850929940437\n",
      "Epoch:  28 Val Loss:  2.302585092994046\n",
      "Epoch:  29 Train Loss:  2.3025850929940437\n",
      "Epoch:  29 Val Loss:  2.302585092994046\n",
      "Epoch:  30 Train Loss:  2.3025850929940437\n",
      "Epoch:  30 Val Loss:  2.302585092994046\n",
      "Epoch:  31 Train Loss:  2.3025850929940437\n",
      "Epoch:  31 Val Loss:  2.302585092994046\n",
      "Epoch:  32 Train Loss:  2.3025850929940437\n",
      "Epoch:  32 Val Loss:  2.302585092994046\n",
      "Epoch:  33 Train Loss:  2.3025850929940437\n",
      "Epoch:  33 Val Loss:  2.302585092994046\n",
      "Epoch:  34 Train Loss:  2.3025850929940437\n",
      "Epoch:  34 Val Loss:  2.302585092994046\n",
      "Epoch:  35 Train Loss:  2.3025850929940437\n",
      "Epoch:  35 Val Loss:  2.302585092994046\n",
      "Epoch:  36 Train Loss:  2.3025850929940437\n",
      "Epoch:  36 Val Loss:  2.302585092994046\n",
      "Epoch:  37 Train Loss:  2.3025850929940437\n",
      "Epoch:  37 Val Loss:  2.302585092994046\n",
      "Epoch:  38 Train Loss:  2.3025850929940437\n",
      "Epoch:  38 Val Loss:  2.302585092994046\n",
      "Epoch:  39 Train Loss:  2.3025850929940437\n",
      "Epoch:  39 Val Loss:  2.302585092994046\n",
      "Epoch:  40 Train Loss:  2.3025850929940437\n",
      "Epoch:  40 Val Loss:  2.302585092994046\n",
      "Epoch:  41 Train Loss:  2.3025850929940437\n",
      "Epoch:  41 Val Loss:  2.302585092994046\n",
      "Epoch:  42 Train Loss:  2.3025850929940437\n",
      "Epoch:  42 Val Loss:  2.302585092994046\n",
      "Epoch:  43 Train Loss:  2.3025850929940437\n",
      "Epoch:  43 Val Loss:  2.302585092994046\n",
      "Epoch:  44 Train Loss:  2.3025850929940437\n",
      "Epoch:  44 Val Loss:  2.302585092994046\n",
      "Epoch:  45 Train Loss:  2.3025850929940437\n",
      "Epoch:  45 Val Loss:  2.302585092994046\n",
      "Epoch:  46 Train Loss:  2.3025850929940437\n",
      "Epoch:  46 Val Loss:  2.302585092994046\n",
      "Epoch:  47 Train Loss:  2.3025850929940437\n",
      "Epoch:  47 Val Loss:  2.302585092994046\n",
      "Epoch:  48 Train Loss:  2.3025850929940437\n",
      "Epoch:  48 Val Loss:  2.302585092994046\n",
      "Epoch:  49 Train Loss:  2.3025850929940437\n",
      "Epoch:  49 Val Loss:  2.302585092994046\n",
      "Epoch:  50 Train Loss:  2.3025850929940437\n",
      "Epoch:  50 Val Loss:  2.302585092994046\n",
      "Epoch:  51 Train Loss:  2.3025850929940437\n",
      "Epoch:  51 Val Loss:  2.302585092994046\n",
      "Epoch:  52 Train Loss:  2.3025850929940437\n",
      "Epoch:  52 Val Loss:  2.302585092994046\n",
      "Epoch:  53 Train Loss:  2.3025850929940437\n",
      "Epoch:  53 Val Loss:  2.302585092994046\n",
      "Epoch:  54 Train Loss:  2.3025850929940437\n",
      "Epoch:  54 Val Loss:  2.302585092994046\n",
      "Epoch:  55 Train Loss:  2.3025850929940437\n",
      "Epoch:  55 Val Loss:  2.302585092994046\n",
      "Epoch:  56 Train Loss:  2.3025850929940437\n",
      "Epoch:  56 Val Loss:  2.302585092994046\n",
      "Epoch:  57 Train Loss:  2.3025850929940437\n",
      "Epoch:  57 Val Loss:  2.302585092994046\n",
      "Epoch:  58 Train Loss:  2.3025850929940437\n",
      "Epoch:  58 Val Loss:  2.302585092994046\n",
      "Epoch:  59 Train Loss:  2.3025850929940437\n",
      "Epoch:  59 Val Loss:  2.302585092994046\n",
      "Epoch:  60 Train Loss:  2.3025850929940437\n",
      "Epoch:  60 Val Loss:  2.302585092994046\n",
      "Epoch:  61 Train Loss:  2.3025850929940437\n",
      "Epoch:  61 Val Loss:  2.302585092994046\n",
      "Epoch:  62 Train Loss:  2.3025850929940437\n",
      "Epoch:  62 Val Loss:  2.302585092994046\n",
      "Epoch:  63 Train Loss:  2.3025850929940437\n",
      "Epoch:  63 Val Loss:  2.302585092994046\n",
      "Epoch:  64 Train Loss:  2.3025850929940437\n",
      "Epoch:  64 Val Loss:  2.302585092994046\n",
      "Epoch:  65 Train Loss:  2.3025850929940437\n",
      "Epoch:  65 Val Loss:  2.302585092994046\n",
      "Epoch:  66 Train Loss:  2.3025850929940437\n",
      "Epoch:  66 Val Loss:  2.302585092994046\n",
      "Epoch:  67 Train Loss:  2.3025850929940437\n",
      "Epoch:  67 Val Loss:  2.302585092994046\n",
      "Epoch:  68 Train Loss:  2.3025850929940437\n",
      "Epoch:  68 Val Loss:  2.302585092994046\n",
      "Epoch:  69 Train Loss:  2.3025850929940437\n",
      "Epoch:  69 Val Loss:  2.302585092994046\n",
      "Epoch:  70 Train Loss:  2.3025850929940437\n",
      "Epoch:  70 Val Loss:  2.302585092994046\n",
      "Epoch:  71 Train Loss:  2.3025850929940437\n",
      "Epoch:  71 Val Loss:  2.302585092994046\n",
      "Epoch:  72 Train Loss:  2.3025850929940437\n",
      "Epoch:  72 Val Loss:  2.302585092994046\n",
      "Epoch:  73 Train Loss:  2.3025850929940437\n",
      "Epoch:  73 Val Loss:  2.302585092994046\n",
      "Epoch:  74 Train Loss:  2.3025850929940437\n",
      "Epoch:  74 Val Loss:  2.302585092994046\n",
      "Epoch:  75 Train Loss:  2.3025850929940437\n",
      "Epoch:  75 Val Loss:  2.302585092994046\n",
      "Epoch:  76 Train Loss:  2.3025850929940437\n",
      "Epoch:  76 Val Loss:  2.302585092994046\n",
      "Epoch:  77 Train Loss:  2.3025850929940437\n",
      "Epoch:  77 Val Loss:  2.302585092994046\n",
      "Epoch:  78 Train Loss:  2.3025850929940437\n",
      "Epoch:  78 Val Loss:  2.302585092994046\n",
      "Epoch:  79 Train Loss:  2.3025850929940437\n",
      "Epoch:  79 Val Loss:  2.302585092994046\n",
      "Epoch:  80 Train Loss:  2.3025850929940437\n",
      "Epoch:  80 Val Loss:  2.302585092994046\n",
      "Epoch:  81 Train Loss:  2.3025850929940437\n",
      "Epoch:  81 Val Loss:  2.302585092994046\n",
      "Epoch:  82 Train Loss:  2.3025850929940437\n",
      "Epoch:  82 Val Loss:  2.302585092994046\n",
      "Epoch:  83 Train Loss:  2.3025850929940437\n",
      "Epoch:  83 Val Loss:  2.302585092994046\n",
      "Epoch:  84 Train Loss:  2.3025850929940437\n",
      "Epoch:  84 Val Loss:  2.302585092994046\n",
      "Epoch:  85 Train Loss:  2.3025850929940437\n",
      "Epoch:  85 Val Loss:  2.302585092994046\n",
      "Epoch:  86 Train Loss:  2.3025850929940437\n",
      "Epoch:  86 Val Loss:  2.302585092994046\n",
      "Epoch:  87 Train Loss:  2.3025850929940437\n",
      "Epoch:  87 Val Loss:  2.302585092994046\n",
      "Epoch:  88 Train Loss:  2.3025850929940437\n",
      "Epoch:  88 Val Loss:  2.302585092994046\n",
      "Epoch:  89 Train Loss:  2.3025850929940437\n",
      "Epoch:  89 Val Loss:  2.302585092994046\n",
      "Epoch:  90 Train Loss:  2.3025850929940437\n",
      "Epoch:  90 Val Loss:  2.302585092994046\n",
      "Epoch:  91 Train Loss:  2.3025850929940437\n",
      "Epoch:  91 Val Loss:  2.302585092994046\n",
      "Epoch:  92 Train Loss:  2.3025850929940437\n",
      "Epoch:  92 Val Loss:  2.302585092994046\n",
      "Epoch:  93 Train Loss:  2.3025850929940437\n",
      "Epoch:  93 Val Loss:  2.302585092994046\n",
      "Epoch:  94 Train Loss:  2.3025850929940437\n",
      "Epoch:  94 Val Loss:  2.302585092994046\n",
      "Epoch:  95 Train Loss:  2.3025850929940437\n",
      "Epoch:  95 Val Loss:  2.302585092994046\n",
      "Epoch:  96 Train Loss:  2.3025850929940437\n",
      "Epoch:  96 Val Loss:  2.302585092994046\n",
      "Epoch:  97 Train Loss:  2.3025850929940437\n",
      "Epoch:  97 Val Loss:  2.302585092994046\n",
      "Epoch:  98 Train Loss:  2.3025850929940437\n",
      "Epoch:  98 Val Loss:  2.302585092994046\n",
      "Epoch:  99 Train Loss:  2.3025850929940437\n",
      "Epoch:  99 Val Loss:  2.302585092994046\n",
      "Training completed\n",
      "tanh random\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  1.5464881791517524\n",
      "Epoch:  0 Val Loss:  1.5297250588896627\n",
      "Epoch:  1 Train Loss:  1.4062831120046808\n",
      "Epoch:  1 Val Loss:  1.388172678522049\n",
      "Epoch:  2 Train Loss:  1.3115705278894452\n",
      "Epoch:  2 Val Loss:  1.2947752890189022\n",
      "Epoch:  3 Train Loss:  1.236239813285973\n",
      "Epoch:  3 Val Loss:  1.2217822524626933\n",
      "Epoch:  4 Train Loss:  1.1732959888370833\n",
      "Epoch:  4 Val Loss:  1.1614253645768458\n",
      "Epoch:  5 Train Loss:  1.119325466092571\n",
      "Epoch:  5 Val Loss:  1.1101694192950613\n",
      "Epoch:  6 Train Loss:  1.0722995937964637\n",
      "Epoch:  6 Val Loss:  1.0658712678395525\n",
      "Epoch:  7 Train Loss:  1.0308127367526514\n",
      "Epoch:  7 Val Loss:  1.0270168351643445\n",
      "Epoch:  8 Train Loss:  0.9938612215991649\n",
      "Epoch:  8 Val Loss:  0.9925149093704994\n",
      "Epoch:  9 Train Loss:  0.9606897036446661\n",
      "Epoch:  9 Val Loss:  0.9615913972778007\n",
      "Epoch:  10 Train Loss:  0.9307086492276287\n",
      "Epoch:  10 Val Loss:  0.9336844765518958\n",
      "Epoch:  11 Train Loss:  0.903457211355135\n",
      "Epoch:  11 Val Loss:  0.9083588683598093\n",
      "Epoch:  12 Train Loss:  0.8785751545297263\n",
      "Epoch:  12 Val Loss:  0.8852604813573184\n",
      "Epoch:  13 Train Loss:  0.8557663855680433\n",
      "Epoch:  13 Val Loss:  0.86409654124342\n",
      "Epoch:  14 Train Loss:  0.8347720885860346\n",
      "Epoch:  14 Val Loss:  0.8446221743713551\n",
      "Epoch:  15 Train Loss:  0.8153627174238223\n",
      "Epoch:  15 Val Loss:  0.8266320891583345\n",
      "Epoch:  16 Train Loss:  0.7973395695938194\n",
      "Epoch:  16 Val Loss:  0.8099539530045794\n",
      "Epoch:  17 Train Loss:  0.7805370710369983\n",
      "Epoch:  17 Val Loss:  0.7944405213901179\n",
      "Epoch:  18 Train Loss:  0.7648202254250333\n",
      "Epoch:  18 Val Loss:  0.7799641878763104\n",
      "Epoch:  19 Train Loss:  0.7500767540054346\n",
      "Epoch:  19 Val Loss:  0.7664144634236871\n",
      "Epoch:  20 Train Loss:  0.7362091538750227\n",
      "Epoch:  20 Val Loss:  0.7536958561691097\n",
      "Epoch:  21 Train Loss:  0.7231306242821856\n",
      "Epoch:  21 Val Loss:  0.7417251888930707\n",
      "Epoch:  22 Train Loss:  0.7107641451537389\n",
      "Epoch:  22 Val Loss:  0.7304290043216202\n",
      "Epoch:  23 Train Loss:  0.6990422387002758\n",
      "Epoch:  23 Val Loss:  0.7197417437045222\n",
      "Epoch:  24 Train Loss:  0.6879061112188356\n",
      "Epoch:  24 Val Loss:  0.7096048266881811\n",
      "Epoch:  25 Train Loss:  0.6773043162821499\n",
      "Epoch:  25 Val Loss:  0.6999663303084733\n",
      "Epoch:  26 Train Loss:  0.6671914664432134\n",
      "Epoch:  26 Val Loss:  0.6907808250697552\n",
      "Epoch:  27 Train Loss:  0.6575272196532872\n",
      "Epoch:  27 Val Loss:  0.6820090640498021\n",
      "Epoch:  28 Train Loss:  0.6482755519798891\n",
      "Epoch:  28 Val Loss:  0.6736174649777084\n",
      "Epoch:  29 Train Loss:  0.6394042854612908\n",
      "Epoch:  29 Val Loss:  0.665577464189918\n",
      "Epoch:  30 Train Loss:  0.6308847824738635\n",
      "Epoch:  30 Val Loss:  0.657864821287649\n",
      "Epoch:  31 Train Loss:  0.6226916760824026\n",
      "Epoch:  31 Val Loss:  0.6504589090754794\n",
      "Epoch:  32 Train Loss:  0.6148025480601902\n",
      "Epoch:  32 Val Loss:  0.6433420048754009\n",
      "Epoch:  33 Train Loss:  0.6071975468078593\n",
      "Epoch:  33 Val Loss:  0.6364986096174725\n",
      "Epoch:  34 Train Loss:  0.5998589903960109\n",
      "Epoch:  34 Val Loss:  0.6299148356401961\n",
      "Epoch:  35 Train Loss:  0.5927710038685464\n",
      "Epoch:  35 Val Loss:  0.6235779024247042\n",
      "Epoch:  36 Train Loss:  0.5859192151525241\n",
      "Epoch:  36 Val Loss:  0.6174757605850738\n",
      "Epoch:  37 Train Loss:  0.5792905121821563\n",
      "Epoch:  37 Val Loss:  0.6115968448078605\n",
      "Epoch:  38 Train Loss:  0.5728728532815823\n",
      "Epoch:  38 Val Loss:  0.605929945112911\n",
      "Epoch:  39 Train Loss:  0.5666551170264252\n",
      "Epoch:  39 Val Loss:  0.6004641773570507\n",
      "Epoch:  40 Train Loss:  0.5606269776568005\n",
      "Epoch:  40 Val Loss:  0.595189025517365\n",
      "Epoch:  41 Train Loss:  0.5547787993812646\n",
      "Epoch:  41 Val Loss:  0.5900944239016824\n",
      "Epoch:  42 Train Loss:  0.5491015515266616\n",
      "Epoch:  42 Val Loss:  0.5851708493957679\n",
      "Epoch:  43 Train Loss:  0.5435867490865073\n",
      "Epoch:  43 Val Loss:  0.5804094007179781\n",
      "Epoch:  44 Train Loss:  0.5382264186071014\n",
      "Epoch:  44 Val Loss:  0.5758018504677107\n",
      "Epoch:  45 Train Loss:  0.5330130823389027\n",
      "Epoch:  45 Val Loss:  0.571340663771698\n",
      "Epoch:  46 Train Loss:  0.5279397496659174\n",
      "Epoch:  46 Val Loss:  0.5670189831857994\n",
      "Epoch:  47 Train Loss:  0.5229999057780911\n",
      "Epoch:  47 Val Loss:  0.5628305835352142\n",
      "Epoch:  48 Train Loss:  0.5181874916324276\n",
      "Epoch:  48 Val Loss:  0.5587698035360218\n",
      "Epoch:  49 Train Loss:  0.5134968739511007\n",
      "Epoch:  49 Val Loss:  0.5548314636040929\n",
      "Epoch:  50 Train Loss:  0.5089228076998247\n",
      "Epoch:  50 Val Loss:  0.551010780493241\n",
      "Epoch:  51 Train Loss:  0.5044603954538561\n",
      "Epoch:  51 Val Loss:  0.5473032883819987\n",
      "Epoch:  52 Train Loss:  0.500105048175576\n",
      "Epoch:  52 Val Loss:  0.5437047727108429\n",
      "Epoch:  53 Train Loss:  0.49585245060511746\n",
      "Epoch:  53 Val Loss:  0.5402112187193596\n",
      "Epoch:  54 Train Loss:  0.4916985325112587\n",
      "Epoch:  54 Val Loss:  0.5368187730938301\n",
      "Epoch:  55 Train Loss:  0.487639445349701\n",
      "Epoch:  55 Val Loss:  0.5335237156165193\n",
      "Epoch:  56 Train Loss:  0.4836715430353014\n",
      "Epoch:  56 Val Loss:  0.5303224380991878\n",
      "Epoch:  57 Train Loss:  0.4797913656587018\n",
      "Epoch:  57 Val Loss:  0.5272114291243747\n",
      "Epoch:  58 Train Loss:  0.4759956256704103\n",
      "Epoch:  58 Val Loss:  0.5241872641161338\n",
      "Epoch:  59 Train Loss:  0.4722811966633631\n",
      "Epoch:  59 Val Loss:  0.5212466005497824\n",
      "Epoch:  60 Train Loss:  0.4686451049131191\n",
      "Epoch:  60 Val Loss:  0.5183861778548502\n",
      "Epoch:  61 Train Loss:  0.46508452327979394\n",
      "Epoch:  61 Val Loss:  0.515602821156157\n",
      "Epoch:  62 Train Loss:  0.46159676638359814\n",
      "Epoch:  62 Val Loss:  0.5128934476902357\n",
      "Epoch:  63 Train Loss:  0.45817928563450677\n",
      "Epoch:  63 Val Loss:  0.5102550745961607\n",
      "Epoch:  64 Train Loss:  0.4548296629110879\n",
      "Epoch:  64 Val Loss:  0.5076848268106985\n",
      "Epoch:  65 Train Loss:  0.45154560228492163\n",
      "Epoch:  65 Val Loss:  0.5051799439955977\n",
      "Epoch:  66 Train Loss:  0.4483249198869623\n",
      "Epoch:  66 Val Loss:  0.5027377857627622\n",
      "Epoch:  67 Train Loss:  0.44516553258654845\n",
      "Epoch:  67 Val Loss:  0.5003558348666297\n",
      "Epoch:  68 Train Loss:  0.44206544649531976\n",
      "Epoch:  68 Val Loss:  0.4980316984045237\n",
      "Epoch:  69 Train Loss:  0.439022746389814\n",
      "Epoch:  69 Val Loss:  0.49576310732408446\n",
      "Epoch:  70 Train Loss:  0.4360355869826959\n",
      "Epoch:  70 Val Loss:  0.4935479146422711\n",
      "Epoch:  71 Train Loss:  0.4331021866126021\n",
      "Epoch:  71 Val Loss:  0.4913840927389971\n",
      "Epoch:  72 Train Loss:  0.43022082345664076\n",
      "Epoch:  72 Val Loss:  0.4892697299466057\n",
      "Epoch:  73 Train Loss:  0.4273898339187433\n",
      "Epoch:  73 Val Loss:  0.48720302648642355\n",
      "Epoch:  74 Train Loss:  0.42460761252757573\n",
      "Epoch:  74 Val Loss:  0.4851822896793138\n",
      "Epoch:  75 Train Loss:  0.4218726125568873\n",
      "Epoch:  75 Val Loss:  0.48320592832592363\n",
      "Epoch:  76 Train Loss:  0.4191833466542808\n",
      "Epoch:  76 Val Loss:  0.4812724462177862\n",
      "Epoch:  77 Train Loss:  0.41653838696726453\n",
      "Epoch:  77 Val Loss:  0.4793804348667478\n",
      "Epoch:  78 Train Loss:  0.4139363645019366\n",
      "Epoch:  78 Val Loss:  0.477528565674311\n",
      "Epoch:  79 Train Loss:  0.41137596766879914\n",
      "Epoch:  79 Val Loss:  0.47571558186005813\n",
      "Epoch:  80 Train Loss:  0.4088559401246496\n",
      "Epoch:  80 Val Loss:  0.4739402905079638\n",
      "Epoch:  81 Train Loss:  0.40637507810289697\n",
      "Epoch:  81 Val Loss:  0.47220155507194483\n",
      "Epoch:  82 Train Loss:  0.40393222744869894\n",
      "Epoch:  82 Val Loss:  0.4704982886207155\n",
      "Epoch:  83 Train Loss:  0.40152628055748457\n",
      "Epoch:  83 Val Loss:  0.4688294480133564\n",
      "Epoch:  84 Train Loss:  0.3991561733731485\n",
      "Epoch:  84 Val Loss:  0.4671940290963411\n",
      "Epoch:  85 Train Loss:  0.39682088255114156\n",
      "Epoch:  85 Val Loss:  0.4655910629144\n",
      "Epoch:  86 Train Loss:  0.39451942284512764\n",
      "Epoch:  86 Val Loss:  0.46401961284490617\n",
      "Epoch:  87 Train Loss:  0.39225084474290234\n",
      "Epoch:  87 Val Loss:  0.4624787725091259\n",
      "Epoch:  88 Train Loss:  0.39001423236061383\n",
      "Epoch:  88 Val Loss:  0.4609676642888075\n",
      "Epoch:  89 Train Loss:  0.38780870160052566\n",
      "Epoch:  89 Val Loss:  0.45948543828119454\n",
      "Epoch:  90 Train Loss:  0.38563339857906775\n",
      "Epoch:  90 Val Loss:  0.45803127155159673\n",
      "Epoch:  91 Train Loss:  0.3834874983312203\n",
      "Epoch:  91 Val Loss:  0.45660436757919154\n",
      "Epoch:  92 Train Loss:  0.3813702037900196\n",
      "Epoch:  92 Val Loss:  0.45520395582847506\n",
      "Epoch:  93 Train Loss:  0.3792807450261901\n",
      "Epoch:  93 Val Loss:  0.4538292914083945\n",
      "Epoch:  94 Train Loss:  0.37721837871601815\n",
      "Epoch:  94 Val Loss:  0.4524796548002493\n",
      "Epoch:  95 Train Loss:  0.3751823877901705\n",
      "Epoch:  95 Val Loss:  0.4511543516438359\n",
      "Epoch:  96 Train Loss:  0.3731720812058167\n",
      "Epoch:  96 Val Loss:  0.4498527125712148\n",
      "Epoch:  97 Train Loss:  0.3711867937806735\n",
      "Epoch:  97 Val Loss:  0.44857409307216456\n",
      "Epoch:  98 Train Loss:  0.36922588602994194\n",
      "Epoch:  98 Val Loss:  0.4473178733681761\n",
      "Epoch:  99 Train Loss:  0.3672887439538604\n",
      "Epoch:  99 Val Loss:  0.44608345826547774\n",
      "Training completed\n",
      "tanh normal\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  2.0157711320080054\n",
      "Epoch:  0 Val Loss:  2.0578527860358236\n",
      "Epoch:  1 Train Loss:  1.7188254026089211\n",
      "Epoch:  1 Val Loss:  1.7398609015216808\n",
      "Epoch:  2 Train Loss:  1.6868355100613774\n",
      "Epoch:  2 Val Loss:  1.706622083082292\n",
      "Epoch:  3 Train Loss:  1.6739904953701983\n",
      "Epoch:  3 Val Loss:  1.6976634626718874\n",
      "Epoch:  4 Train Loss:  1.6635058206450462\n",
      "Epoch:  4 Val Loss:  1.6923852512249693\n",
      "Epoch:  5 Train Loss:  1.6537861304441297\n",
      "Epoch:  5 Val Loss:  1.6882645185576353\n",
      "Epoch:  6 Train Loss:  1.6446232886377978\n",
      "Epoch:  6 Val Loss:  1.684413850786994\n",
      "Epoch:  7 Train Loss:  1.635919977460411\n",
      "Epoch:  7 Val Loss:  1.6806978780273407\n",
      "Epoch:  8 Train Loss:  1.6276112032250445\n",
      "Epoch:  8 Val Loss:  1.6771880328613287\n",
      "Epoch:  9 Train Loss:  1.6196308333832488\n",
      "Epoch:  9 Val Loss:  1.6738177242020305\n",
      "Epoch:  10 Train Loss:  1.6118739224638519\n",
      "Epoch:  10 Val Loss:  1.670547204460835\n",
      "Epoch:  11 Train Loss:  1.6043271236862684\n",
      "Epoch:  11 Val Loss:  1.6672921603996445\n",
      "Epoch:  12 Train Loss:  1.5969887606604314\n",
      "Epoch:  12 Val Loss:  1.664017292932872\n",
      "Epoch:  13 Train Loss:  1.5898395462426425\n",
      "Epoch:  13 Val Loss:  1.6608203385623306\n",
      "Epoch:  14 Train Loss:  1.5829565048571796\n",
      "Epoch:  14 Val Loss:  1.6577962315159638\n",
      "Epoch:  15 Train Loss:  1.576317568981663\n",
      "Epoch:  15 Val Loss:  1.6549513142528798\n",
      "Epoch:  16 Train Loss:  1.5698694283385157\n",
      "Epoch:  16 Val Loss:  1.652205186418334\n",
      "Epoch:  17 Train Loss:  1.56357285330774\n",
      "Epoch:  17 Val Loss:  1.6495385567739889\n",
      "Epoch:  18 Train Loss:  1.5574178213554175\n",
      "Epoch:  18 Val Loss:  1.646980651341186\n",
      "Epoch:  19 Train Loss:  1.5514151895749315\n",
      "Epoch:  19 Val Loss:  1.644587110044074\n",
      "Epoch:  20 Train Loss:  1.5455730964932255\n",
      "Epoch:  20 Val Loss:  1.6423653260822209\n",
      "Epoch:  21 Train Loss:  1.539847978349799\n",
      "Epoch:  21 Val Loss:  1.64030034290801\n",
      "Epoch:  22 Train Loss:  1.5342473514818569\n",
      "Epoch:  22 Val Loss:  1.6383456344646\n",
      "Epoch:  23 Train Loss:  1.5287740739941134\n",
      "Epoch:  23 Val Loss:  1.6364347771876873\n",
      "Epoch:  24 Train Loss:  1.5234128598608845\n",
      "Epoch:  24 Val Loss:  1.6345462481362159\n",
      "Epoch:  25 Train Loss:  1.5181722515392453\n",
      "Epoch:  25 Val Loss:  1.6326737407641743\n",
      "Epoch:  26 Train Loss:  1.513046844005862\n",
      "Epoch:  26 Val Loss:  1.6308175712460293\n",
      "Epoch:  27 Train Loss:  1.5080255606460817\n",
      "Epoch:  27 Val Loss:  1.628989195687778\n",
      "Epoch:  28 Train Loss:  1.5030896156119924\n",
      "Epoch:  28 Val Loss:  1.6271911909385548\n",
      "Epoch:  29 Train Loss:  1.4982316957312678\n",
      "Epoch:  29 Val Loss:  1.6254093184933562\n",
      "Epoch:  30 Train Loss:  1.49345761492037\n",
      "Epoch:  30 Val Loss:  1.623660430647464\n",
      "Epoch:  31 Train Loss:  1.4887597395093266\n",
      "Epoch:  31 Val Loss:  1.6219512085114594\n",
      "Epoch:  32 Train Loss:  1.484142175236152\n",
      "Epoch:  32 Val Loss:  1.6202778791265045\n",
      "Epoch:  33 Train Loss:  1.4796241650703574\n",
      "Epoch:  33 Val Loss:  1.6186390211815753\n",
      "Epoch:  34 Train Loss:  1.4751909306495496\n",
      "Epoch:  34 Val Loss:  1.617005885075534\n",
      "Epoch:  35 Train Loss:  1.4708231646611085\n",
      "Epoch:  35 Val Loss:  1.6153677105606155\n",
      "Epoch:  36 Train Loss:  1.4665253624453\n",
      "Epoch:  36 Val Loss:  1.6137461945825404\n",
      "Epoch:  37 Train Loss:  1.4623127321388238\n",
      "Epoch:  37 Val Loss:  1.6121516019425148\n",
      "Epoch:  38 Train Loss:  1.4581664494026914\n",
      "Epoch:  38 Val Loss:  1.6105785011221436\n",
      "Epoch:  39 Train Loss:  1.4540834068559116\n",
      "Epoch:  39 Val Loss:  1.6090226828087062\n",
      "Epoch:  40 Train Loss:  1.4500511798085636\n",
      "Epoch:  40 Val Loss:  1.6074848345394006\n",
      "Epoch:  41 Train Loss:  1.4460654729119786\n",
      "Epoch:  41 Val Loss:  1.6059742260135546\n",
      "Epoch:  42 Train Loss:  1.4421264684978317\n",
      "Epoch:  42 Val Loss:  1.604500924763438\n",
      "Epoch:  43 Train Loss:  1.438235606953788\n",
      "Epoch:  43 Val Loss:  1.6030709926771072\n",
      "Epoch:  44 Train Loss:  1.4343988858607193\n",
      "Epoch:  44 Val Loss:  1.601679672437945\n",
      "Epoch:  45 Train Loss:  1.4306162140983834\n",
      "Epoch:  45 Val Loss:  1.6003102218597085\n",
      "Epoch:  46 Train Loss:  1.4268862898696466\n",
      "Epoch:  46 Val Loss:  1.5989629394460616\n",
      "Epoch:  47 Train Loss:  1.4232028168407744\n",
      "Epoch:  47 Val Loss:  1.5976356938374607\n",
      "Epoch:  48 Train Loss:  1.4195572679892694\n",
      "Epoch:  48 Val Loss:  1.5963314845861691\n",
      "Epoch:  49 Train Loss:  1.4159455235521288\n",
      "Epoch:  49 Val Loss:  1.5950526050056406\n",
      "Epoch:  50 Train Loss:  1.4123660697860627\n",
      "Epoch:  50 Val Loss:  1.5937973002758892\n",
      "Epoch:  51 Train Loss:  1.4088142786793\n",
      "Epoch:  51 Val Loss:  1.5925621564023908\n",
      "Epoch:  52 Train Loss:  1.405284748011253\n",
      "Epoch:  52 Val Loss:  1.5913442921037266\n",
      "Epoch:  53 Train Loss:  1.4017859305529612\n",
      "Epoch:  53 Val Loss:  1.5901422447719489\n",
      "Epoch:  54 Train Loss:  1.3983309428820452\n",
      "Epoch:  54 Val Loss:  1.5889577816918874\n",
      "Epoch:  55 Train Loss:  1.3949144998044118\n",
      "Epoch:  55 Val Loss:  1.587789494355562\n",
      "Epoch:  56 Train Loss:  1.3915412821678463\n",
      "Epoch:  56 Val Loss:  1.5866400852847196\n",
      "Epoch:  57 Train Loss:  1.388213479490007\n",
      "Epoch:  57 Val Loss:  1.585512404760113\n",
      "Epoch:  58 Train Loss:  1.3849259167873131\n",
      "Epoch:  58 Val Loss:  1.5844046845320348\n",
      "Epoch:  59 Train Loss:  1.381673514414428\n",
      "Epoch:  59 Val Loss:  1.5833177727232008\n",
      "Epoch:  60 Train Loss:  1.3784478229114434\n",
      "Epoch:  60 Val Loss:  1.5822472273212862\n",
      "Epoch:  61 Train Loss:  1.3752481448540643\n",
      "Epoch:  61 Val Loss:  1.5811919912858807\n",
      "Epoch:  62 Train Loss:  1.3720738834199802\n",
      "Epoch:  62 Val Loss:  1.5801542507184974\n",
      "Epoch:  63 Train Loss:  1.3689276410685283\n",
      "Epoch:  63 Val Loss:  1.5791367417845164\n",
      "Epoch:  64 Train Loss:  1.365804193803478\n",
      "Epoch:  64 Val Loss:  1.5781211743256525\n",
      "Epoch:  65 Train Loss:  1.3627034775892295\n",
      "Epoch:  65 Val Loss:  1.5771155448229437\n",
      "Epoch:  66 Train Loss:  1.3596258175970188\n",
      "Epoch:  66 Val Loss:  1.5761292720660318\n",
      "Epoch:  67 Train Loss:  1.3565712249819708\n",
      "Epoch:  67 Val Loss:  1.5751578819306897\n",
      "Epoch:  68 Train Loss:  1.353544327484895\n",
      "Epoch:  68 Val Loss:  1.5741974196609072\n",
      "Epoch:  69 Train Loss:  1.3505489894932097\n",
      "Epoch:  69 Val Loss:  1.5732471253110911\n",
      "Epoch:  70 Train Loss:  1.3475826819449428\n",
      "Epoch:  70 Val Loss:  1.5723087804291742\n",
      "Epoch:  71 Train Loss:  1.3446457654428534\n",
      "Epoch:  71 Val Loss:  1.5713815544691803\n",
      "Epoch:  72 Train Loss:  1.3417382683117238\n",
      "Epoch:  72 Val Loss:  1.5704652213960613\n",
      "Epoch:  73 Train Loss:  1.3388608917438325\n",
      "Epoch:  73 Val Loss:  1.5695561847163793\n",
      "Epoch:  74 Train Loss:  1.3360135806319333\n",
      "Epoch:  74 Val Loss:  1.5686534289001755\n",
      "Epoch:  75 Train Loss:  1.3331945482430678\n",
      "Epoch:  75 Val Loss:  1.567757441256165\n",
      "Epoch:  76 Train Loss:  1.3303999718863697\n",
      "Epoch:  76 Val Loss:  1.5668699461483178\n",
      "Epoch:  77 Train Loss:  1.3276262211108814\n",
      "Epoch:  77 Val Loss:  1.5659924637145304\n",
      "Epoch:  78 Train Loss:  1.3248713851460523\n",
      "Epoch:  78 Val Loss:  1.5651289611633061\n",
      "Epoch:  79 Train Loss:  1.3221331777669627\n",
      "Epoch:  79 Val Loss:  1.5642855914607063\n",
      "Epoch:  80 Train Loss:  1.3194122233196377\n",
      "Epoch:  80 Val Loss:  1.5634630510337588\n",
      "Epoch:  81 Train Loss:  1.316717647310604\n",
      "Epoch:  81 Val Loss:  1.5626578176954307\n",
      "Epoch:  82 Train Loss:  1.3140476722929129\n",
      "Epoch:  82 Val Loss:  1.5618732270922602\n",
      "Epoch:  83 Train Loss:  1.3113986573585692\n",
      "Epoch:  83 Val Loss:  1.5611113094945828\n",
      "Epoch:  84 Train Loss:  1.3087677030481377\n",
      "Epoch:  84 Val Loss:  1.5603708590125935\n",
      "Epoch:  85 Train Loss:  1.3061520148209984\n",
      "Epoch:  85 Val Loss:  1.5596497911033995\n",
      "Epoch:  86 Train Loss:  1.3035528910104135\n",
      "Epoch:  86 Val Loss:  1.5589432680677178\n",
      "Epoch:  87 Train Loss:  1.3009733667062222\n",
      "Epoch:  87 Val Loss:  1.5582471679235856\n",
      "Epoch:  88 Train Loss:  1.298416119032226\n",
      "Epoch:  88 Val Loss:  1.5575638303510615\n",
      "Epoch:  89 Train Loss:  1.2958815787077145\n",
      "Epoch:  89 Val Loss:  1.5568932993870852\n",
      "Epoch:  90 Train Loss:  1.293366712732827\n",
      "Epoch:  90 Val Loss:  1.5562321201849785\n",
      "Epoch:  91 Train Loss:  1.2908698678941308\n",
      "Epoch:  91 Val Loss:  1.5555795877004457\n",
      "Epoch:  92 Train Loss:  1.2883906527639424\n",
      "Epoch:  92 Val Loss:  1.5549378735687758\n",
      "Epoch:  93 Train Loss:  1.2859279292670018\n",
      "Epoch:  93 Val Loss:  1.554304790463362\n",
      "Epoch:  94 Train Loss:  1.2834824338296604\n",
      "Epoch:  94 Val Loss:  1.5536747424327606\n",
      "Epoch:  95 Train Loss:  1.2810589829142371\n",
      "Epoch:  95 Val Loss:  1.553048166746632\n",
      "Epoch:  96 Train Loss:  1.2786548386503598\n",
      "Epoch:  96 Val Loss:  1.5524290267782577\n",
      "Epoch:  97 Train Loss:  1.2762633762075508\n",
      "Epoch:  97 Val Loss:  1.5518177194680804\n",
      "Epoch:  98 Train Loss:  1.2738851391211736\n",
      "Epoch:  98 Val Loss:  1.5512177480965181\n",
      "Epoch:  99 Train Loss:  1.271523491752586\n",
      "Epoch:  99 Val Loss:  1.5506338731145584\n",
      "Training completed\n",
      "relu zero\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  2.3025850929940437\n",
      "Epoch:  0 Val Loss:  2.302585092994046\n",
      "Epoch:  1 Train Loss:  2.3025850929940437\n",
      "Epoch:  1 Val Loss:  2.302585092994046\n",
      "Epoch:  2 Train Loss:  2.3025850929940437\n",
      "Epoch:  2 Val Loss:  2.302585092994046\n",
      "Epoch:  3 Train Loss:  2.3025850929940437\n",
      "Epoch:  3 Val Loss:  2.302585092994046\n",
      "Epoch:  4 Train Loss:  2.3025850929940437\n",
      "Epoch:  4 Val Loss:  2.302585092994046\n",
      "Epoch:  5 Train Loss:  2.3025850929940437\n",
      "Epoch:  5 Val Loss:  2.302585092994046\n",
      "Epoch:  6 Train Loss:  2.3025850929940437\n",
      "Epoch:  6 Val Loss:  2.302585092994046\n",
      "Epoch:  7 Train Loss:  2.3025850929940437\n",
      "Epoch:  7 Val Loss:  2.302585092994046\n",
      "Epoch:  8 Train Loss:  2.3025850929940437\n",
      "Epoch:  8 Val Loss:  2.302585092994046\n",
      "Epoch:  9 Train Loss:  2.3025850929940437\n",
      "Epoch:  9 Val Loss:  2.302585092994046\n",
      "Epoch:  10 Train Loss:  2.3025850929940437\n",
      "Epoch:  10 Val Loss:  2.302585092994046\n",
      "Epoch:  11 Train Loss:  2.3025850929940437\n",
      "Epoch:  11 Val Loss:  2.302585092994046\n",
      "Epoch:  12 Train Loss:  2.3025850929940437\n",
      "Epoch:  12 Val Loss:  2.302585092994046\n",
      "Epoch:  13 Train Loss:  2.3025850929940437\n",
      "Epoch:  13 Val Loss:  2.302585092994046\n",
      "Epoch:  14 Train Loss:  2.3025850929940437\n",
      "Epoch:  14 Val Loss:  2.302585092994046\n",
      "Epoch:  15 Train Loss:  2.3025850929940437\n",
      "Epoch:  15 Val Loss:  2.302585092994046\n",
      "Epoch:  16 Train Loss:  2.3025850929940437\n",
      "Epoch:  16 Val Loss:  2.302585092994046\n",
      "Epoch:  17 Train Loss:  2.3025850929940437\n",
      "Epoch:  17 Val Loss:  2.302585092994046\n",
      "Epoch:  18 Train Loss:  2.3025850929940437\n",
      "Epoch:  18 Val Loss:  2.302585092994046\n",
      "Epoch:  19 Train Loss:  2.3025850929940437\n",
      "Epoch:  19 Val Loss:  2.302585092994046\n",
      "Epoch:  20 Train Loss:  2.3025850929940437\n",
      "Epoch:  20 Val Loss:  2.302585092994046\n",
      "Epoch:  21 Train Loss:  2.3025850929940437\n",
      "Epoch:  21 Val Loss:  2.302585092994046\n",
      "Epoch:  22 Train Loss:  2.3025850929940437\n",
      "Epoch:  22 Val Loss:  2.302585092994046\n",
      "Epoch:  23 Train Loss:  2.3025850929940437\n",
      "Epoch:  23 Val Loss:  2.302585092994046\n",
      "Epoch:  24 Train Loss:  2.3025850929940437\n",
      "Epoch:  24 Val Loss:  2.302585092994046\n",
      "Epoch:  25 Train Loss:  2.3025850929940437\n",
      "Epoch:  25 Val Loss:  2.302585092994046\n",
      "Epoch:  26 Train Loss:  2.3025850929940437\n",
      "Epoch:  26 Val Loss:  2.302585092994046\n",
      "Epoch:  27 Train Loss:  2.3025850929940437\n",
      "Epoch:  27 Val Loss:  2.302585092994046\n",
      "Epoch:  28 Train Loss:  2.3025850929940437\n",
      "Epoch:  28 Val Loss:  2.302585092994046\n",
      "Epoch:  29 Train Loss:  2.3025850929940437\n",
      "Epoch:  29 Val Loss:  2.302585092994046\n",
      "Epoch:  30 Train Loss:  2.3025850929940437\n",
      "Epoch:  30 Val Loss:  2.302585092994046\n",
      "Epoch:  31 Train Loss:  2.3025850929940437\n",
      "Epoch:  31 Val Loss:  2.302585092994046\n",
      "Epoch:  32 Train Loss:  2.3025850929940437\n",
      "Epoch:  32 Val Loss:  2.302585092994046\n",
      "Epoch:  33 Train Loss:  2.3025850929940437\n",
      "Epoch:  33 Val Loss:  2.302585092994046\n",
      "Epoch:  34 Train Loss:  2.3025850929940437\n",
      "Epoch:  34 Val Loss:  2.302585092994046\n",
      "Epoch:  35 Train Loss:  2.3025850929940437\n",
      "Epoch:  35 Val Loss:  2.302585092994046\n",
      "Epoch:  36 Train Loss:  2.3025850929940437\n",
      "Epoch:  36 Val Loss:  2.302585092994046\n",
      "Epoch:  37 Train Loss:  2.3025850929940437\n",
      "Epoch:  37 Val Loss:  2.302585092994046\n",
      "Epoch:  38 Train Loss:  2.3025850929940437\n",
      "Epoch:  38 Val Loss:  2.302585092994046\n",
      "Epoch:  39 Train Loss:  2.3025850929940437\n",
      "Epoch:  39 Val Loss:  2.302585092994046\n",
      "Epoch:  40 Train Loss:  2.3025850929940437\n",
      "Epoch:  40 Val Loss:  2.302585092994046\n",
      "Epoch:  41 Train Loss:  2.3025850929940437\n",
      "Epoch:  41 Val Loss:  2.302585092994046\n",
      "Epoch:  42 Train Loss:  2.3025850929940437\n",
      "Epoch:  42 Val Loss:  2.302585092994046\n",
      "Epoch:  43 Train Loss:  2.3025850929940437\n",
      "Epoch:  43 Val Loss:  2.302585092994046\n",
      "Epoch:  44 Train Loss:  2.3025850929940437\n",
      "Epoch:  44 Val Loss:  2.302585092994046\n",
      "Epoch:  45 Train Loss:  2.3025850929940437\n",
      "Epoch:  45 Val Loss:  2.302585092994046\n",
      "Epoch:  46 Train Loss:  2.3025850929940437\n",
      "Epoch:  46 Val Loss:  2.302585092994046\n",
      "Epoch:  47 Train Loss:  2.3025850929940437\n",
      "Epoch:  47 Val Loss:  2.302585092994046\n",
      "Epoch:  48 Train Loss:  2.3025850929940437\n",
      "Epoch:  48 Val Loss:  2.302585092994046\n",
      "Epoch:  49 Train Loss:  2.3025850929940437\n",
      "Epoch:  49 Val Loss:  2.302585092994046\n",
      "Epoch:  50 Train Loss:  2.3025850929940437\n",
      "Epoch:  50 Val Loss:  2.302585092994046\n",
      "Epoch:  51 Train Loss:  2.3025850929940437\n",
      "Epoch:  51 Val Loss:  2.302585092994046\n",
      "Epoch:  52 Train Loss:  2.3025850929940437\n",
      "Epoch:  52 Val Loss:  2.302585092994046\n",
      "Epoch:  53 Train Loss:  2.3025850929940437\n",
      "Epoch:  53 Val Loss:  2.302585092994046\n",
      "Epoch:  54 Train Loss:  2.3025850929940437\n",
      "Epoch:  54 Val Loss:  2.302585092994046\n",
      "Epoch:  55 Train Loss:  2.3025850929940437\n",
      "Epoch:  55 Val Loss:  2.302585092994046\n",
      "Epoch:  56 Train Loss:  2.3025850929940437\n",
      "Epoch:  56 Val Loss:  2.302585092994046\n",
      "Epoch:  57 Train Loss:  2.3025850929940437\n",
      "Epoch:  57 Val Loss:  2.302585092994046\n",
      "Epoch:  58 Train Loss:  2.3025850929940437\n",
      "Epoch:  58 Val Loss:  2.302585092994046\n",
      "Epoch:  59 Train Loss:  2.3025850929940437\n",
      "Epoch:  59 Val Loss:  2.302585092994046\n",
      "Epoch:  60 Train Loss:  2.3025850929940437\n",
      "Epoch:  60 Val Loss:  2.302585092994046\n",
      "Epoch:  61 Train Loss:  2.3025850929940437\n",
      "Epoch:  61 Val Loss:  2.302585092994046\n",
      "Epoch:  62 Train Loss:  2.3025850929940437\n",
      "Epoch:  62 Val Loss:  2.302585092994046\n",
      "Epoch:  63 Train Loss:  2.3025850929940437\n",
      "Epoch:  63 Val Loss:  2.302585092994046\n",
      "Epoch:  64 Train Loss:  2.3025850929940437\n",
      "Epoch:  64 Val Loss:  2.302585092994046\n",
      "Epoch:  65 Train Loss:  2.3025850929940437\n",
      "Epoch:  65 Val Loss:  2.302585092994046\n",
      "Epoch:  66 Train Loss:  2.3025850929940437\n",
      "Epoch:  66 Val Loss:  2.302585092994046\n",
      "Epoch:  67 Train Loss:  2.3025850929940437\n",
      "Epoch:  67 Val Loss:  2.302585092994046\n",
      "Epoch:  68 Train Loss:  2.3025850929940437\n",
      "Epoch:  68 Val Loss:  2.302585092994046\n",
      "Epoch:  69 Train Loss:  2.3025850929940437\n",
      "Epoch:  69 Val Loss:  2.302585092994046\n",
      "Epoch:  70 Train Loss:  2.3025850929940437\n",
      "Epoch:  70 Val Loss:  2.302585092994046\n",
      "Epoch:  71 Train Loss:  2.3025850929940437\n",
      "Epoch:  71 Val Loss:  2.302585092994046\n",
      "Epoch:  72 Train Loss:  2.3025850929940437\n",
      "Epoch:  72 Val Loss:  2.302585092994046\n",
      "Epoch:  73 Train Loss:  2.3025850929940437\n",
      "Epoch:  73 Val Loss:  2.302585092994046\n",
      "Epoch:  74 Train Loss:  2.3025850929940437\n",
      "Epoch:  74 Val Loss:  2.302585092994046\n",
      "Epoch:  75 Train Loss:  2.3025850929940437\n",
      "Epoch:  75 Val Loss:  2.302585092994046\n",
      "Epoch:  76 Train Loss:  2.3025850929940437\n",
      "Epoch:  76 Val Loss:  2.302585092994046\n",
      "Epoch:  77 Train Loss:  2.3025850929940437\n",
      "Epoch:  77 Val Loss:  2.302585092994046\n",
      "Epoch:  78 Train Loss:  2.3025850929940437\n",
      "Epoch:  78 Val Loss:  2.302585092994046\n",
      "Epoch:  79 Train Loss:  2.3025850929940437\n",
      "Epoch:  79 Val Loss:  2.302585092994046\n",
      "Epoch:  80 Train Loss:  2.3025850929940437\n",
      "Epoch:  80 Val Loss:  2.302585092994046\n",
      "Epoch:  81 Train Loss:  2.3025850929940437\n",
      "Epoch:  81 Val Loss:  2.302585092994046\n",
      "Epoch:  82 Train Loss:  2.3025850929940437\n",
      "Epoch:  82 Val Loss:  2.302585092994046\n",
      "Epoch:  83 Train Loss:  2.3025850929940437\n",
      "Epoch:  83 Val Loss:  2.302585092994046\n",
      "Epoch:  84 Train Loss:  2.3025850929940437\n",
      "Epoch:  84 Val Loss:  2.302585092994046\n",
      "Epoch:  85 Train Loss:  2.3025850929940437\n",
      "Epoch:  85 Val Loss:  2.302585092994046\n",
      "Epoch:  86 Train Loss:  2.3025850929940437\n",
      "Epoch:  86 Val Loss:  2.302585092994046\n",
      "Epoch:  87 Train Loss:  2.3025850929940437\n",
      "Epoch:  87 Val Loss:  2.302585092994046\n",
      "Epoch:  88 Train Loss:  2.3025850929940437\n",
      "Epoch:  88 Val Loss:  2.302585092994046\n",
      "Epoch:  89 Train Loss:  2.3025850929940437\n",
      "Epoch:  89 Val Loss:  2.302585092994046\n",
      "Epoch:  90 Train Loss:  2.3025850929940437\n",
      "Epoch:  90 Val Loss:  2.302585092994046\n",
      "Epoch:  91 Train Loss:  2.3025850929940437\n",
      "Epoch:  91 Val Loss:  2.302585092994046\n",
      "Epoch:  92 Train Loss:  2.3025850929940437\n",
      "Epoch:  92 Val Loss:  2.302585092994046\n",
      "Epoch:  93 Train Loss:  2.3025850929940437\n",
      "Epoch:  93 Val Loss:  2.302585092994046\n",
      "Epoch:  94 Train Loss:  2.3025850929940437\n",
      "Epoch:  94 Val Loss:  2.302585092994046\n",
      "Epoch:  95 Train Loss:  2.3025850929940437\n",
      "Epoch:  95 Val Loss:  2.302585092994046\n",
      "Epoch:  96 Train Loss:  2.3025850929940437\n",
      "Epoch:  96 Val Loss:  2.302585092994046\n",
      "Epoch:  97 Train Loss:  2.3025850929940437\n",
      "Epoch:  97 Val Loss:  2.302585092994046\n",
      "Epoch:  98 Train Loss:  2.3025850929940437\n",
      "Epoch:  98 Val Loss:  2.302585092994046\n",
      "Epoch:  99 Train Loss:  2.3025850929940437\n",
      "Epoch:  99 Val Loss:  2.302585092994046\n",
      "Training completed\n",
      "relu random\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  1.6418973657624152\n",
      "Epoch:  0 Val Loss:  1.6368890470588273\n",
      "Epoch:  1 Train Loss:  1.462917100199322\n",
      "Epoch:  1 Val Loss:  1.4509204441636123\n",
      "Epoch:  2 Train Loss:  1.367417761325647\n",
      "Epoch:  2 Val Loss:  1.3577549817899\n",
      "Epoch:  3 Train Loss:  1.2581080719150224\n",
      "Epoch:  3 Val Loss:  1.249042803142919\n",
      "Epoch:  4 Train Loss:  1.1773432056677882\n",
      "Epoch:  4 Val Loss:  1.1679729341545295\n",
      "Epoch:  5 Train Loss:  1.122077845141701\n",
      "Epoch:  5 Val Loss:  1.1139262093450262\n",
      "Epoch:  6 Train Loss:  1.075388823912053\n",
      "Epoch:  6 Val Loss:  1.0688931386328753\n",
      "Epoch:  7 Train Loss:  1.025142624979532\n",
      "Epoch:  7 Val Loss:  1.0205294830647613\n",
      "Epoch:  8 Train Loss:  0.98033707387901\n",
      "Epoch:  8 Val Loss:  0.9775842534497424\n",
      "Epoch:  9 Train Loss:  0.9368972305942106\n",
      "Epoch:  9 Val Loss:  0.935643636309622\n",
      "Epoch:  10 Train Loss:  0.8948166440839739\n",
      "Epoch:  10 Val Loss:  0.8939227863572278\n",
      "Epoch:  11 Train Loss:  0.8583871966539913\n",
      "Epoch:  11 Val Loss:  0.8580432784856615\n",
      "Epoch:  12 Train Loss:  0.8276861999114863\n",
      "Epoch:  12 Val Loss:  0.8265094184349203\n",
      "Epoch:  13 Train Loss:  0.8030948557459334\n",
      "Epoch:  13 Val Loss:  0.8009573146645284\n",
      "Epoch:  14 Train Loss:  0.7790148360609775\n",
      "Epoch:  14 Val Loss:  0.7764039881669712\n",
      "Epoch:  15 Train Loss:  0.7578129916002148\n",
      "Epoch:  15 Val Loss:  0.7540742774194442\n",
      "Epoch:  16 Train Loss:  0.7378266538494376\n",
      "Epoch:  16 Val Loss:  0.7333481043047507\n",
      "Epoch:  17 Train Loss:  0.7179833548997415\n",
      "Epoch:  17 Val Loss:  0.7123527420751481\n",
      "Epoch:  18 Train Loss:  0.6990776183744193\n",
      "Epoch:  18 Val Loss:  0.6923564654782997\n",
      "Epoch:  19 Train Loss:  0.6812660935418778\n",
      "Epoch:  19 Val Loss:  0.673772638152728\n",
      "Epoch:  20 Train Loss:  0.6653636781787456\n",
      "Epoch:  20 Val Loss:  0.6572356826672889\n",
      "Epoch:  21 Train Loss:  0.6507927891748956\n",
      "Epoch:  21 Val Loss:  0.6421546107015912\n",
      "Epoch:  22 Train Loss:  0.6364455603611169\n",
      "Epoch:  22 Val Loss:  0.6273260406489781\n",
      "Epoch:  23 Train Loss:  0.6237707532852074\n",
      "Epoch:  23 Val Loss:  0.614464140653896\n",
      "Epoch:  24 Train Loss:  0.6116547575590728\n",
      "Epoch:  24 Val Loss:  0.6022168438709227\n",
      "Epoch:  25 Train Loss:  0.6000814227330828\n",
      "Epoch:  25 Val Loss:  0.5904902650806164\n",
      "Epoch:  26 Train Loss:  0.5887122086180877\n",
      "Epoch:  26 Val Loss:  0.5789683832860038\n",
      "Epoch:  27 Train Loss:  0.5786588759075441\n",
      "Epoch:  27 Val Loss:  0.568708319929778\n",
      "Epoch:  28 Train Loss:  0.5685821360444528\n",
      "Epoch:  28 Val Loss:  0.5587032691950468\n",
      "Epoch:  29 Train Loss:  0.5592362783860244\n",
      "Epoch:  29 Val Loss:  0.5494535466683984\n",
      "Epoch:  30 Train Loss:  0.5501408834277709\n",
      "Epoch:  30 Val Loss:  0.540648630449508\n",
      "Epoch:  31 Train Loss:  0.5415007122272711\n",
      "Epoch:  31 Val Loss:  0.5324561922019577\n",
      "Epoch:  32 Train Loss:  0.5332420145640087\n",
      "Epoch:  32 Val Loss:  0.5249242863019987\n",
      "Epoch:  33 Train Loss:  0.5253054380906557\n",
      "Epoch:  33 Val Loss:  0.5177279618865667\n",
      "Epoch:  34 Train Loss:  0.5171690028863352\n",
      "Epoch:  34 Val Loss:  0.5103130481236247\n",
      "Epoch:  35 Train Loss:  0.5096917365369711\n",
      "Epoch:  35 Val Loss:  0.5036045556238605\n",
      "Epoch:  36 Train Loss:  0.5025332029733058\n",
      "Epoch:  36 Val Loss:  0.4972360123564793\n",
      "Epoch:  37 Train Loss:  0.495613817178907\n",
      "Epoch:  37 Val Loss:  0.4913143100317644\n",
      "Epoch:  38 Train Loss:  0.4890769122443635\n",
      "Epoch:  38 Val Loss:  0.48558156704868677\n",
      "Epoch:  39 Train Loss:  0.4828981736008359\n",
      "Epoch:  39 Val Loss:  0.4805003165628439\n",
      "Epoch:  40 Train Loss:  0.47667204368030297\n",
      "Epoch:  40 Val Loss:  0.4752094947704563\n",
      "Epoch:  41 Train Loss:  0.4707279779375927\n",
      "Epoch:  41 Val Loss:  0.47032558577100353\n",
      "Epoch:  42 Train Loss:  0.46552310564394983\n",
      "Epoch:  42 Val Loss:  0.4660012402859641\n",
      "Epoch:  43 Train Loss:  0.45975555102174565\n",
      "Epoch:  43 Val Loss:  0.4613535257904402\n",
      "Epoch:  44 Train Loss:  0.4544795940366721\n",
      "Epoch:  44 Val Loss:  0.45701667592752976\n",
      "Epoch:  45 Train Loss:  0.4495353281796645\n",
      "Epoch:  45 Val Loss:  0.45315301848606804\n",
      "Epoch:  46 Train Loss:  0.44462083121748486\n",
      "Epoch:  46 Val Loss:  0.4489780463687181\n",
      "Epoch:  47 Train Loss:  0.44000276682698686\n",
      "Epoch:  47 Val Loss:  0.44517454302660336\n",
      "Epoch:  48 Train Loss:  0.4351217909902682\n",
      "Epoch:  48 Val Loss:  0.44123288800071686\n",
      "Epoch:  49 Train Loss:  0.43071088571485633\n",
      "Epoch:  49 Val Loss:  0.43757805133689337\n",
      "Epoch:  50 Train Loss:  0.42652339308898807\n",
      "Epoch:  50 Val Loss:  0.43414843839914374\n",
      "Epoch:  51 Train Loss:  0.42231298315194354\n",
      "Epoch:  51 Val Loss:  0.4305312609831869\n",
      "Epoch:  52 Train Loss:  0.41848323292719874\n",
      "Epoch:  52 Val Loss:  0.427154116198558\n",
      "Epoch:  53 Train Loss:  0.41460922933445266\n",
      "Epoch:  53 Val Loss:  0.42390544895730237\n",
      "Epoch:  54 Train Loss:  0.41081533332978637\n",
      "Epoch:  54 Val Loss:  0.4207351670431039\n",
      "Epoch:  55 Train Loss:  0.4069230276715318\n",
      "Epoch:  55 Val Loss:  0.4174841168125659\n",
      "Epoch:  56 Train Loss:  0.403439421706089\n",
      "Epoch:  56 Val Loss:  0.41452856904217533\n",
      "Epoch:  57 Train Loss:  0.3998713568750393\n",
      "Epoch:  57 Val Loss:  0.4114386236015911\n",
      "Epoch:  58 Train Loss:  0.39649743766778095\n",
      "Epoch:  58 Val Loss:  0.4085700853327886\n",
      "Epoch:  59 Train Loss:  0.3934457520034407\n",
      "Epoch:  59 Val Loss:  0.4057961519031329\n",
      "Epoch:  60 Train Loss:  0.39018477376167815\n",
      "Epoch:  60 Val Loss:  0.4032551659625533\n",
      "Epoch:  61 Train Loss:  0.3872114528239814\n",
      "Epoch:  61 Val Loss:  0.40063379794534676\n",
      "Epoch:  62 Train Loss:  0.3840589324746007\n",
      "Epoch:  62 Val Loss:  0.3981427734009894\n",
      "Epoch:  63 Train Loss:  0.3812986715993929\n",
      "Epoch:  63 Val Loss:  0.39566424618560764\n",
      "Epoch:  64 Train Loss:  0.37852420800365294\n",
      "Epoch:  64 Val Loss:  0.3934025664297789\n",
      "Epoch:  65 Train Loss:  0.37582178150059464\n",
      "Epoch:  65 Val Loss:  0.3910187579493521\n",
      "Epoch:  66 Train Loss:  0.37302688576230275\n",
      "Epoch:  66 Val Loss:  0.3888010979505542\n",
      "Epoch:  67 Train Loss:  0.370554387372343\n",
      "Epoch:  67 Val Loss:  0.3867189502323654\n",
      "Epoch:  68 Train Loss:  0.36802255943242085\n",
      "Epoch:  68 Val Loss:  0.38467148377241744\n",
      "Epoch:  69 Train Loss:  0.36539513112193067\n",
      "Epoch:  69 Val Loss:  0.3825061787260418\n",
      "Epoch:  70 Train Loss:  0.36311411606489735\n",
      "Epoch:  70 Val Loss:  0.38056450493905786\n",
      "Epoch:  71 Train Loss:  0.3605184260203362\n",
      "Epoch:  71 Val Loss:  0.3784303245219982\n",
      "Epoch:  72 Train Loss:  0.35808889753611345\n",
      "Epoch:  72 Val Loss:  0.3765957071688567\n",
      "Epoch:  73 Train Loss:  0.35599185779512343\n",
      "Epoch:  73 Val Loss:  0.37478309766698237\n",
      "Epoch:  74 Train Loss:  0.353482397848897\n",
      "Epoch:  74 Val Loss:  0.3727807520749733\n",
      "Epoch:  75 Train Loss:  0.35133418462846705\n",
      "Epoch:  75 Val Loss:  0.3710030998231358\n",
      "Epoch:  76 Train Loss:  0.3490935578187105\n",
      "Epoch:  76 Val Loss:  0.36904139126974916\n",
      "Epoch:  77 Train Loss:  0.3469278439810279\n",
      "Epoch:  77 Val Loss:  0.36728616644096174\n",
      "Epoch:  78 Train Loss:  0.3448157285372839\n",
      "Epoch:  78 Val Loss:  0.36541368080700765\n",
      "Epoch:  79 Train Loss:  0.34267477032287896\n",
      "Epoch:  79 Val Loss:  0.36362002967388324\n",
      "Epoch:  80 Train Loss:  0.34066523070708254\n",
      "Epoch:  80 Val Loss:  0.3620361482121931\n",
      "Epoch:  81 Train Loss:  0.33860267280025835\n",
      "Epoch:  81 Val Loss:  0.36041716045651895\n",
      "Epoch:  82 Train Loss:  0.33672180267725144\n",
      "Epoch:  82 Val Loss:  0.3588522329775516\n",
      "Epoch:  83 Train Loss:  0.3350664832182656\n",
      "Epoch:  83 Val Loss:  0.35741508601833044\n",
      "Epoch:  84 Train Loss:  0.3329464985459749\n",
      "Epoch:  84 Val Loss:  0.3557930892421918\n",
      "Epoch:  85 Train Loss:  0.33116686821310415\n",
      "Epoch:  85 Val Loss:  0.3542880183532157\n",
      "Epoch:  86 Train Loss:  0.3293097403313564\n",
      "Epoch:  86 Val Loss:  0.35273162158460813\n",
      "Epoch:  87 Train Loss:  0.32741509913728817\n",
      "Epoch:  87 Val Loss:  0.3512106247243879\n",
      "Epoch:  88 Train Loss:  0.32580299371035665\n",
      "Epoch:  88 Val Loss:  0.3497528161666181\n",
      "Epoch:  89 Train Loss:  0.32392800536142063\n",
      "Epoch:  89 Val Loss:  0.34827928881554027\n",
      "Epoch:  90 Train Loss:  0.32209404589774854\n",
      "Epoch:  90 Val Loss:  0.34687380574146137\n",
      "Epoch:  91 Train Loss:  0.32050497836536257\n",
      "Epoch:  91 Val Loss:  0.3456414986021088\n",
      "Epoch:  92 Train Loss:  0.3189539784501332\n",
      "Epoch:  92 Val Loss:  0.34422278537435164\n",
      "Epoch:  93 Train Loss:  0.31710433588299924\n",
      "Epoch:  93 Val Loss:  0.34287166616333914\n",
      "Epoch:  94 Train Loss:  0.3153079780671305\n",
      "Epoch:  94 Val Loss:  0.3414924359185919\n",
      "Epoch:  95 Train Loss:  0.31382221132925986\n",
      "Epoch:  95 Val Loss:  0.3402031650123788\n",
      "Epoch:  96 Train Loss:  0.31205967359818576\n",
      "Epoch:  96 Val Loss:  0.3388816997094838\n",
      "Epoch:  97 Train Loss:  0.31035623535210244\n",
      "Epoch:  97 Val Loss:  0.33765462378346717\n",
      "Epoch:  98 Train Loss:  0.3089516204796127\n",
      "Epoch:  98 Val Loss:  0.3364200773071454\n",
      "Epoch:  99 Train Loss:  0.30736586339033467\n",
      "Epoch:  99 Val Loss:  0.33535287941443337\n",
      "Training completed\n",
      "relu normal\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  2.3025850929940437\n",
      "Epoch:  0 Val Loss:  2.302585092994046\n",
      "Epoch:  1 Train Loss:  2.3025850929940437\n",
      "Epoch:  1 Val Loss:  2.302585092994046\n",
      "Epoch:  2 Train Loss:  2.3025850929940437\n",
      "Epoch:  2 Val Loss:  2.302585092994046\n",
      "Epoch:  3 Train Loss:  2.3025850929940437\n",
      "Epoch:  3 Val Loss:  2.302585092994046\n",
      "Epoch:  4 Train Loss:  2.3025850929940437\n",
      "Epoch:  4 Val Loss:  2.302585092994046\n",
      "Epoch:  5 Train Loss:  2.3025850929940437\n",
      "Epoch:  5 Val Loss:  2.302585092994046\n",
      "Epoch:  6 Train Loss:  2.3025850929940437\n",
      "Epoch:  6 Val Loss:  2.302585092994046\n",
      "Epoch:  7 Train Loss:  2.3025850929940437\n",
      "Epoch:  7 Val Loss:  2.302585092994046\n",
      "Epoch:  8 Train Loss:  2.3025850929940437\n",
      "Epoch:  8 Val Loss:  2.302585092994046\n",
      "Epoch:  9 Train Loss:  2.3025850929940437\n",
      "Epoch:  9 Val Loss:  2.302585092994046\n",
      "Epoch:  10 Train Loss:  2.3025850929940437\n",
      "Epoch:  10 Val Loss:  2.302585092994046\n",
      "Epoch:  11 Train Loss:  2.3025850929940437\n",
      "Epoch:  11 Val Loss:  2.302585092994046\n",
      "Epoch:  12 Train Loss:  2.3025850929940437\n",
      "Epoch:  12 Val Loss:  2.302585092994046\n",
      "Epoch:  13 Train Loss:  2.3025850929940437\n",
      "Epoch:  13 Val Loss:  2.302585092994046\n",
      "Epoch:  14 Train Loss:  2.3025850929940437\n",
      "Epoch:  14 Val Loss:  2.302585092994046\n",
      "Epoch:  15 Train Loss:  2.3025850929940437\n",
      "Epoch:  15 Val Loss:  2.302585092994046\n",
      "Epoch:  16 Train Loss:  2.3025850929940437\n",
      "Epoch:  16 Val Loss:  2.302585092994046\n",
      "Epoch:  17 Train Loss:  2.3025850929940437\n",
      "Epoch:  17 Val Loss:  2.302585092994046\n",
      "Epoch:  18 Train Loss:  2.3025850929940437\n",
      "Epoch:  18 Val Loss:  2.302585092994046\n",
      "Epoch:  19 Train Loss:  2.3025850929940437\n",
      "Epoch:  19 Val Loss:  2.302585092994046\n",
      "Epoch:  20 Train Loss:  2.3025850929940437\n",
      "Epoch:  20 Val Loss:  2.302585092994046\n",
      "Epoch:  21 Train Loss:  2.3025850929940437\n",
      "Epoch:  21 Val Loss:  2.302585092994046\n",
      "Epoch:  22 Train Loss:  2.3025850929940437\n",
      "Epoch:  22 Val Loss:  2.302585092994046\n",
      "Epoch:  23 Train Loss:  2.3025850929940437\n",
      "Epoch:  23 Val Loss:  2.302585092994046\n",
      "Epoch:  24 Train Loss:  2.3025850929940437\n",
      "Epoch:  24 Val Loss:  2.302585092994046\n",
      "Epoch:  25 Train Loss:  2.3025850929940437\n",
      "Epoch:  25 Val Loss:  2.302585092994046\n",
      "Epoch:  26 Train Loss:  2.3025850929940437\n",
      "Epoch:  26 Val Loss:  2.302585092994046\n",
      "Epoch:  27 Train Loss:  2.3025850929940437\n",
      "Epoch:  27 Val Loss:  2.302585092994046\n",
      "Epoch:  28 Train Loss:  2.3025850929940437\n",
      "Epoch:  28 Val Loss:  2.302585092994046\n",
      "Epoch:  29 Train Loss:  2.3025850929940437\n",
      "Epoch:  29 Val Loss:  2.302585092994046\n",
      "Epoch:  30 Train Loss:  2.3025850929940437\n",
      "Epoch:  30 Val Loss:  2.302585092994046\n",
      "Epoch:  31 Train Loss:  2.3025850929940437\n",
      "Epoch:  31 Val Loss:  2.302585092994046\n",
      "Epoch:  32 Train Loss:  2.3025850929940437\n",
      "Epoch:  32 Val Loss:  2.302585092994046\n",
      "Epoch:  33 Train Loss:  2.3025850929940437\n",
      "Epoch:  33 Val Loss:  2.302585092994046\n",
      "Epoch:  34 Train Loss:  2.3025850929940437\n",
      "Epoch:  34 Val Loss:  2.302585092994046\n",
      "Epoch:  35 Train Loss:  2.3025850929940437\n",
      "Epoch:  35 Val Loss:  2.302585092994046\n",
      "Epoch:  36 Train Loss:  2.3025850929940437\n",
      "Epoch:  36 Val Loss:  2.302585092994046\n",
      "Epoch:  37 Train Loss:  2.3025850929940437\n",
      "Epoch:  37 Val Loss:  2.302585092994046\n",
      "Epoch:  38 Train Loss:  2.3025850929940437\n",
      "Epoch:  38 Val Loss:  2.302585092994046\n",
      "Epoch:  39 Train Loss:  2.3025850929940437\n",
      "Epoch:  39 Val Loss:  2.302585092994046\n",
      "Epoch:  40 Train Loss:  2.3025850929940437\n",
      "Epoch:  40 Val Loss:  2.302585092994046\n",
      "Epoch:  41 Train Loss:  2.3025850929940437\n",
      "Epoch:  41 Val Loss:  2.302585092994046\n",
      "Epoch:  42 Train Loss:  2.3025850929940437\n",
      "Epoch:  42 Val Loss:  2.302585092994046\n",
      "Epoch:  43 Train Loss:  2.3025850929940437\n",
      "Epoch:  43 Val Loss:  2.302585092994046\n",
      "Epoch:  44 Train Loss:  2.3025850929940437\n",
      "Epoch:  44 Val Loss:  2.302585092994046\n",
      "Epoch:  45 Train Loss:  2.3025850929940437\n",
      "Epoch:  45 Val Loss:  2.302585092994046\n",
      "Epoch:  46 Train Loss:  2.3025850929940437\n",
      "Epoch:  46 Val Loss:  2.302585092994046\n",
      "Epoch:  47 Train Loss:  2.3025850929940437\n",
      "Epoch:  47 Val Loss:  2.302585092994046\n",
      "Epoch:  48 Train Loss:  2.3025850929940437\n",
      "Epoch:  48 Val Loss:  2.302585092994046\n",
      "Epoch:  49 Train Loss:  2.3025850929940437\n",
      "Epoch:  49 Val Loss:  2.302585092994046\n",
      "Epoch:  50 Train Loss:  2.3025850929940437\n",
      "Epoch:  50 Val Loss:  2.302585092994046\n",
      "Epoch:  51 Train Loss:  2.3025850929940437\n",
      "Epoch:  51 Val Loss:  2.302585092994046\n",
      "Epoch:  52 Train Loss:  2.3025850929940437\n",
      "Epoch:  52 Val Loss:  2.302585092994046\n",
      "Epoch:  53 Train Loss:  2.3025850929940437\n",
      "Epoch:  53 Val Loss:  2.302585092994046\n",
      "Epoch:  54 Train Loss:  2.3025850929940437\n",
      "Epoch:  54 Val Loss:  2.302585092994046\n",
      "Epoch:  55 Train Loss:  2.3025850929940437\n",
      "Epoch:  55 Val Loss:  2.302585092994046\n",
      "Epoch:  56 Train Loss:  2.3025850929940437\n",
      "Epoch:  56 Val Loss:  2.302585092994046\n",
      "Epoch:  57 Train Loss:  2.3025850929940437\n",
      "Epoch:  57 Val Loss:  2.302585092994046\n",
      "Epoch:  58 Train Loss:  2.3025850929940437\n",
      "Epoch:  58 Val Loss:  2.302585092994046\n",
      "Epoch:  59 Train Loss:  2.3025850929940437\n",
      "Epoch:  59 Val Loss:  2.302585092994046\n",
      "Epoch:  60 Train Loss:  2.3025850929940437\n",
      "Epoch:  60 Val Loss:  2.302585092994046\n",
      "Epoch:  61 Train Loss:  2.3025850929940437\n",
      "Epoch:  61 Val Loss:  2.302585092994046\n",
      "Epoch:  62 Train Loss:  2.3025850929940437\n",
      "Epoch:  62 Val Loss:  2.302585092994046\n",
      "Epoch:  63 Train Loss:  2.3025850929940437\n",
      "Epoch:  63 Val Loss:  2.302585092994046\n",
      "Epoch:  64 Train Loss:  2.3025850929940437\n",
      "Epoch:  64 Val Loss:  2.302585092994046\n",
      "Epoch:  65 Train Loss:  2.3025850929940437\n",
      "Epoch:  65 Val Loss:  2.302585092994046\n",
      "Epoch:  66 Train Loss:  2.3025850929940437\n",
      "Epoch:  66 Val Loss:  2.302585092994046\n",
      "Epoch:  67 Train Loss:  2.3025850929940437\n",
      "Epoch:  67 Val Loss:  2.302585092994046\n",
      "Epoch:  68 Train Loss:  2.3025850929940437\n",
      "Epoch:  68 Val Loss:  2.302585092994046\n",
      "Epoch:  69 Train Loss:  2.3025850929940437\n",
      "Epoch:  69 Val Loss:  2.302585092994046\n",
      "Epoch:  70 Train Loss:  2.3025850929940437\n",
      "Epoch:  70 Val Loss:  2.302585092994046\n",
      "Epoch:  71 Train Loss:  2.3025850929940437\n",
      "Epoch:  71 Val Loss:  2.302585092994046\n",
      "Epoch:  72 Train Loss:  2.3025850929940437\n",
      "Epoch:  72 Val Loss:  2.302585092994046\n",
      "Epoch:  73 Train Loss:  2.3025850929940437\n",
      "Epoch:  73 Val Loss:  2.302585092994046\n",
      "Epoch:  74 Train Loss:  2.3025850929940437\n",
      "Epoch:  74 Val Loss:  2.302585092994046\n",
      "Epoch:  75 Train Loss:  2.3025850929940437\n",
      "Epoch:  75 Val Loss:  2.302585092994046\n",
      "Epoch:  76 Train Loss:  2.3025850929940437\n",
      "Epoch:  76 Val Loss:  2.302585092994046\n",
      "Epoch:  77 Train Loss:  2.3025850929940437\n",
      "Epoch:  77 Val Loss:  2.302585092994046\n",
      "Epoch:  78 Train Loss:  2.3025850929940437\n",
      "Epoch:  78 Val Loss:  2.302585092994046\n",
      "Epoch:  79 Train Loss:  2.3025850929940437\n",
      "Epoch:  79 Val Loss:  2.302585092994046\n",
      "Epoch:  80 Train Loss:  2.3025850929940437\n",
      "Epoch:  80 Val Loss:  2.302585092994046\n",
      "Epoch:  81 Train Loss:  2.3025850929940437\n",
      "Epoch:  81 Val Loss:  2.302585092994046\n",
      "Epoch:  82 Train Loss:  2.3025850929940437\n",
      "Epoch:  82 Val Loss:  2.302585092994046\n",
      "Epoch:  83 Train Loss:  2.3025850929940437\n",
      "Epoch:  83 Val Loss:  2.302585092994046\n",
      "Epoch:  84 Train Loss:  2.3025850929940437\n",
      "Epoch:  84 Val Loss:  2.302585092994046\n",
      "Epoch:  85 Train Loss:  2.3025850929940437\n",
      "Epoch:  85 Val Loss:  2.302585092994046\n",
      "Epoch:  86 Train Loss:  2.3025850929940437\n",
      "Epoch:  86 Val Loss:  2.302585092994046\n",
      "Epoch:  87 Train Loss:  2.3025850929940437\n",
      "Epoch:  87 Val Loss:  2.302585092994046\n",
      "Epoch:  88 Train Loss:  2.3025850929940437\n",
      "Epoch:  88 Val Loss:  2.302585092994046\n",
      "Epoch:  89 Train Loss:  2.3025850929940437\n",
      "Epoch:  89 Val Loss:  2.302585092994046\n",
      "Epoch:  90 Train Loss:  2.3025850929940437\n",
      "Epoch:  90 Val Loss:  2.302585092994046\n",
      "Epoch:  91 Train Loss:  2.3025850929940437\n",
      "Epoch:  91 Val Loss:  2.302585092994046\n",
      "Epoch:  92 Train Loss:  2.3025850929940437\n",
      "Epoch:  92 Val Loss:  2.302585092994046\n",
      "Epoch:  93 Train Loss:  2.3025850929940437\n",
      "Epoch:  93 Val Loss:  2.302585092994046\n",
      "Epoch:  94 Train Loss:  2.3025850929940437\n",
      "Epoch:  94 Val Loss:  2.302585092994046\n",
      "Epoch:  95 Train Loss:  2.3025850929940437\n",
      "Epoch:  95 Val Loss:  2.302585092994046\n",
      "Epoch:  96 Train Loss:  2.3025850929940437\n",
      "Epoch:  96 Val Loss:  2.302585092994046\n",
      "Epoch:  97 Train Loss:  2.3025850929940437\n",
      "Epoch:  97 Val Loss:  2.302585092994046\n",
      "Epoch:  98 Train Loss:  2.3025850929940437\n",
      "Epoch:  98 Val Loss:  2.302585092994046\n",
      "Epoch:  99 Train Loss:  2.3025850929940437\n",
      "Epoch:  99 Val Loss:  2.302585092994046\n",
      "Training completed\n",
      "leaky_relu zero\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  2.3025850929940437\n",
      "Epoch:  0 Val Loss:  2.302585092994046\n",
      "Epoch:  1 Train Loss:  2.3025850929940437\n",
      "Epoch:  1 Val Loss:  2.302585092994046\n",
      "Epoch:  2 Train Loss:  2.3025850929940437\n",
      "Epoch:  2 Val Loss:  2.302585092994046\n",
      "Epoch:  3 Train Loss:  2.3025850929940437\n",
      "Epoch:  3 Val Loss:  2.302585092994046\n",
      "Epoch:  4 Train Loss:  2.3025850929940437\n",
      "Epoch:  4 Val Loss:  2.302585092994046\n",
      "Epoch:  5 Train Loss:  2.3025850929940437\n",
      "Epoch:  5 Val Loss:  2.302585092994046\n",
      "Epoch:  6 Train Loss:  2.3025850929940437\n",
      "Epoch:  6 Val Loss:  2.302585092994046\n",
      "Epoch:  7 Train Loss:  2.3025850929940437\n",
      "Epoch:  7 Val Loss:  2.302585092994046\n",
      "Epoch:  8 Train Loss:  2.3025850929940437\n",
      "Epoch:  8 Val Loss:  2.302585092994046\n",
      "Epoch:  9 Train Loss:  2.3025850929940437\n",
      "Epoch:  9 Val Loss:  2.302585092994046\n",
      "Epoch:  10 Train Loss:  2.3025850929940437\n",
      "Epoch:  10 Val Loss:  2.302585092994046\n",
      "Epoch:  11 Train Loss:  2.3025850929940437\n",
      "Epoch:  11 Val Loss:  2.302585092994046\n",
      "Epoch:  12 Train Loss:  2.3025850929940437\n",
      "Epoch:  12 Val Loss:  2.302585092994046\n",
      "Epoch:  13 Train Loss:  2.3025850929940437\n",
      "Epoch:  13 Val Loss:  2.302585092994046\n",
      "Epoch:  14 Train Loss:  2.3025850929940437\n",
      "Epoch:  14 Val Loss:  2.302585092994046\n",
      "Epoch:  15 Train Loss:  2.3025850929940437\n",
      "Epoch:  15 Val Loss:  2.302585092994046\n",
      "Epoch:  16 Train Loss:  2.3025850929940437\n",
      "Epoch:  16 Val Loss:  2.302585092994046\n",
      "Epoch:  17 Train Loss:  2.3025850929940437\n",
      "Epoch:  17 Val Loss:  2.302585092994046\n",
      "Epoch:  18 Train Loss:  2.3025850929940437\n",
      "Epoch:  18 Val Loss:  2.302585092994046\n",
      "Epoch:  19 Train Loss:  2.3025850929940437\n",
      "Epoch:  19 Val Loss:  2.302585092994046\n",
      "Epoch:  20 Train Loss:  2.3025850929940437\n",
      "Epoch:  20 Val Loss:  2.302585092994046\n",
      "Epoch:  21 Train Loss:  2.3025850929940437\n",
      "Epoch:  21 Val Loss:  2.302585092994046\n",
      "Epoch:  22 Train Loss:  2.3025850929940437\n",
      "Epoch:  22 Val Loss:  2.302585092994046\n",
      "Epoch:  23 Train Loss:  2.3025850929940437\n",
      "Epoch:  23 Val Loss:  2.302585092994046\n",
      "Epoch:  24 Train Loss:  2.3025850929940437\n",
      "Epoch:  24 Val Loss:  2.302585092994046\n",
      "Epoch:  25 Train Loss:  2.3025850929940437\n",
      "Epoch:  25 Val Loss:  2.302585092994046\n",
      "Epoch:  26 Train Loss:  2.3025850929940437\n",
      "Epoch:  26 Val Loss:  2.302585092994046\n",
      "Epoch:  27 Train Loss:  2.3025850929940437\n",
      "Epoch:  27 Val Loss:  2.302585092994046\n",
      "Epoch:  28 Train Loss:  2.3025850929940437\n",
      "Epoch:  28 Val Loss:  2.302585092994046\n",
      "Epoch:  29 Train Loss:  2.3025850929940437\n",
      "Epoch:  29 Val Loss:  2.302585092994046\n",
      "Epoch:  30 Train Loss:  2.3025850929940437\n",
      "Epoch:  30 Val Loss:  2.302585092994046\n",
      "Epoch:  31 Train Loss:  2.3025850929940437\n",
      "Epoch:  31 Val Loss:  2.302585092994046\n",
      "Epoch:  32 Train Loss:  2.3025850929940437\n",
      "Epoch:  32 Val Loss:  2.302585092994046\n",
      "Epoch:  33 Train Loss:  2.3025850929940437\n",
      "Epoch:  33 Val Loss:  2.302585092994046\n",
      "Epoch:  34 Train Loss:  2.3025850929940437\n",
      "Epoch:  34 Val Loss:  2.302585092994046\n",
      "Epoch:  35 Train Loss:  2.3025850929940437\n",
      "Epoch:  35 Val Loss:  2.302585092994046\n",
      "Epoch:  36 Train Loss:  2.3025850929940437\n",
      "Epoch:  36 Val Loss:  2.302585092994046\n",
      "Epoch:  37 Train Loss:  2.3025850929940437\n",
      "Epoch:  37 Val Loss:  2.302585092994046\n",
      "Epoch:  38 Train Loss:  2.3025850929940437\n",
      "Epoch:  38 Val Loss:  2.302585092994046\n",
      "Epoch:  39 Train Loss:  2.3025850929940437\n",
      "Epoch:  39 Val Loss:  2.302585092994046\n",
      "Epoch:  40 Train Loss:  2.3025850929940437\n",
      "Epoch:  40 Val Loss:  2.302585092994046\n",
      "Epoch:  41 Train Loss:  2.3025850929940437\n",
      "Epoch:  41 Val Loss:  2.302585092994046\n",
      "Epoch:  42 Train Loss:  2.3025850929940437\n",
      "Epoch:  42 Val Loss:  2.302585092994046\n",
      "Epoch:  43 Train Loss:  2.3025850929940437\n",
      "Epoch:  43 Val Loss:  2.302585092994046\n",
      "Epoch:  44 Train Loss:  2.3025850929940437\n",
      "Epoch:  44 Val Loss:  2.302585092994046\n",
      "Epoch:  45 Train Loss:  2.3025850929940437\n",
      "Epoch:  45 Val Loss:  2.302585092994046\n",
      "Epoch:  46 Train Loss:  2.3025850929940437\n",
      "Epoch:  46 Val Loss:  2.302585092994046\n",
      "Epoch:  47 Train Loss:  2.3025850929940437\n",
      "Epoch:  47 Val Loss:  2.302585092994046\n",
      "Epoch:  48 Train Loss:  2.3025850929940437\n",
      "Epoch:  48 Val Loss:  2.302585092994046\n",
      "Epoch:  49 Train Loss:  2.3025850929940437\n",
      "Epoch:  49 Val Loss:  2.302585092994046\n",
      "Epoch:  50 Train Loss:  2.3025850929940437\n",
      "Epoch:  50 Val Loss:  2.302585092994046\n",
      "Epoch:  51 Train Loss:  2.3025850929940437\n",
      "Epoch:  51 Val Loss:  2.302585092994046\n",
      "Epoch:  52 Train Loss:  2.3025850929940437\n",
      "Epoch:  52 Val Loss:  2.302585092994046\n",
      "Epoch:  53 Train Loss:  2.3025850929940437\n",
      "Epoch:  53 Val Loss:  2.302585092994046\n",
      "Epoch:  54 Train Loss:  2.3025850929940437\n",
      "Epoch:  54 Val Loss:  2.302585092994046\n",
      "Epoch:  55 Train Loss:  2.3025850929940437\n",
      "Epoch:  55 Val Loss:  2.302585092994046\n",
      "Epoch:  56 Train Loss:  2.3025850929940437\n",
      "Epoch:  56 Val Loss:  2.302585092994046\n",
      "Epoch:  57 Train Loss:  2.3025850929940437\n",
      "Epoch:  57 Val Loss:  2.302585092994046\n",
      "Epoch:  58 Train Loss:  2.3025850929940437\n",
      "Epoch:  58 Val Loss:  2.302585092994046\n",
      "Epoch:  59 Train Loss:  2.3025850929940437\n",
      "Epoch:  59 Val Loss:  2.302585092994046\n",
      "Epoch:  60 Train Loss:  2.3025850929940437\n",
      "Epoch:  60 Val Loss:  2.302585092994046\n",
      "Epoch:  61 Train Loss:  2.3025850929940437\n",
      "Epoch:  61 Val Loss:  2.302585092994046\n",
      "Epoch:  62 Train Loss:  2.3025850929940437\n",
      "Epoch:  62 Val Loss:  2.302585092994046\n",
      "Epoch:  63 Train Loss:  2.3025850929940437\n",
      "Epoch:  63 Val Loss:  2.302585092994046\n",
      "Epoch:  64 Train Loss:  2.3025850929940437\n",
      "Epoch:  64 Val Loss:  2.302585092994046\n",
      "Epoch:  65 Train Loss:  2.3025850929940437\n",
      "Epoch:  65 Val Loss:  2.302585092994046\n",
      "Epoch:  66 Train Loss:  2.3025850929940437\n",
      "Epoch:  66 Val Loss:  2.302585092994046\n",
      "Epoch:  67 Train Loss:  2.3025850929940437\n",
      "Epoch:  67 Val Loss:  2.302585092994046\n",
      "Epoch:  68 Train Loss:  2.3025850929940437\n",
      "Epoch:  68 Val Loss:  2.302585092994046\n",
      "Epoch:  69 Train Loss:  2.3025850929940437\n",
      "Epoch:  69 Val Loss:  2.302585092994046\n",
      "Epoch:  70 Train Loss:  2.3025850929940437\n",
      "Epoch:  70 Val Loss:  2.302585092994046\n",
      "Epoch:  71 Train Loss:  2.3025850929940437\n",
      "Epoch:  71 Val Loss:  2.302585092994046\n",
      "Epoch:  72 Train Loss:  2.3025850929940437\n",
      "Epoch:  72 Val Loss:  2.302585092994046\n",
      "Epoch:  73 Train Loss:  2.3025850929940437\n",
      "Epoch:  73 Val Loss:  2.302585092994046\n",
      "Epoch:  74 Train Loss:  2.3025850929940437\n",
      "Epoch:  74 Val Loss:  2.302585092994046\n",
      "Epoch:  75 Train Loss:  2.3025850929940437\n",
      "Epoch:  75 Val Loss:  2.302585092994046\n",
      "Epoch:  76 Train Loss:  2.3025850929940437\n",
      "Epoch:  76 Val Loss:  2.302585092994046\n",
      "Epoch:  77 Train Loss:  2.3025850929940437\n",
      "Epoch:  77 Val Loss:  2.302585092994046\n",
      "Epoch:  78 Train Loss:  2.3025850929940437\n",
      "Epoch:  78 Val Loss:  2.302585092994046\n",
      "Epoch:  79 Train Loss:  2.3025850929940437\n",
      "Epoch:  79 Val Loss:  2.302585092994046\n",
      "Epoch:  80 Train Loss:  2.3025850929940437\n",
      "Epoch:  80 Val Loss:  2.302585092994046\n",
      "Epoch:  81 Train Loss:  2.3025850929940437\n",
      "Epoch:  81 Val Loss:  2.302585092994046\n",
      "Epoch:  82 Train Loss:  2.3025850929940437\n",
      "Epoch:  82 Val Loss:  2.302585092994046\n",
      "Epoch:  83 Train Loss:  2.3025850929940437\n",
      "Epoch:  83 Val Loss:  2.302585092994046\n",
      "Epoch:  84 Train Loss:  2.3025850929940437\n",
      "Epoch:  84 Val Loss:  2.302585092994046\n",
      "Epoch:  85 Train Loss:  2.3025850929940437\n",
      "Epoch:  85 Val Loss:  2.302585092994046\n",
      "Epoch:  86 Train Loss:  2.3025850929940437\n",
      "Epoch:  86 Val Loss:  2.302585092994046\n",
      "Epoch:  87 Train Loss:  2.3025850929940437\n",
      "Epoch:  87 Val Loss:  2.302585092994046\n",
      "Epoch:  88 Train Loss:  2.3025850929940437\n",
      "Epoch:  88 Val Loss:  2.302585092994046\n",
      "Epoch:  89 Train Loss:  2.3025850929940437\n",
      "Epoch:  89 Val Loss:  2.302585092994046\n",
      "Epoch:  90 Train Loss:  2.3025850929940437\n",
      "Epoch:  90 Val Loss:  2.302585092994046\n",
      "Epoch:  91 Train Loss:  2.3025850929940437\n",
      "Epoch:  91 Val Loss:  2.302585092994046\n",
      "Epoch:  92 Train Loss:  2.3025850929940437\n",
      "Epoch:  92 Val Loss:  2.302585092994046\n",
      "Epoch:  93 Train Loss:  2.3025850929940437\n",
      "Epoch:  93 Val Loss:  2.302585092994046\n",
      "Epoch:  94 Train Loss:  2.3025850929940437\n",
      "Epoch:  94 Val Loss:  2.302585092994046\n",
      "Epoch:  95 Train Loss:  2.3025850929940437\n",
      "Epoch:  95 Val Loss:  2.302585092994046\n",
      "Epoch:  96 Train Loss:  2.3025850929940437\n",
      "Epoch:  96 Val Loss:  2.302585092994046\n",
      "Epoch:  97 Train Loss:  2.3025850929940437\n",
      "Epoch:  97 Val Loss:  2.302585092994046\n",
      "Epoch:  98 Train Loss:  2.3025850929940437\n",
      "Epoch:  98 Val Loss:  2.302585092994046\n",
      "Epoch:  99 Train Loss:  2.3025850929940437\n",
      "Epoch:  99 Val Loss:  2.302585092994046\n",
      "Training completed\n",
      "leaky_relu random\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  1.771985233166207\n",
      "Epoch:  0 Val Loss:  1.7242170025460204\n",
      "Epoch:  1 Train Loss:  1.4596895096269342\n",
      "Epoch:  1 Val Loss:  1.4126842830263946\n",
      "Epoch:  2 Train Loss:  1.2941089458512327\n",
      "Epoch:  2 Val Loss:  1.2438675966678852\n",
      "Epoch:  3 Train Loss:  1.1799097424191132\n",
      "Epoch:  3 Val Loss:  1.1307097926118825\n",
      "Epoch:  4 Train Loss:  1.0948388291378277\n",
      "Epoch:  4 Val Loss:  1.0504836629466991\n",
      "Epoch:  5 Train Loss:  1.0269349175569078\n",
      "Epoch:  5 Val Loss:  0.9868566767094173\n",
      "Epoch:  6 Train Loss:  0.9710692606229802\n",
      "Epoch:  6 Val Loss:  0.9349131726930534\n",
      "Epoch:  7 Train Loss:  0.922509804765213\n",
      "Epoch:  7 Val Loss:  0.889881722561809\n",
      "Epoch:  8 Train Loss:  0.8820209137640798\n",
      "Epoch:  8 Val Loss:  0.8520061433944993\n",
      "Epoch:  9 Train Loss:  0.8470954443717983\n",
      "Epoch:  9 Val Loss:  0.8195172873714731\n",
      "Epoch:  10 Train Loss:  0.8153410143215158\n",
      "Epoch:  10 Val Loss:  0.7903793550116923\n",
      "Epoch:  11 Train Loss:  0.7885697107094612\n",
      "Epoch:  11 Val Loss:  0.7658549309612046\n",
      "Epoch:  12 Train Loss:  0.7649917806116391\n",
      "Epoch:  12 Val Loss:  0.7442097756413457\n",
      "Epoch:  13 Train Loss:  0.7438435934641044\n",
      "Epoch:  13 Val Loss:  0.7251237145831477\n",
      "Epoch:  14 Train Loss:  0.7242088280629538\n",
      "Epoch:  14 Val Loss:  0.7073081400955431\n",
      "Epoch:  15 Train Loss:  0.7054923651956281\n",
      "Epoch:  15 Val Loss:  0.6899123919257925\n",
      "Epoch:  16 Train Loss:  0.688294985990371\n",
      "Epoch:  16 Val Loss:  0.6739983111681777\n",
      "Epoch:  17 Train Loss:  0.6715208491836964\n",
      "Epoch:  17 Val Loss:  0.6585965084610775\n",
      "Epoch:  18 Train Loss:  0.6561628887632047\n",
      "Epoch:  18 Val Loss:  0.6443076573238039\n",
      "Epoch:  19 Train Loss:  0.6423314561178168\n",
      "Epoch:  19 Val Loss:  0.6316385915701233\n",
      "Epoch:  20 Train Loss:  0.6291175097142471\n",
      "Epoch:  20 Val Loss:  0.6197072856999718\n",
      "Epoch:  21 Train Loss:  0.6171929462610347\n",
      "Epoch:  21 Val Loss:  0.608825940985276\n",
      "Epoch:  22 Train Loss:  0.6050992987731134\n",
      "Epoch:  22 Val Loss:  0.5978346415549906\n",
      "Epoch:  23 Train Loss:  0.5940646264969611\n",
      "Epoch:  23 Val Loss:  0.5877694745895145\n",
      "Epoch:  24 Train Loss:  0.5844744334116764\n",
      "Epoch:  24 Val Loss:  0.5789756147920803\n",
      "Epoch:  25 Train Loss:  0.5753116663698884\n",
      "Epoch:  25 Val Loss:  0.5706070143523589\n",
      "Epoch:  26 Train Loss:  0.5661825167633983\n",
      "Epoch:  26 Val Loss:  0.5622521242009112\n",
      "Epoch:  27 Train Loss:  0.5579545689900487\n",
      "Epoch:  27 Val Loss:  0.5550542695651576\n",
      "Epoch:  28 Train Loss:  0.5499195074422535\n",
      "Epoch:  28 Val Loss:  0.547548910118264\n",
      "Epoch:  29 Train Loss:  0.542600253776702\n",
      "Epoch:  29 Val Loss:  0.5409086251326891\n",
      "Epoch:  30 Train Loss:  0.5358307143380111\n",
      "Epoch:  30 Val Loss:  0.5346252880540574\n",
      "Epoch:  31 Train Loss:  0.5295856287748613\n",
      "Epoch:  31 Val Loss:  0.5288328871945678\n",
      "Epoch:  32 Train Loss:  0.522992429689366\n",
      "Epoch:  32 Val Loss:  0.522893222982486\n",
      "Epoch:  33 Train Loss:  0.5167420320390784\n",
      "Epoch:  33 Val Loss:  0.517117465638481\n",
      "Epoch:  34 Train Loss:  0.5103869592722087\n",
      "Epoch:  34 Val Loss:  0.5112018800691384\n",
      "Epoch:  35 Train Loss:  0.5049077092485631\n",
      "Epoch:  35 Val Loss:  0.5063946558720457\n",
      "Epoch:  36 Train Loss:  0.499327372688799\n",
      "Epoch:  36 Val Loss:  0.5013402483756796\n",
      "Epoch:  37 Train Loss:  0.49398056845497224\n",
      "Epoch:  37 Val Loss:  0.49661601381592074\n",
      "Epoch:  38 Train Loss:  0.4890919299093694\n",
      "Epoch:  38 Val Loss:  0.4923284241362441\n",
      "Epoch:  39 Train Loss:  0.4842204942050948\n",
      "Epoch:  39 Val Loss:  0.48783879409124703\n",
      "Epoch:  40 Train Loss:  0.47937302540066057\n",
      "Epoch:  40 Val Loss:  0.48368642059223604\n",
      "Epoch:  41 Train Loss:  0.47440619722045424\n",
      "Epoch:  41 Val Loss:  0.47897196070303516\n",
      "Epoch:  42 Train Loss:  0.47045904420719\n",
      "Epoch:  42 Val Loss:  0.47586426444178515\n",
      "Epoch:  43 Train Loss:  0.46581816141417276\n",
      "Epoch:  43 Val Loss:  0.47188158597563745\n",
      "Epoch:  44 Train Loss:  0.46139961553885794\n",
      "Epoch:  44 Val Loss:  0.4676454602160481\n",
      "Epoch:  45 Train Loss:  0.4571623187546251\n",
      "Epoch:  45 Val Loss:  0.4638442499994809\n",
      "Epoch:  46 Train Loss:  0.45305206665434766\n",
      "Epoch:  46 Val Loss:  0.4600186713674461\n",
      "Epoch:  47 Train Loss:  0.4492163309746962\n",
      "Epoch:  47 Val Loss:  0.45661156361705535\n",
      "Epoch:  48 Train Loss:  0.4452744674852305\n",
      "Epoch:  48 Val Loss:  0.45312734248592834\n",
      "Epoch:  49 Train Loss:  0.44143570424131734\n",
      "Epoch:  49 Val Loss:  0.4496158808687236\n",
      "Epoch:  50 Train Loss:  0.4380005621847178\n",
      "Epoch:  50 Val Loss:  0.4467617738257589\n",
      "Epoch:  51 Train Loss:  0.4338103178657803\n",
      "Epoch:  51 Val Loss:  0.4429487564740756\n",
      "Epoch:  52 Train Loss:  0.4303572519831873\n",
      "Epoch:  52 Val Loss:  0.4398713926636272\n",
      "Epoch:  53 Train Loss:  0.42688382442156786\n",
      "Epoch:  53 Val Loss:  0.4368253382205483\n",
      "Epoch:  54 Train Loss:  0.4238111015051798\n",
      "Epoch:  54 Val Loss:  0.4341600156650412\n",
      "Epoch:  55 Train Loss:  0.4206038364599689\n",
      "Epoch:  55 Val Loss:  0.43138121180336503\n",
      "Epoch:  56 Train Loss:  0.41693314482159766\n",
      "Epoch:  56 Val Loss:  0.42802448898217343\n",
      "Epoch:  57 Train Loss:  0.41424052112471343\n",
      "Epoch:  57 Val Loss:  0.42572618358606207\n",
      "Epoch:  58 Train Loss:  0.4109081656045161\n",
      "Epoch:  58 Val Loss:  0.4228953849106376\n",
      "Epoch:  59 Train Loss:  0.4073691728385148\n",
      "Epoch:  59 Val Loss:  0.41945037272191155\n",
      "Epoch:  60 Train Loss:  0.404333092236296\n",
      "Epoch:  60 Val Loss:  0.41687005100218405\n",
      "Epoch:  61 Train Loss:  0.4012998733943927\n",
      "Epoch:  61 Val Loss:  0.41416013957827436\n",
      "Epoch:  62 Train Loss:  0.3983493461940334\n",
      "Epoch:  62 Val Loss:  0.411673876115578\n",
      "Epoch:  63 Train Loss:  0.39529004652337996\n",
      "Epoch:  63 Val Loss:  0.40878469730482253\n",
      "Epoch:  64 Train Loss:  0.3924260267919312\n",
      "Epoch:  64 Val Loss:  0.40653138379818504\n",
      "Epoch:  65 Train Loss:  0.3894319041860783\n",
      "Epoch:  65 Val Loss:  0.40363658166737965\n",
      "Epoch:  66 Train Loss:  0.3867083810351192\n",
      "Epoch:  66 Val Loss:  0.4012890352700751\n",
      "Epoch:  67 Train Loss:  0.3841177116147481\n",
      "Epoch:  67 Val Loss:  0.39911243492578335\n",
      "Epoch:  68 Train Loss:  0.38165290860397094\n",
      "Epoch:  68 Val Loss:  0.39702164586197203\n",
      "Epoch:  69 Train Loss:  0.3785836173386958\n",
      "Epoch:  69 Val Loss:  0.39434903527342174\n",
      "Epoch:  70 Train Loss:  0.37620281714894094\n",
      "Epoch:  70 Val Loss:  0.3920699571508646\n",
      "Epoch:  71 Train Loss:  0.37374034030267844\n",
      "Epoch:  71 Val Loss:  0.3900436887410593\n",
      "Epoch:  72 Train Loss:  0.3711588490085297\n",
      "Epoch:  72 Val Loss:  0.387730250823717\n",
      "Epoch:  73 Train Loss:  0.368703822759542\n",
      "Epoch:  73 Val Loss:  0.3858254844987184\n",
      "Epoch:  74 Train Loss:  0.36645184916295287\n",
      "Epoch:  74 Val Loss:  0.3836768968480383\n",
      "Epoch:  75 Train Loss:  0.3637800697877504\n",
      "Epoch:  75 Val Loss:  0.38134725593402113\n",
      "Epoch:  76 Train Loss:  0.36131924855918224\n",
      "Epoch:  76 Val Loss:  0.3793297329719045\n",
      "Epoch:  77 Train Loss:  0.35883658663076157\n",
      "Epoch:  77 Val Loss:  0.3772904024309833\n",
      "Epoch:  78 Train Loss:  0.3565667325915208\n",
      "Epoch:  78 Val Loss:  0.37515362489832965\n",
      "Epoch:  79 Train Loss:  0.3545001247317246\n",
      "Epoch:  79 Val Loss:  0.37327783169993006\n",
      "Epoch:  80 Train Loss:  0.3523734263696388\n",
      "Epoch:  80 Val Loss:  0.37200996923490837\n",
      "Epoch:  81 Train Loss:  0.35007289883005477\n",
      "Epoch:  81 Val Loss:  0.3698394958126963\n",
      "Epoch:  82 Train Loss:  0.34769927546349877\n",
      "Epoch:  82 Val Loss:  0.3676373074827141\n",
      "Epoch:  83 Train Loss:  0.34559945500475975\n",
      "Epoch:  83 Val Loss:  0.36643820171932134\n",
      "Epoch:  84 Train Loss:  0.34335570339686866\n",
      "Epoch:  84 Val Loss:  0.36415267468677887\n",
      "Epoch:  85 Train Loss:  0.34126315647303723\n",
      "Epoch:  85 Val Loss:  0.3629828436995291\n",
      "Epoch:  86 Train Loss:  0.3391971852766236\n",
      "Epoch:  86 Val Loss:  0.36092002230274506\n",
      "Epoch:  87 Train Loss:  0.33770757762407433\n",
      "Epoch:  87 Val Loss:  0.3598339804009558\n",
      "Epoch:  88 Train Loss:  0.33611994617635077\n",
      "Epoch:  88 Val Loss:  0.3589500318227771\n",
      "Epoch:  89 Train Loss:  0.3341076838459186\n",
      "Epoch:  89 Val Loss:  0.3566736678271815\n",
      "Epoch:  90 Train Loss:  0.33203719108042085\n",
      "Epoch:  90 Val Loss:  0.35533061365513297\n",
      "Epoch:  91 Train Loss:  0.3302438511988486\n",
      "Epoch:  91 Val Loss:  0.3538286891780559\n",
      "Epoch:  92 Train Loss:  0.3282470944773817\n",
      "Epoch:  92 Val Loss:  0.3525037164776555\n",
      "Epoch:  93 Train Loss:  0.3263072988982932\n",
      "Epoch:  93 Val Loss:  0.35096903589995165\n",
      "Epoch:  94 Train Loss:  0.3245957521677749\n",
      "Epoch:  94 Val Loss:  0.3496814725493653\n",
      "Epoch:  95 Train Loss:  0.3229146528389398\n",
      "Epoch:  95 Val Loss:  0.3487282457434202\n",
      "Epoch:  96 Train Loss:  0.3209299120485512\n",
      "Epoch:  96 Val Loss:  0.3464476996126741\n",
      "Epoch:  97 Train Loss:  0.31945326981706174\n",
      "Epoch:  97 Val Loss:  0.34580950559469786\n",
      "Epoch:  98 Train Loss:  0.3175402344634188\n",
      "Epoch:  98 Val Loss:  0.3440242088724554\n",
      "Epoch:  99 Train Loss:  0.3159740904423738\n",
      "Epoch:  99 Val Loss:  0.34323252562707285\n",
      "Training completed\n",
      "leaky_relu normal\n",
      "Started Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6152/1016213629.py:35: RuntimeWarning: invalid value encountered in subtract\n",
      "  e_x = np.exp(x - x.max(axis=0, keepdims=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Train Loss:  nan\n",
      "Epoch:  0 Val Loss:  nan\n",
      "Epoch:  1 Train Loss:  nan\n",
      "Epoch:  1 Val Loss:  nan\n",
      "Epoch:  2 Train Loss:  nan\n",
      "Epoch:  2 Val Loss:  nan\n",
      "Epoch:  3 Train Loss:  nan\n",
      "Epoch:  3 Val Loss:  nan\n",
      "Epoch:  4 Train Loss:  nan\n",
      "Epoch:  4 Val Loss:  nan\n",
      "Epoch:  5 Train Loss:  nan\n",
      "Epoch:  5 Val Loss:  nan\n",
      "Epoch:  6 Train Loss:  nan\n",
      "Epoch:  6 Val Loss:  nan\n",
      "Epoch:  7 Train Loss:  nan\n",
      "Epoch:  7 Val Loss:  nan\n",
      "Epoch:  8 Train Loss:  nan\n",
      "Epoch:  8 Val Loss:  nan\n",
      "Epoch:  9 Train Loss:  nan\n",
      "Epoch:  9 Val Loss:  nan\n",
      "Epoch:  10 Train Loss:  nan\n",
      "Epoch:  10 Val Loss:  nan\n",
      "Epoch:  11 Train Loss:  nan\n",
      "Epoch:  11 Val Loss:  nan\n",
      "Epoch:  12 Train Loss:  nan\n",
      "Epoch:  12 Val Loss:  nan\n",
      "Epoch:  13 Train Loss:  nan\n",
      "Epoch:  13 Val Loss:  nan\n",
      "Epoch:  14 Train Loss:  nan\n",
      "Epoch:  14 Val Loss:  nan\n",
      "Epoch:  15 Train Loss:  nan\n",
      "Epoch:  15 Val Loss:  nan\n",
      "Epoch:  16 Train Loss:  nan\n",
      "Epoch:  16 Val Loss:  nan\n",
      "Epoch:  17 Train Loss:  nan\n",
      "Epoch:  17 Val Loss:  nan\n",
      "Epoch:  18 Train Loss:  nan\n",
      "Epoch:  18 Val Loss:  nan\n",
      "Epoch:  19 Train Loss:  nan\n",
      "Epoch:  19 Val Loss:  nan\n",
      "Epoch:  20 Train Loss:  nan\n",
      "Epoch:  20 Val Loss:  nan\n",
      "Epoch:  21 Train Loss:  nan\n",
      "Epoch:  21 Val Loss:  nan\n",
      "Epoch:  22 Train Loss:  nan\n",
      "Epoch:  22 Val Loss:  nan\n",
      "Epoch:  23 Train Loss:  nan\n",
      "Epoch:  23 Val Loss:  nan\n",
      "Epoch:  24 Train Loss:  nan\n",
      "Epoch:  24 Val Loss:  nan\n",
      "Epoch:  25 Train Loss:  nan\n",
      "Epoch:  25 Val Loss:  nan\n",
      "Epoch:  26 Train Loss:  nan\n",
      "Epoch:  26 Val Loss:  nan\n",
      "Epoch:  27 Train Loss:  nan\n",
      "Epoch:  27 Val Loss:  nan\n",
      "Epoch:  28 Train Loss:  nan\n",
      "Epoch:  28 Val Loss:  nan\n",
      "Epoch:  29 Train Loss:  nan\n",
      "Epoch:  29 Val Loss:  nan\n",
      "Epoch:  30 Train Loss:  nan\n",
      "Epoch:  30 Val Loss:  nan\n",
      "Epoch:  31 Train Loss:  nan\n",
      "Epoch:  31 Val Loss:  nan\n",
      "Epoch:  32 Train Loss:  nan\n",
      "Epoch:  32 Val Loss:  nan\n",
      "Epoch:  33 Train Loss:  nan\n",
      "Epoch:  33 Val Loss:  nan\n",
      "Epoch:  34 Train Loss:  nan\n",
      "Epoch:  34 Val Loss:  nan\n",
      "Epoch:  35 Train Loss:  nan\n",
      "Epoch:  35 Val Loss:  nan\n",
      "Epoch:  36 Train Loss:  nan\n",
      "Epoch:  36 Val Loss:  nan\n",
      "Epoch:  37 Train Loss:  nan\n",
      "Epoch:  37 Val Loss:  nan\n",
      "Epoch:  38 Train Loss:  nan\n",
      "Epoch:  38 Val Loss:  nan\n",
      "Epoch:  39 Train Loss:  nan\n",
      "Epoch:  39 Val Loss:  nan\n",
      "Epoch:  40 Train Loss:  nan\n",
      "Epoch:  40 Val Loss:  nan\n",
      "Epoch:  41 Train Loss:  nan\n",
      "Epoch:  41 Val Loss:  nan\n",
      "Epoch:  42 Train Loss:  nan\n",
      "Epoch:  42 Val Loss:  nan\n",
      "Epoch:  43 Train Loss:  nan\n",
      "Epoch:  43 Val Loss:  nan\n",
      "Epoch:  44 Train Loss:  nan\n",
      "Epoch:  44 Val Loss:  nan\n",
      "Epoch:  45 Train Loss:  nan\n",
      "Epoch:  45 Val Loss:  nan\n",
      "Epoch:  46 Train Loss:  nan\n",
      "Epoch:  46 Val Loss:  nan\n",
      "Epoch:  47 Train Loss:  nan\n",
      "Epoch:  47 Val Loss:  nan\n",
      "Epoch:  48 Train Loss:  nan\n",
      "Epoch:  48 Val Loss:  nan\n",
      "Epoch:  49 Train Loss:  nan\n",
      "Epoch:  49 Val Loss:  nan\n",
      "Epoch:  50 Train Loss:  nan\n",
      "Epoch:  50 Val Loss:  nan\n",
      "Epoch:  51 Train Loss:  nan\n",
      "Epoch:  51 Val Loss:  nan\n",
      "Epoch:  52 Train Loss:  nan\n",
      "Epoch:  52 Val Loss:  nan\n",
      "Epoch:  53 Train Loss:  nan\n",
      "Epoch:  53 Val Loss:  nan\n",
      "Epoch:  54 Train Loss:  nan\n",
      "Epoch:  54 Val Loss:  nan\n",
      "Epoch:  55 Train Loss:  nan\n",
      "Epoch:  55 Val Loss:  nan\n",
      "Epoch:  56 Train Loss:  nan\n",
      "Epoch:  56 Val Loss:  nan\n",
      "Epoch:  57 Train Loss:  nan\n",
      "Epoch:  57 Val Loss:  nan\n",
      "Epoch:  58 Train Loss:  nan\n",
      "Epoch:  58 Val Loss:  nan\n",
      "Epoch:  59 Train Loss:  nan\n",
      "Epoch:  59 Val Loss:  nan\n",
      "Epoch:  60 Train Loss:  nan\n",
      "Epoch:  60 Val Loss:  nan\n",
      "Epoch:  61 Train Loss:  nan\n",
      "Epoch:  61 Val Loss:  nan\n",
      "Epoch:  62 Train Loss:  nan\n",
      "Epoch:  62 Val Loss:  nan\n",
      "Epoch:  63 Train Loss:  nan\n",
      "Epoch:  63 Val Loss:  nan\n",
      "Epoch:  64 Train Loss:  nan\n",
      "Epoch:  64 Val Loss:  nan\n",
      "Epoch:  65 Train Loss:  nan\n",
      "Epoch:  65 Val Loss:  nan\n",
      "Epoch:  66 Train Loss:  nan\n",
      "Epoch:  66 Val Loss:  nan\n",
      "Epoch:  67 Train Loss:  nan\n",
      "Epoch:  67 Val Loss:  nan\n",
      "Epoch:  68 Train Loss:  nan\n",
      "Epoch:  68 Val Loss:  nan\n",
      "Epoch:  69 Train Loss:  nan\n",
      "Epoch:  69 Val Loss:  nan\n",
      "Epoch:  70 Train Loss:  nan\n",
      "Epoch:  70 Val Loss:  nan\n",
      "Epoch:  71 Train Loss:  nan\n",
      "Epoch:  71 Val Loss:  nan\n",
      "Epoch:  72 Train Loss:  nan\n",
      "Epoch:  72 Val Loss:  nan\n",
      "Epoch:  73 Train Loss:  nan\n",
      "Epoch:  73 Val Loss:  nan\n",
      "Epoch:  74 Train Loss:  nan\n",
      "Epoch:  74 Val Loss:  nan\n",
      "Epoch:  75 Train Loss:  nan\n",
      "Epoch:  75 Val Loss:  nan\n",
      "Epoch:  76 Train Loss:  nan\n",
      "Epoch:  76 Val Loss:  nan\n",
      "Epoch:  77 Train Loss:  nan\n",
      "Epoch:  77 Val Loss:  nan\n",
      "Epoch:  78 Train Loss:  nan\n",
      "Epoch:  78 Val Loss:  nan\n",
      "Epoch:  79 Train Loss:  nan\n",
      "Epoch:  79 Val Loss:  nan\n",
      "Epoch:  80 Train Loss:  nan\n",
      "Epoch:  80 Val Loss:  nan\n",
      "Epoch:  81 Train Loss:  nan\n",
      "Epoch:  81 Val Loss:  nan\n",
      "Epoch:  82 Train Loss:  nan\n",
      "Epoch:  82 Val Loss:  nan\n",
      "Epoch:  83 Train Loss:  nan\n",
      "Epoch:  83 Val Loss:  nan\n",
      "Epoch:  84 Train Loss:  nan\n",
      "Epoch:  84 Val Loss:  nan\n",
      "Epoch:  85 Train Loss:  nan\n",
      "Epoch:  85 Val Loss:  nan\n",
      "Epoch:  86 Train Loss:  nan\n",
      "Epoch:  86 Val Loss:  nan\n",
      "Epoch:  87 Train Loss:  nan\n",
      "Epoch:  87 Val Loss:  nan\n",
      "Epoch:  88 Train Loss:  nan\n",
      "Epoch:  88 Val Loss:  nan\n",
      "Epoch:  89 Train Loss:  nan\n",
      "Epoch:  89 Val Loss:  nan\n",
      "Epoch:  90 Train Loss:  nan\n",
      "Epoch:  90 Val Loss:  nan\n",
      "Epoch:  91 Train Loss:  nan\n",
      "Epoch:  91 Val Loss:  nan\n",
      "Epoch:  92 Train Loss:  nan\n",
      "Epoch:  92 Val Loss:  nan\n",
      "Epoch:  93 Train Loss:  nan\n",
      "Epoch:  93 Val Loss:  nan\n",
      "Epoch:  94 Train Loss:  nan\n",
      "Epoch:  94 Val Loss:  nan\n",
      "Epoch:  95 Train Loss:  nan\n",
      "Epoch:  95 Val Loss:  nan\n",
      "Epoch:  96 Train Loss:  nan\n",
      "Epoch:  96 Val Loss:  nan\n",
      "Epoch:  97 Train Loss:  nan\n",
      "Epoch:  97 Val Loss:  nan\n",
      "Epoch:  98 Train Loss:  nan\n",
      "Epoch:  98 Val Loss:  nan\n",
      "Epoch:  99 Train Loss:  nan\n",
      "Epoch:  99 Val Loss:  nan\n",
      "Training completed\n",
      "linear zero\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  2.3025850929940437\n",
      "Epoch:  0 Val Loss:  2.302585092994046\n",
      "Epoch:  1 Train Loss:  2.3025850929940437\n",
      "Epoch:  1 Val Loss:  2.302585092994046\n",
      "Epoch:  2 Train Loss:  2.3025850929940437\n",
      "Epoch:  2 Val Loss:  2.302585092994046\n",
      "Epoch:  3 Train Loss:  2.3025850929940437\n",
      "Epoch:  3 Val Loss:  2.302585092994046\n",
      "Epoch:  4 Train Loss:  2.3025850929940437\n",
      "Epoch:  4 Val Loss:  2.302585092994046\n",
      "Epoch:  5 Train Loss:  2.3025850929940437\n",
      "Epoch:  5 Val Loss:  2.302585092994046\n",
      "Epoch:  6 Train Loss:  2.3025850929940437\n",
      "Epoch:  6 Val Loss:  2.302585092994046\n",
      "Epoch:  7 Train Loss:  2.3025850929940437\n",
      "Epoch:  7 Val Loss:  2.302585092994046\n",
      "Epoch:  8 Train Loss:  2.3025850929940437\n",
      "Epoch:  8 Val Loss:  2.302585092994046\n",
      "Epoch:  9 Train Loss:  2.3025850929940437\n",
      "Epoch:  9 Val Loss:  2.302585092994046\n",
      "Epoch:  10 Train Loss:  2.3025850929940437\n",
      "Epoch:  10 Val Loss:  2.302585092994046\n",
      "Epoch:  11 Train Loss:  2.3025850929940437\n",
      "Epoch:  11 Val Loss:  2.302585092994046\n",
      "Epoch:  12 Train Loss:  2.3025850929940437\n",
      "Epoch:  12 Val Loss:  2.302585092994046\n",
      "Epoch:  13 Train Loss:  2.3025850929940437\n",
      "Epoch:  13 Val Loss:  2.302585092994046\n",
      "Epoch:  14 Train Loss:  2.3025850929940437\n",
      "Epoch:  14 Val Loss:  2.302585092994046\n",
      "Epoch:  15 Train Loss:  2.3025850929940437\n",
      "Epoch:  15 Val Loss:  2.302585092994046\n",
      "Epoch:  16 Train Loss:  2.3025850929940437\n",
      "Epoch:  16 Val Loss:  2.302585092994046\n",
      "Epoch:  17 Train Loss:  2.3025850929940437\n",
      "Epoch:  17 Val Loss:  2.302585092994046\n",
      "Epoch:  18 Train Loss:  2.3025850929940437\n",
      "Epoch:  18 Val Loss:  2.302585092994046\n",
      "Epoch:  19 Train Loss:  2.3025850929940437\n",
      "Epoch:  19 Val Loss:  2.302585092994046\n",
      "Epoch:  20 Train Loss:  2.3025850929940437\n",
      "Epoch:  20 Val Loss:  2.302585092994046\n",
      "Epoch:  21 Train Loss:  2.3025850929940437\n",
      "Epoch:  21 Val Loss:  2.302585092994046\n",
      "Epoch:  22 Train Loss:  2.3025850929940437\n",
      "Epoch:  22 Val Loss:  2.302585092994046\n",
      "Epoch:  23 Train Loss:  2.3025850929940437\n",
      "Epoch:  23 Val Loss:  2.302585092994046\n",
      "Epoch:  24 Train Loss:  2.3025850929940437\n",
      "Epoch:  24 Val Loss:  2.302585092994046\n",
      "Epoch:  25 Train Loss:  2.3025850929940437\n",
      "Epoch:  25 Val Loss:  2.302585092994046\n",
      "Epoch:  26 Train Loss:  2.3025850929940437\n",
      "Epoch:  26 Val Loss:  2.302585092994046\n",
      "Epoch:  27 Train Loss:  2.3025850929940437\n",
      "Epoch:  27 Val Loss:  2.302585092994046\n",
      "Epoch:  28 Train Loss:  2.3025850929940437\n",
      "Epoch:  28 Val Loss:  2.302585092994046\n",
      "Epoch:  29 Train Loss:  2.3025850929940437\n",
      "Epoch:  29 Val Loss:  2.302585092994046\n",
      "Epoch:  30 Train Loss:  2.3025850929940437\n",
      "Epoch:  30 Val Loss:  2.302585092994046\n",
      "Epoch:  31 Train Loss:  2.3025850929940437\n",
      "Epoch:  31 Val Loss:  2.302585092994046\n",
      "Epoch:  32 Train Loss:  2.3025850929940437\n",
      "Epoch:  32 Val Loss:  2.302585092994046\n",
      "Epoch:  33 Train Loss:  2.3025850929940437\n",
      "Epoch:  33 Val Loss:  2.302585092994046\n",
      "Epoch:  34 Train Loss:  2.3025850929940437\n",
      "Epoch:  34 Val Loss:  2.302585092994046\n",
      "Epoch:  35 Train Loss:  2.3025850929940437\n",
      "Epoch:  35 Val Loss:  2.302585092994046\n",
      "Epoch:  36 Train Loss:  2.3025850929940437\n",
      "Epoch:  36 Val Loss:  2.302585092994046\n",
      "Epoch:  37 Train Loss:  2.3025850929940437\n",
      "Epoch:  37 Val Loss:  2.302585092994046\n",
      "Epoch:  38 Train Loss:  2.3025850929940437\n",
      "Epoch:  38 Val Loss:  2.302585092994046\n",
      "Epoch:  39 Train Loss:  2.3025850929940437\n",
      "Epoch:  39 Val Loss:  2.302585092994046\n",
      "Epoch:  40 Train Loss:  2.3025850929940437\n",
      "Epoch:  40 Val Loss:  2.302585092994046\n",
      "Epoch:  41 Train Loss:  2.3025850929940437\n",
      "Epoch:  41 Val Loss:  2.302585092994046\n",
      "Epoch:  42 Train Loss:  2.3025850929940437\n",
      "Epoch:  42 Val Loss:  2.302585092994046\n",
      "Epoch:  43 Train Loss:  2.3025850929940437\n",
      "Epoch:  43 Val Loss:  2.302585092994046\n",
      "Epoch:  44 Train Loss:  2.3025850929940437\n",
      "Epoch:  44 Val Loss:  2.302585092994046\n",
      "Epoch:  45 Train Loss:  2.3025850929940437\n",
      "Epoch:  45 Val Loss:  2.302585092994046\n",
      "Epoch:  46 Train Loss:  2.3025850929940437\n",
      "Epoch:  46 Val Loss:  2.302585092994046\n",
      "Epoch:  47 Train Loss:  2.3025850929940437\n",
      "Epoch:  47 Val Loss:  2.302585092994046\n",
      "Epoch:  48 Train Loss:  2.3025850929940437\n",
      "Epoch:  48 Val Loss:  2.302585092994046\n",
      "Epoch:  49 Train Loss:  2.3025850929940437\n",
      "Epoch:  49 Val Loss:  2.302585092994046\n",
      "Epoch:  50 Train Loss:  2.3025850929940437\n",
      "Epoch:  50 Val Loss:  2.302585092994046\n",
      "Epoch:  51 Train Loss:  2.3025850929940437\n",
      "Epoch:  51 Val Loss:  2.302585092994046\n",
      "Epoch:  52 Train Loss:  2.3025850929940437\n",
      "Epoch:  52 Val Loss:  2.302585092994046\n",
      "Epoch:  53 Train Loss:  2.3025850929940437\n",
      "Epoch:  53 Val Loss:  2.302585092994046\n",
      "Epoch:  54 Train Loss:  2.3025850929940437\n",
      "Epoch:  54 Val Loss:  2.302585092994046\n",
      "Epoch:  55 Train Loss:  2.3025850929940437\n",
      "Epoch:  55 Val Loss:  2.302585092994046\n",
      "Epoch:  56 Train Loss:  2.3025850929940437\n",
      "Epoch:  56 Val Loss:  2.302585092994046\n",
      "Epoch:  57 Train Loss:  2.3025850929940437\n",
      "Epoch:  57 Val Loss:  2.302585092994046\n",
      "Epoch:  58 Train Loss:  2.3025850929940437\n",
      "Epoch:  58 Val Loss:  2.302585092994046\n",
      "Epoch:  59 Train Loss:  2.3025850929940437\n",
      "Epoch:  59 Val Loss:  2.302585092994046\n",
      "Epoch:  60 Train Loss:  2.3025850929940437\n",
      "Epoch:  60 Val Loss:  2.302585092994046\n",
      "Epoch:  61 Train Loss:  2.3025850929940437\n",
      "Epoch:  61 Val Loss:  2.302585092994046\n",
      "Epoch:  62 Train Loss:  2.3025850929940437\n",
      "Epoch:  62 Val Loss:  2.302585092994046\n",
      "Epoch:  63 Train Loss:  2.3025850929940437\n",
      "Epoch:  63 Val Loss:  2.302585092994046\n",
      "Epoch:  64 Train Loss:  2.3025850929940437\n",
      "Epoch:  64 Val Loss:  2.302585092994046\n",
      "Epoch:  65 Train Loss:  2.3025850929940437\n",
      "Epoch:  65 Val Loss:  2.302585092994046\n",
      "Epoch:  66 Train Loss:  2.3025850929940437\n",
      "Epoch:  66 Val Loss:  2.302585092994046\n",
      "Epoch:  67 Train Loss:  2.3025850929940437\n",
      "Epoch:  67 Val Loss:  2.302585092994046\n",
      "Epoch:  68 Train Loss:  2.3025850929940437\n",
      "Epoch:  68 Val Loss:  2.302585092994046\n",
      "Epoch:  69 Train Loss:  2.3025850929940437\n",
      "Epoch:  69 Val Loss:  2.302585092994046\n",
      "Epoch:  70 Train Loss:  2.3025850929940437\n",
      "Epoch:  70 Val Loss:  2.302585092994046\n",
      "Epoch:  71 Train Loss:  2.3025850929940437\n",
      "Epoch:  71 Val Loss:  2.302585092994046\n",
      "Epoch:  72 Train Loss:  2.3025850929940437\n",
      "Epoch:  72 Val Loss:  2.302585092994046\n",
      "Epoch:  73 Train Loss:  2.3025850929940437\n",
      "Epoch:  73 Val Loss:  2.302585092994046\n",
      "Epoch:  74 Train Loss:  2.3025850929940437\n",
      "Epoch:  74 Val Loss:  2.302585092994046\n",
      "Epoch:  75 Train Loss:  2.3025850929940437\n",
      "Epoch:  75 Val Loss:  2.302585092994046\n",
      "Epoch:  76 Train Loss:  2.3025850929940437\n",
      "Epoch:  76 Val Loss:  2.302585092994046\n",
      "Epoch:  77 Train Loss:  2.3025850929940437\n",
      "Epoch:  77 Val Loss:  2.302585092994046\n",
      "Epoch:  78 Train Loss:  2.3025850929940437\n",
      "Epoch:  78 Val Loss:  2.302585092994046\n",
      "Epoch:  79 Train Loss:  2.3025850929940437\n",
      "Epoch:  79 Val Loss:  2.302585092994046\n",
      "Epoch:  80 Train Loss:  2.3025850929940437\n",
      "Epoch:  80 Val Loss:  2.302585092994046\n",
      "Epoch:  81 Train Loss:  2.3025850929940437\n",
      "Epoch:  81 Val Loss:  2.302585092994046\n",
      "Epoch:  82 Train Loss:  2.3025850929940437\n",
      "Epoch:  82 Val Loss:  2.302585092994046\n",
      "Epoch:  83 Train Loss:  2.3025850929940437\n",
      "Epoch:  83 Val Loss:  2.302585092994046\n",
      "Epoch:  84 Train Loss:  2.3025850929940437\n",
      "Epoch:  84 Val Loss:  2.302585092994046\n",
      "Epoch:  85 Train Loss:  2.3025850929940437\n",
      "Epoch:  85 Val Loss:  2.302585092994046\n",
      "Epoch:  86 Train Loss:  2.3025850929940437\n",
      "Epoch:  86 Val Loss:  2.302585092994046\n",
      "Epoch:  87 Train Loss:  2.3025850929940437\n",
      "Epoch:  87 Val Loss:  2.302585092994046\n",
      "Epoch:  88 Train Loss:  2.3025850929940437\n",
      "Epoch:  88 Val Loss:  2.302585092994046\n",
      "Epoch:  89 Train Loss:  2.3025850929940437\n",
      "Epoch:  89 Val Loss:  2.302585092994046\n",
      "Epoch:  90 Train Loss:  2.3025850929940437\n",
      "Epoch:  90 Val Loss:  2.302585092994046\n",
      "Epoch:  91 Train Loss:  2.3025850929940437\n",
      "Epoch:  91 Val Loss:  2.302585092994046\n",
      "Epoch:  92 Train Loss:  2.3025850929940437\n",
      "Epoch:  92 Val Loss:  2.302585092994046\n",
      "Epoch:  93 Train Loss:  2.3025850929940437\n",
      "Epoch:  93 Val Loss:  2.302585092994046\n",
      "Epoch:  94 Train Loss:  2.3025850929940437\n",
      "Epoch:  94 Val Loss:  2.302585092994046\n",
      "Epoch:  95 Train Loss:  2.3025850929940437\n",
      "Epoch:  95 Val Loss:  2.302585092994046\n",
      "Epoch:  96 Train Loss:  2.3025850929940437\n",
      "Epoch:  96 Val Loss:  2.302585092994046\n",
      "Epoch:  97 Train Loss:  2.3025850929940437\n",
      "Epoch:  97 Val Loss:  2.302585092994046\n",
      "Epoch:  98 Train Loss:  2.3025850929940437\n",
      "Epoch:  98 Val Loss:  2.302585092994046\n",
      "Epoch:  99 Train Loss:  2.3025850929940437\n",
      "Epoch:  99 Val Loss:  2.302585092994046\n",
      "Training completed\n",
      "linear random\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  nan\n",
      "Epoch:  0 Val Loss:  nan\n",
      "Epoch:  1 Train Loss:  nan\n",
      "Epoch:  1 Val Loss:  nan\n",
      "Epoch:  2 Train Loss:  nan\n",
      "Epoch:  2 Val Loss:  nan\n",
      "Epoch:  3 Train Loss:  nan\n",
      "Epoch:  3 Val Loss:  nan\n",
      "Epoch:  4 Train Loss:  nan\n",
      "Epoch:  4 Val Loss:  nan\n",
      "Epoch:  5 Train Loss:  nan\n",
      "Epoch:  5 Val Loss:  nan\n",
      "Epoch:  6 Train Loss:  nan\n",
      "Epoch:  6 Val Loss:  nan\n",
      "Epoch:  7 Train Loss:  nan\n",
      "Epoch:  7 Val Loss:  nan\n",
      "Epoch:  8 Train Loss:  nan\n",
      "Epoch:  8 Val Loss:  nan\n",
      "Epoch:  9 Train Loss:  nan\n",
      "Epoch:  9 Val Loss:  nan\n",
      "Epoch:  10 Train Loss:  nan\n",
      "Epoch:  10 Val Loss:  nan\n",
      "Epoch:  11 Train Loss:  nan\n",
      "Epoch:  11 Val Loss:  nan\n",
      "Epoch:  12 Train Loss:  nan\n",
      "Epoch:  12 Val Loss:  nan\n",
      "Epoch:  13 Train Loss:  nan\n",
      "Epoch:  13 Val Loss:  nan\n",
      "Epoch:  14 Train Loss:  nan\n",
      "Epoch:  14 Val Loss:  nan\n",
      "Epoch:  15 Train Loss:  nan\n",
      "Epoch:  15 Val Loss:  nan\n",
      "Epoch:  16 Train Loss:  nan\n",
      "Epoch:  16 Val Loss:  nan\n",
      "Epoch:  17 Train Loss:  nan\n",
      "Epoch:  17 Val Loss:  nan\n",
      "Epoch:  18 Train Loss:  nan\n",
      "Epoch:  18 Val Loss:  nan\n",
      "Epoch:  19 Train Loss:  nan\n",
      "Epoch:  19 Val Loss:  nan\n",
      "Epoch:  20 Train Loss:  nan\n",
      "Epoch:  20 Val Loss:  nan\n",
      "Epoch:  21 Train Loss:  nan\n",
      "Epoch:  21 Val Loss:  nan\n",
      "Epoch:  22 Train Loss:  nan\n",
      "Epoch:  22 Val Loss:  nan\n",
      "Epoch:  23 Train Loss:  nan\n",
      "Epoch:  23 Val Loss:  nan\n",
      "Epoch:  24 Train Loss:  nan\n",
      "Epoch:  24 Val Loss:  nan\n",
      "Epoch:  25 Train Loss:  nan\n",
      "Epoch:  25 Val Loss:  nan\n",
      "Epoch:  26 Train Loss:  nan\n",
      "Epoch:  26 Val Loss:  nan\n",
      "Epoch:  27 Train Loss:  nan\n",
      "Epoch:  27 Val Loss:  nan\n",
      "Epoch:  28 Train Loss:  nan\n",
      "Epoch:  28 Val Loss:  nan\n",
      "Epoch:  29 Train Loss:  nan\n",
      "Epoch:  29 Val Loss:  nan\n",
      "Epoch:  30 Train Loss:  nan\n",
      "Epoch:  30 Val Loss:  nan\n",
      "Epoch:  31 Train Loss:  nan\n",
      "Epoch:  31 Val Loss:  nan\n",
      "Epoch:  32 Train Loss:  nan\n",
      "Epoch:  32 Val Loss:  nan\n",
      "Epoch:  33 Train Loss:  nan\n",
      "Epoch:  33 Val Loss:  nan\n",
      "Epoch:  34 Train Loss:  nan\n",
      "Epoch:  34 Val Loss:  nan\n",
      "Epoch:  35 Train Loss:  nan\n",
      "Epoch:  35 Val Loss:  nan\n",
      "Epoch:  36 Train Loss:  nan\n",
      "Epoch:  36 Val Loss:  nan\n",
      "Epoch:  37 Train Loss:  nan\n",
      "Epoch:  37 Val Loss:  nan\n",
      "Epoch:  38 Train Loss:  nan\n",
      "Epoch:  38 Val Loss:  nan\n",
      "Epoch:  39 Train Loss:  nan\n",
      "Epoch:  39 Val Loss:  nan\n",
      "Epoch:  40 Train Loss:  nan\n",
      "Epoch:  40 Val Loss:  nan\n",
      "Epoch:  41 Train Loss:  nan\n",
      "Epoch:  41 Val Loss:  nan\n",
      "Epoch:  42 Train Loss:  nan\n",
      "Epoch:  42 Val Loss:  nan\n",
      "Epoch:  43 Train Loss:  nan\n",
      "Epoch:  43 Val Loss:  nan\n",
      "Epoch:  44 Train Loss:  nan\n",
      "Epoch:  44 Val Loss:  nan\n",
      "Epoch:  45 Train Loss:  nan\n",
      "Epoch:  45 Val Loss:  nan\n",
      "Epoch:  46 Train Loss:  nan\n",
      "Epoch:  46 Val Loss:  nan\n",
      "Epoch:  47 Train Loss:  nan\n",
      "Epoch:  47 Val Loss:  nan\n",
      "Epoch:  48 Train Loss:  nan\n",
      "Epoch:  48 Val Loss:  nan\n",
      "Epoch:  49 Train Loss:  nan\n",
      "Epoch:  49 Val Loss:  nan\n",
      "Epoch:  50 Train Loss:  nan\n",
      "Epoch:  50 Val Loss:  nan\n",
      "Epoch:  51 Train Loss:  nan\n",
      "Epoch:  51 Val Loss:  nan\n",
      "Epoch:  52 Train Loss:  nan\n",
      "Epoch:  52 Val Loss:  nan\n",
      "Epoch:  53 Train Loss:  nan\n",
      "Epoch:  53 Val Loss:  nan\n",
      "Epoch:  54 Train Loss:  nan\n",
      "Epoch:  54 Val Loss:  nan\n",
      "Epoch:  55 Train Loss:  nan\n",
      "Epoch:  55 Val Loss:  nan\n",
      "Epoch:  56 Train Loss:  nan\n",
      "Epoch:  56 Val Loss:  nan\n",
      "Epoch:  57 Train Loss:  nan\n",
      "Epoch:  57 Val Loss:  nan\n",
      "Epoch:  58 Train Loss:  nan\n",
      "Epoch:  58 Val Loss:  nan\n",
      "Epoch:  59 Train Loss:  nan\n",
      "Epoch:  59 Val Loss:  nan\n",
      "Epoch:  60 Train Loss:  nan\n",
      "Epoch:  60 Val Loss:  nan\n",
      "Epoch:  61 Train Loss:  nan\n",
      "Epoch:  61 Val Loss:  nan\n",
      "Epoch:  62 Train Loss:  nan\n",
      "Epoch:  62 Val Loss:  nan\n",
      "Epoch:  63 Train Loss:  nan\n",
      "Epoch:  63 Val Loss:  nan\n",
      "Epoch:  64 Train Loss:  nan\n",
      "Epoch:  64 Val Loss:  nan\n",
      "Epoch:  65 Train Loss:  nan\n",
      "Epoch:  65 Val Loss:  nan\n",
      "Epoch:  66 Train Loss:  nan\n",
      "Epoch:  66 Val Loss:  nan\n",
      "Epoch:  67 Train Loss:  nan\n",
      "Epoch:  67 Val Loss:  nan\n",
      "Epoch:  68 Train Loss:  nan\n",
      "Epoch:  68 Val Loss:  nan\n",
      "Epoch:  69 Train Loss:  nan\n",
      "Epoch:  69 Val Loss:  nan\n",
      "Epoch:  70 Train Loss:  nan\n",
      "Epoch:  70 Val Loss:  nan\n",
      "Epoch:  71 Train Loss:  nan\n",
      "Epoch:  71 Val Loss:  nan\n",
      "Epoch:  72 Train Loss:  nan\n",
      "Epoch:  72 Val Loss:  nan\n",
      "Epoch:  73 Train Loss:  nan\n",
      "Epoch:  73 Val Loss:  nan\n",
      "Epoch:  74 Train Loss:  nan\n",
      "Epoch:  74 Val Loss:  nan\n",
      "Epoch:  75 Train Loss:  nan\n",
      "Epoch:  75 Val Loss:  nan\n",
      "Epoch:  76 Train Loss:  nan\n",
      "Epoch:  76 Val Loss:  nan\n",
      "Epoch:  77 Train Loss:  nan\n",
      "Epoch:  77 Val Loss:  nan\n",
      "Epoch:  78 Train Loss:  nan\n",
      "Epoch:  78 Val Loss:  nan\n",
      "Epoch:  79 Train Loss:  nan\n",
      "Epoch:  79 Val Loss:  nan\n",
      "Epoch:  80 Train Loss:  nan\n",
      "Epoch:  80 Val Loss:  nan\n",
      "Epoch:  81 Train Loss:  nan\n",
      "Epoch:  81 Val Loss:  nan\n",
      "Epoch:  82 Train Loss:  nan\n",
      "Epoch:  82 Val Loss:  nan\n",
      "Epoch:  83 Train Loss:  nan\n",
      "Epoch:  83 Val Loss:  nan\n",
      "Epoch:  84 Train Loss:  nan\n",
      "Epoch:  84 Val Loss:  nan\n",
      "Epoch:  85 Train Loss:  nan\n",
      "Epoch:  85 Val Loss:  nan\n",
      "Epoch:  86 Train Loss:  nan\n",
      "Epoch:  86 Val Loss:  nan\n",
      "Epoch:  87 Train Loss:  nan\n",
      "Epoch:  87 Val Loss:  nan\n",
      "Epoch:  88 Train Loss:  nan\n",
      "Epoch:  88 Val Loss:  nan\n",
      "Epoch:  89 Train Loss:  nan\n",
      "Epoch:  89 Val Loss:  nan\n",
      "Epoch:  90 Train Loss:  nan\n",
      "Epoch:  90 Val Loss:  nan\n",
      "Epoch:  91 Train Loss:  nan\n",
      "Epoch:  91 Val Loss:  nan\n",
      "Epoch:  92 Train Loss:  nan\n",
      "Epoch:  92 Val Loss:  nan\n",
      "Epoch:  93 Train Loss:  nan\n",
      "Epoch:  93 Val Loss:  nan\n",
      "Epoch:  94 Train Loss:  nan\n",
      "Epoch:  94 Val Loss:  nan\n",
      "Epoch:  95 Train Loss:  nan\n",
      "Epoch:  95 Val Loss:  nan\n",
      "Epoch:  96 Train Loss:  nan\n",
      "Epoch:  96 Val Loss:  nan\n",
      "Epoch:  97 Train Loss:  nan\n",
      "Epoch:  97 Val Loss:  nan\n",
      "Epoch:  98 Train Loss:  nan\n",
      "Epoch:  98 Val Loss:  nan\n",
      "Epoch:  99 Train Loss:  nan\n",
      "Epoch:  99 Val Loss:  nan\n",
      "Training completed\n",
      "linear normal\n",
      "Started Training\n",
      "Epoch:  0 Train Loss:  nan\n",
      "Epoch:  0 Val Loss:  nan\n",
      "Epoch:  1 Train Loss:  nan\n",
      "Epoch:  1 Val Loss:  nan\n",
      "Epoch:  2 Train Loss:  nan\n",
      "Epoch:  2 Val Loss:  nan\n",
      "Epoch:  3 Train Loss:  nan\n",
      "Epoch:  3 Val Loss:  nan\n",
      "Epoch:  4 Train Loss:  nan\n",
      "Epoch:  4 Val Loss:  nan\n",
      "Epoch:  5 Train Loss:  nan\n",
      "Epoch:  5 Val Loss:  nan\n",
      "Epoch:  6 Train Loss:  nan\n",
      "Epoch:  6 Val Loss:  nan\n",
      "Epoch:  7 Train Loss:  nan\n",
      "Epoch:  7 Val Loss:  nan\n",
      "Epoch:  8 Train Loss:  nan\n",
      "Epoch:  8 Val Loss:  nan\n",
      "Epoch:  9 Train Loss:  nan\n",
      "Epoch:  9 Val Loss:  nan\n",
      "Epoch:  10 Train Loss:  nan\n",
      "Epoch:  10 Val Loss:  nan\n",
      "Epoch:  11 Train Loss:  nan\n",
      "Epoch:  11 Val Loss:  nan\n",
      "Epoch:  12 Train Loss:  nan\n",
      "Epoch:  12 Val Loss:  nan\n",
      "Epoch:  13 Train Loss:  nan\n",
      "Epoch:  13 Val Loss:  nan\n",
      "Epoch:  14 Train Loss:  nan\n",
      "Epoch:  14 Val Loss:  nan\n",
      "Epoch:  15 Train Loss:  nan\n",
      "Epoch:  15 Val Loss:  nan\n",
      "Epoch:  16 Train Loss:  nan\n",
      "Epoch:  16 Val Loss:  nan\n",
      "Epoch:  17 Train Loss:  nan\n",
      "Epoch:  17 Val Loss:  nan\n",
      "Epoch:  18 Train Loss:  nan\n",
      "Epoch:  18 Val Loss:  nan\n",
      "Epoch:  19 Train Loss:  nan\n",
      "Epoch:  19 Val Loss:  nan\n",
      "Epoch:  20 Train Loss:  nan\n",
      "Epoch:  20 Val Loss:  nan\n",
      "Epoch:  21 Train Loss:  nan\n",
      "Epoch:  21 Val Loss:  nan\n",
      "Epoch:  22 Train Loss:  nan\n",
      "Epoch:  22 Val Loss:  nan\n",
      "Epoch:  23 Train Loss:  nan\n",
      "Epoch:  23 Val Loss:  nan\n",
      "Epoch:  24 Train Loss:  nan\n",
      "Epoch:  24 Val Loss:  nan\n",
      "Epoch:  25 Train Loss:  nan\n",
      "Epoch:  25 Val Loss:  nan\n",
      "Epoch:  26 Train Loss:  nan\n",
      "Epoch:  26 Val Loss:  nan\n",
      "Epoch:  27 Train Loss:  nan\n",
      "Epoch:  27 Val Loss:  nan\n",
      "Epoch:  28 Train Loss:  nan\n",
      "Epoch:  28 Val Loss:  nan\n",
      "Epoch:  29 Train Loss:  nan\n",
      "Epoch:  29 Val Loss:  nan\n",
      "Epoch:  30 Train Loss:  nan\n",
      "Epoch:  30 Val Loss:  nan\n",
      "Epoch:  31 Train Loss:  nan\n",
      "Epoch:  31 Val Loss:  nan\n",
      "Epoch:  32 Train Loss:  nan\n",
      "Epoch:  32 Val Loss:  nan\n",
      "Epoch:  33 Train Loss:  nan\n",
      "Epoch:  33 Val Loss:  nan\n",
      "Epoch:  34 Train Loss:  nan\n",
      "Epoch:  34 Val Loss:  nan\n",
      "Epoch:  35 Train Loss:  nan\n",
      "Epoch:  35 Val Loss:  nan\n",
      "Epoch:  36 Train Loss:  nan\n",
      "Epoch:  36 Val Loss:  nan\n",
      "Epoch:  37 Train Loss:  nan\n",
      "Epoch:  37 Val Loss:  nan\n",
      "Epoch:  38 Train Loss:  nan\n",
      "Epoch:  38 Val Loss:  nan\n",
      "Epoch:  39 Train Loss:  nan\n",
      "Epoch:  39 Val Loss:  nan\n",
      "Epoch:  40 Train Loss:  nan\n",
      "Epoch:  40 Val Loss:  nan\n",
      "Epoch:  41 Train Loss:  nan\n",
      "Epoch:  41 Val Loss:  nan\n",
      "Epoch:  42 Train Loss:  nan\n",
      "Epoch:  42 Val Loss:  nan\n",
      "Epoch:  43 Train Loss:  nan\n",
      "Epoch:  43 Val Loss:  nan\n",
      "Epoch:  44 Train Loss:  nan\n",
      "Epoch:  44 Val Loss:  nan\n",
      "Epoch:  45 Train Loss:  nan\n",
      "Epoch:  45 Val Loss:  nan\n",
      "Epoch:  46 Train Loss:  nan\n",
      "Epoch:  46 Val Loss:  nan\n",
      "Epoch:  47 Train Loss:  nan\n",
      "Epoch:  47 Val Loss:  nan\n",
      "Epoch:  48 Train Loss:  nan\n",
      "Epoch:  48 Val Loss:  nan\n",
      "Epoch:  49 Train Loss:  nan\n",
      "Epoch:  49 Val Loss:  nan\n",
      "Epoch:  50 Train Loss:  nan\n",
      "Epoch:  50 Val Loss:  nan\n",
      "Epoch:  51 Train Loss:  nan\n",
      "Epoch:  51 Val Loss:  nan\n",
      "Epoch:  52 Train Loss:  nan\n",
      "Epoch:  52 Val Loss:  nan\n",
      "Epoch:  53 Train Loss:  nan\n",
      "Epoch:  53 Val Loss:  nan\n",
      "Epoch:  54 Train Loss:  nan\n",
      "Epoch:  54 Val Loss:  nan\n",
      "Epoch:  55 Train Loss:  nan\n",
      "Epoch:  55 Val Loss:  nan\n",
      "Epoch:  56 Train Loss:  nan\n",
      "Epoch:  56 Val Loss:  nan\n",
      "Epoch:  57 Train Loss:  nan\n",
      "Epoch:  57 Val Loss:  nan\n",
      "Epoch:  58 Train Loss:  nan\n",
      "Epoch:  58 Val Loss:  nan\n",
      "Epoch:  59 Train Loss:  nan\n",
      "Epoch:  59 Val Loss:  nan\n",
      "Epoch:  60 Train Loss:  nan\n",
      "Epoch:  60 Val Loss:  nan\n",
      "Epoch:  61 Train Loss:  nan\n",
      "Epoch:  61 Val Loss:  nan\n",
      "Epoch:  62 Train Loss:  nan\n",
      "Epoch:  62 Val Loss:  nan\n",
      "Epoch:  63 Train Loss:  nan\n",
      "Epoch:  63 Val Loss:  nan\n",
      "Epoch:  64 Train Loss:  nan\n",
      "Epoch:  64 Val Loss:  nan\n",
      "Epoch:  65 Train Loss:  nan\n",
      "Epoch:  65 Val Loss:  nan\n",
      "Epoch:  66 Train Loss:  nan\n",
      "Epoch:  66 Val Loss:  nan\n",
      "Epoch:  67 Train Loss:  nan\n",
      "Epoch:  67 Val Loss:  nan\n",
      "Epoch:  68 Train Loss:  nan\n",
      "Epoch:  68 Val Loss:  nan\n",
      "Epoch:  69 Train Loss:  nan\n",
      "Epoch:  69 Val Loss:  nan\n",
      "Epoch:  70 Train Loss:  nan\n",
      "Epoch:  70 Val Loss:  nan\n",
      "Epoch:  71 Train Loss:  nan\n",
      "Epoch:  71 Val Loss:  nan\n",
      "Epoch:  72 Train Loss:  nan\n",
      "Epoch:  72 Val Loss:  nan\n",
      "Epoch:  73 Train Loss:  nan\n",
      "Epoch:  73 Val Loss:  nan\n",
      "Epoch:  74 Train Loss:  nan\n",
      "Epoch:  74 Val Loss:  nan\n",
      "Epoch:  75 Train Loss:  nan\n",
      "Epoch:  75 Val Loss:  nan\n",
      "Epoch:  76 Train Loss:  nan\n",
      "Epoch:  76 Val Loss:  nan\n",
      "Epoch:  77 Train Loss:  nan\n",
      "Epoch:  77 Val Loss:  nan\n",
      "Epoch:  78 Train Loss:  nan\n",
      "Epoch:  78 Val Loss:  nan\n",
      "Epoch:  79 Train Loss:  nan\n",
      "Epoch:  79 Val Loss:  nan\n",
      "Epoch:  80 Train Loss:  nan\n",
      "Epoch:  80 Val Loss:  nan\n",
      "Epoch:  81 Train Loss:  nan\n",
      "Epoch:  81 Val Loss:  nan\n",
      "Epoch:  82 Train Loss:  nan\n",
      "Epoch:  82 Val Loss:  nan\n",
      "Epoch:  83 Train Loss:  nan\n",
      "Epoch:  83 Val Loss:  nan\n",
      "Epoch:  84 Train Loss:  nan\n",
      "Epoch:  84 Val Loss:  nan\n",
      "Epoch:  85 Train Loss:  nan\n",
      "Epoch:  85 Val Loss:  nan\n",
      "Epoch:  86 Train Loss:  nan\n",
      "Epoch:  86 Val Loss:  nan\n",
      "Epoch:  87 Train Loss:  nan\n",
      "Epoch:  87 Val Loss:  nan\n",
      "Epoch:  88 Train Loss:  nan\n",
      "Epoch:  88 Val Loss:  nan\n",
      "Epoch:  89 Train Loss:  nan\n",
      "Epoch:  89 Val Loss:  nan\n",
      "Epoch:  90 Train Loss:  nan\n",
      "Epoch:  90 Val Loss:  nan\n",
      "Epoch:  91 Train Loss:  nan\n",
      "Epoch:  91 Val Loss:  nan\n",
      "Epoch:  92 Train Loss:  nan\n",
      "Epoch:  92 Val Loss:  nan\n",
      "Epoch:  93 Train Loss:  nan\n",
      "Epoch:  93 Val Loss:  nan\n",
      "Epoch:  94 Train Loss:  nan\n",
      "Epoch:  94 Val Loss:  nan\n",
      "Epoch:  95 Train Loss:  nan\n",
      "Epoch:  95 Val Loss:  nan\n",
      "Epoch:  96 Train Loss:  nan\n",
      "Epoch:  96 Val Loss:  nan\n",
      "Epoch:  97 Train Loss:  nan\n",
      "Epoch:  97 Val Loss:  nan\n",
      "Epoch:  98 Train Loss:  nan\n",
      "Epoch:  98 Val Loss:  nan\n",
      "Epoch:  99 Train Loss:  nan\n",
      "Epoch:  99 Val Loss:  nan\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "activation=[\"sigmoid\",\"tanh\",\"relu\",\"leaky_relu\",\"linear\"]\n",
    "initialisation=[\"zero\",\"random\",\"normal\"]\n",
    "for i in activation:\n",
    "    for j in initialisation:\n",
    "        print(i,j)\n",
    "        nn=neuralnetwork(layers=4, Layer_sizes=[256, 128, 64, 32], lr=0.1, activation=i, initialisation=j, epochs=100, batch_size=128)\n",
    "        nn.fit(x_train.T,y_train.T,x_test.T,y_test.T)\n",
    "        y_pred=nn.predict(x_test)\n",
    "        nn.score(y_pred,y_test)\n",
    "        nn.save_model(i+\"_\"+j+\".joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
